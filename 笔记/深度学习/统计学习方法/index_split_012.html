<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos311156" class="calibre_"><span class="calibre1"><span class="bold">第7章　支持向量机</span></span></p><p class="calibre_13">支持向量机（support vector machines，SVM）是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划（convex quadratic programming）的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法是求解凸二次规划的最优化算法。</p><p class="calibre_8">支持向量机学习方法包含构建由简至繁的模型：线性可分支持向量机（linear support vector machine in linearly separable case）、线性支持向量机（linear support vector machine）及非线性支持向量机（non-linear support vector machine）。简单模型是复杂模型的基础，也是复杂模型的特殊情况。当训练数据线性可分时，通过硬间隔最大化（hard margin maximization），学习一个线性的分类器，即线性可分支持向量机，又称为硬间隔支持向量机；当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization），也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；当训练数据线性不可分时，通过使用核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。</p><p class="calibre_8">当输入空间为欧氏空间或离散集合、特征空间为希尔伯特空间时，核函数（kernel function）表示将输入从输入空间映射到特征空间得到的特征向量之间的内积。通过使用核函数可以学习非线性支持向量机，等价于隐式地在高维的特征空间中学习线性支持向量机。这样的方法称为核技巧。核方法（kernel method）是比支持向量机更为一般的机器学习方法。</p><p class="calibre_8">Cortes与Vapnik提出线性支持向量机，Boser、Guyon与Vapnik又引入核技巧，提出非线性支持向量机。</p><p class="calibre_8">本章按照上述思路介绍3类支持向量机、核函数及一种快速学习算法——序列最小最优化算法（SMO）。</p><p id="filepos313569" class="calibre_6"><span class="calibre9"><span class="bold">7.1　线性可分支持向量机与硬间隔最大化</span></span></p><p id="filepos313699" class="calibre_6"><span class="calibre2"><span class="bold">7.1.1　线性可分支持向量机</span></span></p><p class="calibre_7">考虑一个二类分类问题。假设输入空间与特征空间为两个不同的空间。输入空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间。线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量。所以，输入都由输入空间转换到特征空间，支持向量机的学习是在特征空间进行的。</p><p class="calibre_8">假设给定一个特征空间上的训练数据集</p><p class="calibre_20"><img src="images/00158.jpg" class="calibre_383"/></p><p class="calibre_24">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}，i＝1,2,…,N，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为第i个特征向量，也称为实例，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的类标记，当y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1时，称x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为正例；当y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1时，称x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为负例，(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)称为样本点。再假设训练数据集是线性可分的（见定义2.2）。</p><p class="calibre_8">学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。分离超平面对应于方程w·x+b＝0，它由法向量w和截距b决定，可用(w,b)来表示。分离超平面将特征空间划分为两部分，一部分是正类，一部分是负类。法向量指向的一侧为正类，另一侧为负类。</p><p class="calibre_8">一般地，当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略，求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。</p><p class="calibre_8"><span class="bold">定义7.1（线性可分支持向量机）　</span>给定线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为</p><p class="calibre_20"><img src="images/00160.jpg" class="calibre_384"/></p><p class="calibre_22">以及相应的分类决策函数</p><p class="calibre_20"><img src="images/00164.jpg" class="calibre_385"/></p><p class="calibre_22">称为线性可分支持向量机。</p><p class="calibre_8">考虑如图7.1所示的二维特征空间中的分类问题。图中“○”表示正例，“×”表示负例。训练数据集线性可分，这时有许多直线能将两类数据正确划分。线性可分支持向量机对应着将两类数据正确划分并且间隔最大的直线，如图7.1所示。</p><p class="calibre_20"><img src="images/00451.jpg" class="calibre_386"/></p><p class="calibre_20"><span class="calibre4">图7.1　二类分类问题</span></p><p class="calibre_27">间隔最大及相应的约束最优化问题将在下面叙述。这里先介绍函数间隔和几何间隔的概念。</p><p id="filepos317605" class="calibre_6"><span class="calibre2"><span class="bold">7.1.2　函数间隔和几何间隔</span></span></p><p class="calibre_7">在图7.1中，有A，B，C三个点，表示3个实例，均在分离超平面的正类一侧，预测它们的类。点A距分离超平面较远，若预测该点为正类，就比较确信预测是正确的；点C距分离超平面较近，若预测该点为正类就不那么确信；点B介于点A与C之间，预测其为正类的确信度也在A与C之间。</p><p class="calibre_8">一般来说，一个点距离分离超平面的远近可以表示分类预测的确信程度。在超平面w·x+b＝0确定的情况下，|w·x+b|能够相对地表示点x距离超平面的远近。而w·x+b的符号与类标记y的符号是否一致能够表示分类是否正确。所以可用量y(w·x+b)来表示分类的正确性及确信度，这就是函数间隔（functional margin）的概念。</p><p class="calibre_8"><span class="bold">定义7.2（函数间隔）</span>　对于给定的训练数据集T和超平面(w,b)，定义超平面(w,b)关于样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的函数间隔为</p><p class="calibre_20"><img src="images/00171.jpg" class="calibre_387"/></p><p class="calibre_22">定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的函数间隔之最小值，即</p><p class="calibre_20"><img src="images/00177.jpg" class="calibre_388"/></p><p class="calibre_24">函数间隔可以表示分类预测的正确性及确信度。但是选择分离超平面时，只有函数间隔还不够。因为只要成比例地改变w和b，例如将它们改为2w和2b，超平面并没有改变，但函数间隔却成为原来的2倍。这一事实启示我们，可以对分离超平面的法向量w加某些约束，如规范化，||w||＝ 1，使得间隔是确定的。这时函数间隔成为几何间隔（geometric margin）。</p><p class="calibre_8">图7.2给出了超平面(w,b)及其法向量w。点A表示某一实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，其类标记为y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1。点A与超平面(w,b)的距离由线段AB给出，记作<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。</p><p class="calibre_20"><img src="images/00181.jpg" class="calibre_389"/></p><p class="calibre_22">其中，||w||为w的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>范数。这是点A在超平面正的一侧的情形。如果点A在超平面负的一侧，即y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1，那么点与超平面的距离为</p><p class="calibre_20"><img src="images/00182.jpg" class="calibre_390"/></p><p class="calibre_24">一般地，当样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)被超平面(w,b)正确分类时，点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>与超平面(w,b)的距离是</p><p class="calibre_20"><img src="images/00100.jpg" class="calibre_391"/></p><p class="calibre_24">由这一事实导出几何间隔的概念。</p><p class="calibre_20"><img src="images/00206.jpg" class="calibre_392"/></p><p class="calibre_20"><span class="calibre4">图7.2　几何间隔</span></p><p class="calibre_27"><span class="bold">定义7.3（几何间隔）</span>　对于给定的训练数据集T和超平面(w,b)，定义超平面(w,b)关于样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的几何间隔为</p><p class="calibre_20"><img src="images/00192.jpg" class="calibre_393"/></p><p class="calibre_24">定义超平面(w,b)关于训练数据集T的几何间隔为超平面(w,b)关于T中所有样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的几何间隔之最小值，即</p><p class="calibre_20"><img src="images/00196.jpg" class="calibre_394"/></p><p class="calibre_24">超平面(w,b)关于样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的几何间隔一般是实例点到超平面的带符号的距离（signed distance），当样本点被超平面正确分类时就是实例点到超平面的距离。</p><p class="calibre_8">从函数间隔和几何间隔的定义（式（7.3）～式（7.6））可知，函数间隔和几何间隔有下面的关系：</p><p class="calibre_20"><img src="images/00201.jpg" class="calibre_395"/></p><p class="calibre_22">如果||w||＝1，那么函数间隔和几何间隔相等。如果超平面参数w和b成比例地改变（超平面没有改变），函数间隔也按此比例改变，而几何间隔不变。</p><p id="filepos323041" class="calibre_6"><span class="calibre2"><span class="bold">7.1.3　间隔最大化</span></span></p><p class="calibre_7">支持向量机学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面。对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。这里的间隔最大化又称为硬间隔最大化（与将要讨论的训练数据集近似线性可分时的软间隔最大化相对应）。</p><p class="calibre_8">间隔最大化的直观解释是：对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。也就是说，不仅将正负实例点分开，而且对最难分的实例点（离超平面最近的点）也有足够大的确信度将它们分开。这样的超平面应该对未知的新实例有很好的分类预测能力。</p><p class="calibre_7"><span class="bold">1．最大间隔分离超平面</span></p><p class="calibre_7">下面考虑如何求得一个几何间隔最大的分离超平面，即最大间隔分离超平面。具体地，这个问题可以表示为下面的约束最优化问题：</p><p class="calibre_20"><img src="images/00204.jpg" class="calibre_396"/></p><p class="calibre_22">即我们希望最大化超平面(w,b)关于训练数据集的几何间隔·，约束条件表示的是超平面(w,b)关于每个训练样本点的几何间隔至少是<img src="images/00021.jpg" class="calibre_18"/>。</p><p class="calibre_8">考虑几何间隔和函数间隔的关系式（7.8），可将这个问题改写为</p><p class="calibre_20"><img src="images/00210.jpg" class="calibre_397"/></p><p class="calibre_24">函数间隔<img src="images/00866.jpg" class="calibre_398"/>的取值并不影响最优化问题的解。事实上，假设将w和b按比例改变为<img src="images/00759.jpg" class="calibre_47"/>w和<img src="images/00759.jpg" class="calibre_47"/>b，这时函数间隔成为<img src="images/00759.jpg" class="calibre_47"/><img src="images/00866.jpg" class="calibre_398"/>。函数间隔的这一改变对上面最优化问题的不等式约束没有影响，对目标函数的优化也没有影响，也就是说，它产生一个等价的最优化问题。这样，就可以取<img src="images/00866.jpg" class="calibre_398"/>＝1。将<img src="images/00866.jpg" class="calibre_398"/>＝1代入上面的最优化问题，注意到最大化<img src="images/00447.jpg" class="calibre_110"/>和最小化<img src="images/00811.jpg" class="calibre_61"/>||w||<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>是等价的，于是就得到下面的线性可分支持向量机学习的最优化问题</p><p class="calibre_20"><img src="images/00216.jpg" class="calibre_399"/></p><p class="calibre_22">这是一个凸二次规划（convex quadratic programming）问题。</p><p class="calibre_8">凸优化问题是指约束最优化问题</p><p class="calibre_20"><img src="images/00219.jpg" class="calibre_400"/></p><p class="calibre_22">其中，目标函数f(w)和约束函数g<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w)都是R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>上的连续可微的凸函数，约束函数h<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w)是R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>上的仿射函数<sup class="calibre5"><small id="filepos326859" class="calibre6"><a href="#filepos446165"><span class="calibre7">[1]</span></a></small></sup>。</p><p class="calibre_8">当目标函数f(w)是二次函数且约束函数g<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w)是仿射函数时，上述凸最优化问题成为凸二次规划问题。</p><p class="calibre_8">如果求出了约束最优化问题（7.13）～（7.14）的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，那么就可以得到最大间隔分离超平面w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0及分类决策函数f(x)＝sign(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)，即线性可分支持向量机模型。</p><p class="calibre_8">综上所述，就有下面的线性可分支持向量机的学习算法——最大间隔法（maximum margin method）。</p><p class="calibre_146"><span class="bold">算法7.1（线性可分支持向量机学习算法——最大间隔法）</span></p><p class="calibre_146">输入：线性可分训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N；</p><p class="calibre_8">输出：最大间隔分离超平面和分类决策函数。</p><p class="calibre_8">（1）构造并求解约束最优化问题：</p><p class="calibre_20"><img src="images/00224.jpg" class="calibre_401"/></p><p class="calibre_22">求得最优解,w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>。</p><p class="calibre_8">（2）由此得到分离超平面：</p><p class="calibre_20"><img src="images/00227.jpg" class="calibre_402"/></p><p class="calibre_22">分类决策函数</p><p class="calibre_20"><img src="images/00229.jpg" class="calibre_403"/></p><p class="calibre_7"><span class="bold">2．最大间隔分离超平面的存在唯一性</span></p><p class="calibre_7">线性可分训练数据集的最大间隔分离超平面是存在且唯一的。</p><p class="calibre_8"><span class="bold">定理7.1（最大间隔分离超平面的存在唯一性）</span>　若训练数据集T线性可分，则可将训练数据集中的样本点完全正确分开的最大间隔分离超平面存在且唯一。</p><p class="calibre_8"><span class="bold">证明</span>　（1）存在性</p><p class="calibre_8">由于训练数据集线性可分，所以算法7.1中的最优化问题（7.13）～（7.14）一定存在可行解。又由于目标函数有下界，所以最优化问题（7.13）～（7.14）必有解，记作(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)。由于训练数据集中既有正类点又有负类点，所以(w,b)＝(0,b)不是最优化的可行解，因而最优解(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)必满足w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>≠0。由此得知分离超平面的存在性。</p><p class="calibre_8">（2）唯一性</p><p class="calibre_8">首先证明最优化问题（7.13）～（7.14）解中w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的唯一性。假设问题（7.13）～（7.14）存在两个最优解(<img src="images/00540.jpg" class="calibre_404"/>,<img src="images/00238.jpg" class="calibre_405"/>)和(<img src="images/00242.jpg" class="calibre_406"/>,<img src="images/00245.jpg" class="calibre_407"/>)。显然||<img src="images/00540.jpg" class="calibre_404"/>||＝||<img src="images/00242.jpg" class="calibre_406"/>||＝c，其中c是一个常数。令<img src="images/00248.jpg" class="calibre_408"/>，易知(w,b)是问题（7.13）～（7.14）的可行解，从而有</p><p class="calibre_20"><img src="images/00252.jpg" class="calibre_409"/></p><p class="calibre_22">上式表明，式中的不等号可变为等号，即||w||＝<img src="images/00811.jpg" class="calibre_61"/>||<img src="images/00540.jpg" class="calibre_404"/>||+<img src="images/00811.jpg" class="calibre_61"/>||<img src="images/00242.jpg" class="calibre_406"/>||，从而有<img src="images/00540.jpg" class="calibre_404"/>＝<img src="images/00759.jpg" class="calibre_47"/><img src="images/00242.jpg" class="calibre_406"/>，|<img src="images/00759.jpg" class="calibre_47"/>|＝ 1。若<img src="images/00759.jpg" class="calibre_47"/>＝-1，则w＝0，(w,b)不是问题（7.13）～（7.14）的可行解，矛盾。因此必有<img src="images/00759.jpg" class="calibre_47"/>＝1，即</p><p class="calibre_20"><img src="images/00046.jpg" class="calibre_410"/></p><p class="calibre_24">由此可以把两个最优解(<img src="images/00540.jpg" class="calibre_404"/>,<img src="images/00238.jpg" class="calibre_405"/>)和(<img src="images/00259.jpg" class="calibre_411"/>,<img src="images/00245.jpg" class="calibre_407"/>)分别写成(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00238.jpg" class="calibre_405"/>)和(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00245.jpg" class="calibre_407"/>)。再证<img src="images/00238.jpg" class="calibre_405"/>＝<img src="images/00245.jpg" class="calibre_407"/>。设<img src="images/00263.jpg" class="calibre_412"/>和<img src="images/00027.jpg" class="calibre_73"/>是集合{x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>|y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1}中分别对应于(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00238.jpg" class="calibre_405"/>)和(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00245.jpg" class="calibre_407"/>)使得问题的不等式等号成立的点，<img src="images/00048.jpg" class="calibre_47"/>和<img src="images/00071.jpg" class="calibre_47"/>是集合{x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>|y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1}中分别对应于(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00238.jpg" class="calibre_405"/>)和(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00245.jpg" class="calibre_407"/>)使得问题的不等式等号成立的点，则由<img src="images/00238.jpg" class="calibre_405"/>＝-<img src="images/00811.jpg" class="calibre_61"/>(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·<img src="images/00263.jpg" class="calibre_412"/>+w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·<img src="images/00048.jpg" class="calibre_47"/>)，<img src="images/00245.jpg" class="calibre_407"/>＝-<img src="images/00811.jpg" class="calibre_61"/>(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·<img src="images/00027.jpg" class="calibre_73"/>+w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·<img src="images/00071.jpg" class="calibre_47"/>)得</p><p class="calibre_20"><img src="images/00092.jpg" class="calibre_413"/></p><p class="calibre_22">又因为</p><p class="calibre_20"><img src="images/00113.jpg" class="calibre_414"/></p><p class="calibre_22">所以，w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·(<img src="images/00263.jpg" class="calibre_412"/>-<img src="images/00027.jpg" class="calibre_73"/>)＝0。同理有w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·(<img src="images/00048.jpg" class="calibre_47"/>-<img src="images/00071.jpg" class="calibre_47"/>)＝0。因此，</p><p class="calibre_20"><img src="images/00133.jpg" class="calibre_415"/></p><p class="calibre_22">由<img src="images/00540.jpg" class="calibre_404"/>＝<img src="images/00242.jpg" class="calibre_406"/>和<img src="images/00238.jpg" class="calibre_405"/>＝<img src="images/00245.jpg" class="calibre_407"/>可知，两个最优解(<img src="images/00540.jpg" class="calibre_404"/>,<img src="images/00238.jpg" class="calibre_405"/>)和(<img src="images/00242.jpg" class="calibre_406"/>,<img src="images/00245.jpg" class="calibre_407"/>)是相同的，解的唯一性得证。</p><p class="calibre_8">由问题（7.13）～（7.14）解的唯一性即得分离超平面是唯一的。</p><p class="calibre_8">（3）分离超平面能将训练数据集中的两类点完全正确地分开。</p><p class="calibre_8">由解满足问题的约束条件即可得知。</p><p class="calibre_7"><span class="bold">3．支持向量和间隔边界</span></p><p class="calibre_7">在线性可分情况下，训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量（support vector）。支持向量是使约束条件式（7.14）等号成立的点，即</p><p class="calibre_20"><img src="images/00154.jpg" class="calibre_416"/></p><p class="calibre_24">对y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1的正例点，支持向量在超平面</p><p class="calibre_20"><img src="images/00179.jpg" class="calibre_417"/></p><p class="calibre_22">上，对y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1的负例点，支持向量在超平面</p><p class="calibre_20"><img src="images/00198.jpg" class="calibre_418"/></p><p class="calibre_22">上。如图7.3所示，在H<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和H<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>上的点就是支持向量。</p><p class="calibre_20"><img src="images/00220.jpg" class="calibre_419"/></p><p class="calibre_20"><span class="calibre4">图7.3　支持向量</span></p><p class="calibre_27">注意到H<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和H<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>平行，并且没有实例点落在它们中间。在H<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>与H<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>之间形成一条长带，分离超平面与它们平行且位于它们中央。长带的宽度，即H<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>与H<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>之间的距离称为间隔（margin）。间隔依赖于分离超平面的法向量w，等于<img src="images/00243.jpg" class="calibre_420"/>·H<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和H<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>称为间隔边界。</p><p class="calibre_8">在决定分离超平面时只有支持向量起作用，而其他实例点并不起作用。如果移动支持向量将改变所求的解；但是如果在间隔边界以外移动其他实例点，甚至去掉这些点，则解是不会改变的。由于支持向量在确定分离超平面中起着决定性作用，所以将这种分类模型称为支持向量机。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。</p><p class="calibre_8"><span class="bold">例7.1 　</span>数据与例2.1相同。已知一个如图7.4所示的训练数据集，其正例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(4,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，负例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试求最大间隔分离超平面。</p><p class="calibre_20"><img src="images/00265.jpg" class="calibre_421"/></p><p class="calibre_20"><span class="calibre4">图7.4　间隔最大分离超平面示例</span></p><p class="calibre_27"><span class="bold">解</span>　按照算法7.1，根据训练数据集构造约束最优化问题：</p><p class="calibre_20"><img src="images/00284.jpg" class="calibre_422"/></p><p class="calibre_22">求得此最优化问题的解w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝w<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝<img src="images/00811.jpg" class="calibre_61"/>，b＝-2。于是最大间隔分离超平面为</p><p class="calibre_20"><img src="images/00481.jpg" class="calibre_423"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>与x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>为支持向量。</p><p id="filepos341250" class="calibre_6"><span class="calibre2"><span class="bold">7.1.4　学习的对偶算法</span></span></p><p class="calibre_7">为了求解线性可分支持向量机的最优化问题（7.13）～（7.14），将它作为原始最优化问题，应用拉格朗日对偶性（参阅附录C），通过求解对偶问题（dual problem）得到原始问题（primal problem）的最优解，这就是线性可分支持向量机的对偶算法（dual algorithm）。这样做的优点，一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题。</p><p class="calibre_8">首先构建拉格朗日函数（Lagrange function）。为此，对每一个不等式约束（7.14）引进拉格朗日乘子（Lagrange multiplier）a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>≥0，i＝1,2,…,N，定义拉格朗日函数：</p><p class="calibre_20"><img src="images/00328.jpg" class="calibre_424"/></p><p class="calibre_22">其中，a＝(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>为拉格朗日乘子向量。</p><p class="calibre_8">根据拉格朗日对偶性，原始问题的对偶问题是极大极小问题：</p><p class="calibre_20"><img src="images/00349.jpg" class="calibre_425"/></p><p class="calibre_22">所以，为了得到对偶问题的解，需要先求L(w,b,a)对w,b的极小，再求对a的极大。</p><p class="calibre_8">（1）求<img src="images/00372.jpg" class="calibre_426"/></p><p class="calibre_8">将拉格朗日函数L(w，b,a)分别对w,b求偏导数并令其等于0。</p><p class="calibre_20"><img src="images/00397.jpg" class="calibre_427"/></p><p class="calibre_22">得</p><p class="calibre_20"><img src="images/01032.jpg" class="calibre_428"/></p><p class="calibre_22">将式（7.19）代入拉格朗日函数（7.18），并利用式（7.20），即得</p><p class="calibre_20"><img src="images/00435.jpg" class="calibre_429"/></p><p class="calibre_22">即</p><p class="calibre_20"><img src="images/00459.jpg" class="calibre_430"/></p><p class="calibre_24">（2）求<img src="images/00372.jpg" class="calibre_426"/>对a的极大，即是对偶问题</p><p class="calibre_20"><img src="images/00290.jpg" class="calibre_431"/></p><p class="calibre_24">将式（7.21）的目标函数由求极大转换成求极小，就得到下面与之等价的对偶最优化问题：</p><p class="calibre_20"><img src="images/00509.jpg" class="calibre_432"/></p><p class="calibre_24">考虑原始最优化问题（7.13）～（7.14）和对偶最优化问题（7.22）～（7.24），原始问题满足定理C.2的条件，所以存在w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,β<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，使w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>是原始问题的解，a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,β<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>是对偶问题的解。这意味着求解原始问题（7.13）～（7.14）可以转换为求解对偶问题（7.22）～（7.24）。</p><p class="calibre_8">对线性可分训练数据集，假设对偶最优化问题（7.22）～（7.24）对a的解为a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00578.jpg" class="calibre_435"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，可以由a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>求得原始最优化问题（7.13）～（7.14）对(w,b)的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>。有下面的定理。</p><p class="calibre_8"><span class="bold">定理7.2</span>　设a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00515.jpg" class="calibre_433"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>是对偶最优化问题（7.22）～（7.24）的解，则存在下标j，使得<img src="images/00602.jpg" class="calibre_436"/>&gt;0，并可按下式求得原始最优化问题（7.13）～（7.14）的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>：</p><p class="calibre_20"><img src="images/00625.jpg" class="calibre_437"/></p><p class="calibre_24"><span class="bold">证明</span>　根据定理C.3，KKT条件成立，即得</p><p class="calibre_20"><img src="images/01065.jpg" class="calibre_438"/></p><p class="calibre_22">由此得</p><p class="calibre_20"><img src="images/00668.jpg" class="calibre_439"/></p><p class="calibre_22">其中至少有一个<img src="images/00602.jpg" class="calibre_436"/>&gt;0（用反证法，假设a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0，由式（7.27）可知w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0，而w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0不是原始最优化问题（7.13）～（7.14）的解，产生矛盾），对此j有</p><p class="calibre_20"><img src="images/00212.jpg" class="calibre_440"/></p><p class="calibre_22">将式（7.25）代入式（7.28）并注意到<img src="images/00320.jpg" class="calibre_404"/>＝1，即得</p><p class="calibre_20"><img src="images/00430.jpg" class="calibre_441"/></p><p class="calibre_24">由此定理可知，分离超平面可以写成</p><p class="calibre_20"><img src="images/00007.jpg" class="calibre_442"/></p><p class="calibre_24">分类决策函数可以写成</p><p class="calibre_20"><img src="images/00557.jpg" class="calibre_443"/></p><p class="calibre_22">这就是说，分类决策函数只依赖于输入x和训练样本输入的内积。式（7.30）称为线性可分支持向量机的对偶形式。</p><p class="calibre_8">综上所述，对于给定的线性可分训练数据集，可以首先求对偶问题（7.22）～（7.24）的解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>；再利用式（7.25）和式（7.26）求得原始问题的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>；从而得到分离超平面及分类决策函数。这种算法称为线性可分支持向量机的对偶学习算法，是线性可分支持向量机学习的基本算法。</p><p class="calibre_146"><span class="bold">算法7.2（线性可分支持向量机学习算法）</span></p><p class="calibre_146">输入：线性可分训练集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N；</p><p class="calibre_8">输出：分离超平面和分类决策函数。</p><p class="calibre_8">（1）构造并求解约束最优化问题</p><p class="calibre_20"><img src="images/00764.jpg" class="calibre_444"/></p><p class="calibre_24">求得最优解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00578.jpg" class="calibre_435"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。</p><p class="calibre_8">（2）计算</p><p class="calibre_20"><img src="images/00942.jpg" class="calibre_445"/></p><p class="calibre_24">并选择a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的一个正分量<img src="images/00602.jpg" class="calibre_436"/>&gt;0，计算</p><p class="calibre_20"><img src="images/00945.jpg" class="calibre_446"/></p><p class="calibre_24">（3）求得分离超平面</p><p class="calibre_20"><img src="images/00949.jpg" class="calibre_447"/></p><p class="calibre_24">分类决策函数：</p><p class="calibre_20"><img src="images/00756.jpg" class="calibre_448"/></p><p class="calibre_24">在线性可分支持向量机中，由式（7.25）、式（7.26）可知，w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>和b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>只依赖于训练数据中对应于<img src="images/00955.jpg" class="calibre_449"/>&gt;0的样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)，而其他样本点对w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>和b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>没有影响。我们将训练数据中对应于<img src="images/00955.jpg" class="calibre_449"/>&gt;0的实例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>称为支持向量。</p><p class="calibre_8"><span class="bold">定义7.4（支持向量）</span>　考虑原始最优化问题（7.13）～（7.14）及对偶最优化问题（7.22）～（7.24），将训练数据集中对应于<img src="images/00955.jpg" class="calibre_449"/>&gt;0的样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>称为支持向量。</p><p class="calibre3" style="margin:0pt; border:0pt; height:1em"> </p><p class="calibre_8">根据这一定义，支持向量一定在间隔边界上。由KKT互补条件可知，</p><p class="calibre_20"><img src="images/00958.jpg" class="calibre_450"/></p><p class="calibre_24">对应于<img src="images/00955.jpg" class="calibre_449"/>&gt;0的实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，有</p><p class="calibre_20"><img src="images/00960.jpg" class="calibre_451"/></p><p class="calibre_24">或</p><p class="calibre_20"><img src="images/00964.jpg" class="calibre_452"/></p><p class="calibre_22">即x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>一定在间隔边界上。这里的支持向量的定义与前面给出的支持向量的定义是一致的。</p><p class="calibre_8"><span class="bold">例7.2</span>　训练数据与例7.1相同。如图7.4所示，正例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(4,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，负例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试用算法7.2求线性可分支持向量机。</p><p class="calibre_8"><span class="bold">解</span>　根据所给数据，对偶问题是</p><p class="calibre_20"><img src="images/00968.jpg" class="calibre_453"/></p><p class="calibre_24">解这一最优化问题。将a<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>代入目标函数并记为</p><p class="calibre_20"><img src="images/00339.jpg" class="calibre_454"/></p><p class="calibre_22">对a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>求偏导数并令其为0，易知s(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)在点<img src="images/00977.jpg" class="calibre_455"/>取极值，但该点不满足约束条件a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>≥0，所以最小值应在边界上达到。</p><p class="calibre_8">当a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝0时，最小值<img src="images/00980.jpg" class="calibre_456"/>；当a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝0时，最小值<img src="images/00984.jpg" class="calibre_457"/>。于是s(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)在a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝<img src="images/00987.jpg" class="calibre_458"/>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝0达到最小，此时a<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝<img src="images/00987.jpg" class="calibre_458"/>。</p><p class="calibre_8">这样，<img src="images/00515.jpg" class="calibre_433"/>＝<img src="images/00990.jpg" class="calibre_406"/>＝<img src="images/00987.jpg" class="calibre_458"/>对应的实例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>是支持向量。根据式（7.25）和式（7.26）计算得</p><p class="calibre_20"><img src="images/01001.jpg" class="calibre_459"/></p><p class="calibre_24">分离超平面为</p><p class="calibre_20"><img src="images/00996.jpg" class="calibre_460"/></p><p class="calibre_24">分类决策函数为</p><p class="calibre_20"><img src="images/00148.jpg" class="calibre_461"/></p><p class="calibre_24">对于线性可分问题，上述线性可分支持向量机的学习（硬间隔最大化）算法是完美的。但是，训练数据集线性可分是理想的情形。在现实问题中，训练数据集往往是线性不可分的，即在样本中出现噪声或特异点。此时，有更一般的学习算法。</p><p id="filepos356743" class="calibre_6"><span class="calibre9"><span class="bold">7.2　线性支持向量机与软间隔最大化</span></span></p><p id="filepos356867" class="calibre_6"><span class="calibre2"><span class="bold">7.2.1　线性支持向量机</span></span></p><p class="calibre_7">线性可分问题的支持向量机学习方法，对线性不可分训练数据是不适用的，因为这时上述方法中的不等式约束并不能都成立。怎么才能将它扩展到线性不可分问题呢？这就需要修改硬间隔最大化，使其成为软间隔最大化。</p><p class="calibre_8">假设给定一个特征空间上的训练数据集</p><p class="calibre_20"><img src="images/01005.jpg" class="calibre_183"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}，i＝1,2,…,N，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为第i个特征向量，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的类标记。再假设训练数据集不是线性可分的。通常情况是，训练数据中有一些特异点（outlier），将这些特异点除去后，剩下大部分的样本点组成的集合是线性可分的。</p><p class="calibre_8">线性不可分意味着某些样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)不能满足函数间隔大于等于1的约束条件（7.14）。为了解决这个问题，可以对每个样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)引进一个松弛变量<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>≥0，使函数间隔加上松弛变量大于等于1。这样，约束条件变为</p><p class="calibre_20"><img src="images/01011.jpg" class="calibre_462"/></p><p class="calibre_22">同时，对每个松弛变量<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，支付一个代价<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。目标函数由原来的<img src="images/00811.jpg" class="calibre_61"/>||w||<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>变成</p><p class="calibre_20"><img src="images/00597.jpg" class="calibre_463"/></p><p class="calibre_22">这里，C&gt;0称为惩罚参数，一般由应用问题决定，C值大时对误分类的惩罚增大，C值小时对误分类的惩罚减小。最小化目标函数（7.31）包含两层含义：使<img src="images/00811.jpg" class="calibre_61"/>||w||<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>尽量小即间隔尽量大，同时使误分类点的个数尽量小，C是调和二者的系数。</p><p class="calibre_8">有了上面的思路，可以和训练数据集线性可分时一样来考虑训练数据集线性不可分时的线性支持向量机学习问题。相应于硬间隔最大化，它称为软间隔最大化。</p><p class="calibre_8">线性不可分的线性支持向量机的学习问题变成如下凸二次规划（convex quadratic programming）问题（原始问题）：</p><p class="calibre_20"><img src="images/01018.jpg" class="calibre_464"/></p><p class="calibre_24">原始问题（7.32）～（7.34）是一个凸二次规划问题，因而关于(w，b,<img src="images/01009.jpg" class="calibre_31"/>)的解是存在的。可以证明w的解是唯一的，但b的解不唯一，b的解存在于一个区间<sup class="calibre5"><small class="calibre6"><span class="calibre7">[11]</span></small></sup>。</p><p class="calibre_8">设问题（7.32）～（7.34）的解是w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，于是可以得到分离超平面w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0及分类决策函数f(x)＝sign(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)。称这样的模型为训练样本线性不可分时的线性支持向量机，简称为线性支持向量机。显然，线性支持向量机包含线性可分支持向量机。由于现实中训练数据集往往是线性不可分的，线性支持向量机具有更广的适用性。</p><p class="calibre_8">下面给出线性支持向量机的定义。</p><p class="calibre_8"><span class="bold">定义7.5（线性支持向量机）</span>　对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题（7.32）～（7.34），得到的分离超平面为</p><p class="calibre_20"><img src="images/01022.jpg" class="calibre_465"/></p><p class="calibre_22">以及相应的分类决策函数</p><p class="calibre_20"><img src="images/01025.jpg" class="calibre_466"/></p><p class="calibre_22">称为线性支持向量机。</p><p id="filepos362268" class="calibre_6"><span class="calibre2"><span class="bold">7.2.2　学习的对偶算法</span></span></p><p class="calibre_7">原始问题（7.32）～（7.34）的对偶问题是</p><p class="calibre_20"><img src="images/01029.jpg" class="calibre_467"/></p><p class="calibre_24">原始最优化问题（7.32）～（7.34）的拉格朗日函数是</p><p class="calibre_20"><img src="images/00073.jpg" class="calibre_468"/></p><p class="calibre_22">其中，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>≥0,<img src="images/01071.jpg" class="calibre_283"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>≥0。</p><p class="calibre_8">对偶问题是拉格朗日函数的极大极小问题。首先求L(w，b,<img src="images/01009.jpg" class="calibre_31"/>,a,<img src="images/01071.jpg" class="calibre_283"/>)对w,b,<img src="images/01009.jpg" class="calibre_31"/>的极小，由</p><p class="calibre_20"><img src="images/00180.jpg" class="calibre_469"/></p><p class="calibre_22">得</p><p class="calibre_20"><img src="images/01040.jpg" class="calibre_470"/></p><p class="calibre_24">将式（7.41）～（7.43）代入式（7.40），得</p><p class="calibre_20"><img src="images/01046.jpg" class="calibre_471"/></p><p class="calibre_24">再对<img src="images/01050.jpg" class="calibre_472"/>L(w，b,<img src="images/01009.jpg" class="calibre_31"/>,a,<img src="images/01071.jpg" class="calibre_283"/>)求a的极大，即得对偶问题：</p><p class="calibre_20"><img src="images/01055.jpg" class="calibre_473"/></p><p class="calibre_20"><img src="images/01060.jpg" class="calibre_474"/></p><p class="calibre_24">将对偶最优化问题（7.44）～（7.48）进行变换：利用等式约束（7.46）消去<img src="images/01071.jpg" class="calibre_283"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，从而只留下变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，并将约束（7.46）～（7.48）写成</p><p class="calibre_20"><img src="images/00081.jpg" class="calibre_475"/></p><p class="calibre_22">再将对目标函数求极大转换为求极小，于是得到对偶问题（7.37）～（7.39）。</p><p class="calibre_8">可以通过求解对偶问题而得到原始问题的解，进而确定分离超平面和决策函数。为此，就可以定理的形式叙述原始问题的最优解和对偶问题的最优解的关系。</p><p class="calibre_8"><span class="bold">定理7.3　</span>设a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00578.jpg" class="calibre_435"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>是对偶问题（7.37）～（7.39）的一个解，若存在a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的一个分量<img src="images/00602.jpg" class="calibre_436"/>，0&lt;<img src="images/00602.jpg" class="calibre_436"/>&lt;C，则原始问题（7.32）～（7.34）的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>可按下式求得：</p><p class="calibre_20"><img src="images/01067.jpg" class="calibre_476"/></p><p class="calibre_24"><span class="bold">证明</span>　原始问题是凸二次规划问题，解满足KKT条件。即得</p><p class="calibre_20"><img src="images/01062.jpg" class="calibre_477"/></p><p class="calibre_24">由式（7.52）易知式（7.50）成立。再由式（7.53）～（7.54）可知，若存在<img src="images/00602.jpg" class="calibre_436"/>，0&lt;<img src="images/00602.jpg" class="calibre_436"/>&lt;C，则y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)-1＝0。由此即得式（7.51）。</p><p class="calibre_8">由此定理可知，分离超平面可以写成</p><p class="calibre_20"><img src="images/00577.jpg" class="calibre_478"/></p><p class="calibre_24">分类决策函数可以写成</p><p class="calibre_20"><img src="images/00553.jpg" class="calibre_479"/></p><p class="calibre_22">式（7.56）为线性支持向量机的对偶形式。</p><p class="calibre_8">综合前面的结果，有下面的算法。</p><p class="calibre_146"><span class="bold">算法7.3（线性支持向量机学习算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N；</p><p class="calibre_8">输出：分离超平面和分类决策函数。</p><p class="calibre_8">（1）选择惩罚参数C&gt;0，构造并求解凸二次规划问题</p><p class="calibre_20"><img src="images/00667.jpg" class="calibre_480"/></p><p class="calibre_24">求得最优解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00578.jpg" class="calibre_435"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。</p><p class="calibre_8">（2）计算<img src="images/00774.jpg" class="calibre_481"/></p><p class="calibre_8">选择a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的一个分量<img src="images/00602.jpg" class="calibre_436"/>适合条件0&lt;<img src="images/00602.jpg" class="calibre_436"/>&lt;C，计算</p><p class="calibre_20"><img src="images/00879.jpg" class="calibre_482"/></p><p class="calibre_24">（3）求得分离超平面</p><p class="calibre_20"><img src="images/00989.jpg" class="calibre_447"/></p><p class="calibre_24">分类决策函数：</p><p class="calibre_20"><img src="images/00032.jpg" class="calibre_483"/></p><p class="calibre_22">步骤（2）中，对任一适合条件0&lt;<img src="images/00602.jpg" class="calibre_436"/>&lt;C的<img src="images/00602.jpg" class="calibre_436"/>，按式（7.51）都可求出b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，但是由于原始问题（7.32）～（7.34）对b的解并不唯一[11]，所以实际计算时可以取在所有符合条件的样本点上的平均值。</p><p id="filepos370076" class="calibre_6"><span class="calibre2"><span class="bold">7.2.3　支持向量</span></span></p><p class="calibre_7">在线性不可分的情况下，将对偶问题（7.37）～（7.39）的解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00578.jpg" class="calibre_435"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>中对应于<img src="images/00955.jpg" class="calibre_449"/>&gt;0的样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)的实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>称为支持向量（软间隔的支持向量）。如图7.5所示，这时的支持向量要比线性可分时的情况复杂一些。图中，分离超平面由实线表示，间隔边界由虚线表示，正例点由“○”表示，负例点由“×”表示。图中还标出了实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>到间隔边界的距离<img src="images/00136.jpg" class="calibre_484"/>。</p><p class="calibre_20"><img src="images/00247.jpg" class="calibre_485"/></p><p class="calibre_20"><span class="calibre4">图7.5　软间隔的支持向量</span></p><p class="calibre_27">软间隔的支持向量x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>或者在间隔边界上，或者在间隔边界与分离超平面之间，或者在分离超平面误分一侧。若<img src="images/00955.jpg" class="calibre_449"/>&lt;C，则<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝0，支持向量x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>恰好落在间隔边界上；若<img src="images/00955.jpg" class="calibre_449"/>＝C，0&lt;<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>&lt;1，则分类正确，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>在间隔边界与分离超平面之间；若<img src="images/00955.jpg" class="calibre_449"/>＝C，<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝1，则x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>在分离超平面上；若<img src="images/00955.jpg" class="calibre_449"/>＝C，<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>&gt;1，则x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>位于分离超平面误分一侧。</p><p id="filepos372892" class="calibre_6"><span class="calibre2"><span class="bold">7.2.4　合页损失函数</span></span></p><p class="calibre_7">对于线性支持向量机学习来说，其模型为分离超平面w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝0及决策函数f(x)＝sign(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>·x+b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)，其学习策略为软间隔最大化，学习算法为凸二次规划。</p><p class="calibre_8">线性支持向量机学习还有另外一种解释，就是最小化以下目标函数：</p><p class="calibre_20"><img src="images/00358.jpg" class="calibre_486"/></p><p class="calibre_22">目标函数的第1项是经验损失或经验风险，函数</p><p class="calibre_20"><img src="images/00465.jpg" class="calibre_487"/></p><p class="calibre_22">称为合页损失函数（hinge loss function）。下标“＋”表示以下取正值的函数。</p><p class="calibre_20"><img src="images/00586.jpg" class="calibre_488"/></p><p class="calibre_22">这就是说，当样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)被正确分类且函数间隔（确信度）y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)大于1时，损失是0，否则损失是1-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)，注意到在图7.5中的实例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>被正确分类，但损失不是0。目标函数的第2项是系数为<img src="images/00759.jpg" class="calibre_47"/>的w的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>范数，是正则化项。</p><p class="calibre_8"><span class="bold">定理7.4　</span>线性支持向量机原始最优化问题：</p><p class="calibre_20"><img src="images/00693.jpg" class="calibre_489"/></p><p class="calibre_22">等价于最优化问题</p><p class="calibre_20"><img src="images/00804.jpg" class="calibre_490"/></p><p class="calibre_24"><span class="bold">证明</span>　可将最优化问题（7.63）写成问题（7.60）～（7.62）。令</p><p class="calibre_20"><img src="images/00908.jpg" class="calibre_491"/></p><p class="calibre_22">则y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)≥1。于是w,b,<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>满足约束条件（7.61）～（7.62）。由式（7.64）有，[1-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)]<sub class="calibre8"><small class="calibre6"><span class="calibre7">+</span></small></sub>＝[<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub> ]<sub class="calibre8"><small class="calibre6"><span class="calibre7">+</span></small></sub>＝<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，所以最优化问题（7.63）可写成</p><p class="calibre_20"><img src="images/01017.jpg" class="calibre_492"/></p><p class="calibre_24">若取<img src="images/00803.jpg" class="calibre_493"/>，则</p><p class="calibre_20"><img src="images/00165.jpg" class="calibre_494"/></p><p class="calibre_22">与式（7.60）等价。</p><p class="calibre_8">反之，也可将最优化问题（7.60）～（7.62）表示成问题（7.63）。</p><p class="calibre_8">合页损失函数的图形如图7.6所示，横轴是函数间隔y(w·x+b)，纵轴是损失。由于函数形状像一个合页，故名合页损失函数。</p><p class="calibre_20"><img src="images/01047.jpg" class="calibre_495"/></p><p class="calibre_20"><span class="calibre4">图7.6　合页损失函数</span></p><p class="calibre_27">图中还画出0-1损失函数，可以认为它是二类分类问题的真正的损失函数，而合页损失函数是0-1损失函数的上界。由于0-1损失函数不是连续可导的，直接优化由其构成的目标函数比较困难，可以认为线性支持向量机是优化由0-1损失函数的上界（合页损失函数）构成的目标函数。这时的上界损失函数又称为代理损失函数（surrogate loss function）。</p><p class="calibre_8">图7.6中虚线显示的是感知机的损失函数[y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)]<sub class="calibre8"><small class="calibre6"><span class="calibre7">+</span></small></sub>。这时，当样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)被正确分类时，损失是0，否则损失是-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)。相比之下，合页损失函数不仅要分类正确，而且确信度足够高时损失才是0。也就是说，合页损失函数对学习有更高的要求。</p><p id="filepos378424" class="calibre_6"><span class="calibre9"><span class="bold">7.3　非线性支持向量机与核函数</span></span></p><p class="calibre_7">对解线性分类问题，线性分类支持向量机是一种非常有效的方法。但是，有时分类问题是非线性的，这时可以使用非线性支持向量机。本节叙述非线性支持向量机，其主要特点是利用核技巧（kernel trick）。为此，先要介绍核技巧。核技巧不仅应用于支持向量机，而且应用于其他统计学习问题。</p><p id="filepos378961" class="calibre_6"><span class="calibre2"><span class="bold">7.3.1　核技巧</span></span></p><p class="calibre_7"><span class="bold">1．非线性分类问题</span></p><p class="calibre_7">非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。先看一个例子：如7.7左图，是一个分类问题，图中“·”表示正实例点，“×”表示负实例点。由图可见，无法用直线（线性模型）将正负实例正确分开，但可以用一条椭圆曲线（非线性模型）将它们正确分开。</p><p class="calibre_20"><img src="images/00765.jpg" class="calibre_496"/></p><p class="calibre_20"><span class="calibre4">图7.7　非线性分类问题与核技巧示例</span></p><p class="calibre_27">一般来说，对给定的一个训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中，实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>属于输入空间，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，对应的标记有两类y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N。如果能用R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>中的一个超曲面将正负例正确分开，则称这个问题为非线性可分问题。</p><p class="calibre_8">非线性问题往往不好求解，所以希望能用解线性分类问题的方法解决这个问题。所采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。对图7.7所示的例子，通过变换，将左图中椭圆变换成右图中的直线，将非线性分类问题变换为线性分类问题。</p><p class="calibre_8">设原空间为x⊂R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>，x＝(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>∊x，新空间为<img src="images/00496.jpg" class="calibre_497"/>⊂R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>，z＝(z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>∊<img src="images/00496.jpg" class="calibre_497"/>，定义从原空间到新空间的变换（映射）：</p><p class="calibre_20"><img src="images/00614.jpg" class="calibre_498"/></p><p class="calibre_22">经过变换z＝Ø(x)，原空间x⊂R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>变换为新空间<img src="images/00496.jpg" class="calibre_497"/>⊂R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>，原空间中的点相应地变换为新空间中的点，原空间中的椭圆</p><p class="calibre_20"><img src="images/00720.jpg" class="calibre_499"/></p><p class="calibre_22">变换成为新空间中的直线</p><p class="calibre_20"><img src="images/00362.jpg" class="calibre_500"/></p><p class="calibre_22">在变换后的新空间里，直线w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>+w<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>+b＝0可以将变换后的正负实例点正确分开。这样，原空间的非线性可分问题就变成了新空间的线性可分问题。</p><p class="calibre_8">上面的例子说明，用线性分类方法求解非线性分类问题分为两步：首先使用一个变换将原空间的数据映射到新空间；然后在新空间里用线性分类学习方法从训练数据中学习分类模型。核技巧就属于这样的方法。</p><p class="calibre_8">核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间（欧氏空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>或离散集合）对应于一个特征空间（希尔伯特空间<img src="images/00911.jpg" class="calibre_322"/>），使得在输入空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>中的超曲面模型对应于特征空间<img src="images/00911.jpg" class="calibre_322"/>中的超平面模型（支持向量机）。这样，分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成。</p><p class="calibre_7"><span class="bold">2．核函数的定义</span></p><p class="calibre_7"><span class="bold">定义7.6（核函数）</span>　设x是输入空间（欧氏空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>的子集或离散集合），又设<img src="images/00911.jpg" class="calibre_322"/>为特征空间（希尔伯特空间），如果存在一个从x到<img src="images/00911.jpg" class="calibre_322"/>的映射</p><p class="calibre_20"><img src="images/00388.jpg" class="calibre_501"/></p><p class="calibre_22">使得对所有x,z∊x，函数K(x,z)满足条件</p><p class="calibre_20"><img src="images/00941.jpg" class="calibre_502"/></p><p class="calibre_22">则称K(x,z)为核函数，Ø(x)为映射函数，式中Ø(x)·Ø(z)为Ø(x)和Ø(z)的内积。</p><p class="calibre3" style="margin:0pt; border:0pt; height:1em"> </p><p class="calibre_8">核技巧的想法是，在学习与预测中只定义核函数K(x,z)，而不显式地定义映射函数Ø。通常，直接计算K(x,z)比较容易，而通过Ø(x)和Ø(z)计算K(x,z)并不容易。注意，Ø是输入空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>到特征空间<img src="images/00911.jpg" class="calibre_322"/>的映射，特征空间<img src="images/00911.jpg" class="calibre_322"/>一般是高维的，甚至是无穷维的。可以看到，对于给定的核K(x,z)，特征空间<img src="images/00911.jpg" class="calibre_322"/>和映射函数Ø的取法并不唯一，可以取不同的特征空间，即便是在同一特征空间里也可以取不同的映射。</p><p class="calibre_8">下面举一个简单的例子来说明核函数和映射函数的关系。</p><p class="calibre_8"><span class="bold">例7.3</span>　假设输入空间是R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>，核函数是K(x,z)＝(x·z)<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>，试找出其相关的特征空间<img src="images/00911.jpg" class="calibre_322"/>和映射Ø(x):R<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>→<img src="images/00911.jpg" class="calibre_322"/>。</p><p class="calibre_8"><span class="bold">解</span>　取特征空间<img src="images/00911.jpg" class="calibre_322"/>＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">3</span></small></sup>，记x＝(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，z＝(z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,z<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，由于</p><p class="calibre_20"><img src="images/00194.jpg" class="calibre_503"/></p><p class="calibre_22">所以可以取映射</p><p class="calibre_20"><img src="images/00300.jpg" class="calibre_504"/></p><p class="calibre_22">容易验证Ø(x)·Ø(z)＝(x·z)<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>＝K(x,z)。</p><p class="calibre_8">仍取<img src="images/00911.jpg" class="calibre_322"/>＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">3</span></small></sup>以及</p><p class="calibre_20"><img src="images/00415.jpg" class="calibre_505"/></p><p class="calibre_22">同样有Ø(x)·Ø(z)＝(x·z)<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup>＝K(x,z)。</p><p class="calibre_8">还可以取<img src="images/00911.jpg" class="calibre_322"/>＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">4</span></small></sup>和</p><p class="calibre_20"><img src="images/00529.jpg" class="calibre_506"/></p><p class="calibre_7"><span class="bold">3．核技巧在支持向量机中的应用</span></p><p class="calibre_7">我们注意到在线性支持向量机的对偶问题中，无论是目标函数还是决策函数（分离超平面）都只涉及输入实例与实例之间的内积。在对偶问题的目标函数（7.37）中的内积x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>可以用核函数K(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>)＝Ø(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)·Ø(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>)来代替。此时对偶问题的目标函数成为</p><p class="calibre_20"><img src="images/00643.jpg" class="calibre_507"/></p><p class="calibre_22">同样，分类决策函数中的内积也可以用核函数代替，而分类决策函数式成为</p><p class="calibre_20"><img src="images/00748.jpg" class="calibre_508"/></p><p class="calibre_24">这等价于经过映射函数Ø将原来的输入空间变换到一个新的特征空间，将输入空间中的内积x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>变换为特征空间中的内积Ø(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)·Ø(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>)，在新的特征空间里从训练样本中学习线性支持向量机。当映射函数是非线性函数时，学习到的含有核函数的支持向量机是非线性分类模型。</p><p class="calibre_8">也就是说，在核函数K(x,z)给定的条件下，可以利用解线性分类问题的方法求解非线性分类问题的支持向量机。学习是隐式地在特征空间进行的，不需要显式地定义特征空间和映射函数。这样的技巧称为核技巧，它是巧妙地利用线性分类学习方法与核函数解决非线性问题的技术。在实际应用中，往往依赖领域知识直接选择核函数，核函数选择的有效性需要通过实验验证。</p><p id="filepos389998" class="calibre_6"><span class="calibre2"><span class="bold">7.3.2　正定核</span></span></p><p class="calibre_7">已知映射函数Ø,可以通过Ø(x)和Ø(z)的内积求得核函数K(x,z)。不用构造映射Ø(x)能否直接判断一个给定的函数K(x,z)是不是核函数？或者说，函数K(x,z)满足什么条件才能成为核函数？</p><p class="calibre_8">本节叙述正定核的充要条件。通常所说的核函数就是正定核函数（positive definite kernel function）。为证明此定理先介绍有关的预备知识。</p><p class="calibre_8">假设K(x,z)是定义在x×x上的对称函数，并且对任意的x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>∊x，K(x,z)关于x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的Gram矩阵是半正定的。可以依据函数K(x,z)，构成一个希尔伯特空间（Hilbert space），其步骤是：首先定义映射Ø并构成向量空间<img src="images/00855.jpg" class="calibre_351"/>；然后在<img src="images/00855.jpg" class="calibre_351"/>上定义内积构成内积空间；最后将<img src="images/00855.jpg" class="calibre_351"/>完备化构成希尔伯特空间。</p><p class="calibre_7"><span class="bold">1．定义映射，构成向量空间</span><img src="images/00855.jpg" class="calibre_351"/></p><p class="calibre_7">先定义映射</p><p class="calibre_20"><img src="images/01054.jpg" class="calibre_509"/></p><p class="calibre_22">根据这一映射，对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊R，i＝1,2,…,m，定义线性组合</p><p class="calibre_20"><img src="images/00006.jpg" class="calibre_510"/></p><p class="calibre_22">考虑由线性组合为元素的集合<img src="images/00855.jpg" class="calibre_351"/>。由于集合<img src="images/00855.jpg" class="calibre_351"/>对加法和数乘运算是封闭的，所以<img src="images/00855.jpg" class="calibre_351"/>构成一个向量空间。</p><p class="calibre_7"><span class="bold">2．在</span><img src="images/00855.jpg" class="calibre_351"/><span class="bold">上定义内积，使其成为内积空间</span></p><p class="calibre_7">在<img src="images/00855.jpg" class="calibre_351"/>上定义一个运算*：对任意f,g∊<img src="images/00855.jpg" class="calibre_351"/>，</p><p class="calibre_20"><img src="images/00191.jpg" class="calibre_511"/></p><p class="calibre_22">定义运算*</p><p class="calibre_20"><img src="images/00298.jpg" class="calibre_512"/></p><p class="calibre_24">证明运算*是空间<img src="images/00855.jpg" class="calibre_351"/>的内积。为此要证：</p><p class="calibre_20"><img src="images/00411.jpg" class="calibre_513"/></p><p class="calibre_24">其中，(1)～（3）由式（7.70）～式（7.72）及K(x,z)的对称性容易得到。现证（4）之式（7.77）。由式（7.70）及式（7.73）可得：</p><p class="calibre_20"><img src="images/00526.jpg" class="calibre_514"/></p><p class="calibre_22">由Gram矩阵的半正定性知上式右端非负，即f*f≥0。</p><p class="calibre_8">再证（4）之式（7.78）。充分性显然。为证必要性，首先证明不等式：</p><p class="calibre_20"><img src="images/00641.jpg" class="calibre_515"/></p><p class="calibre_22">设，f，g∊<img src="images/00855.jpg" class="calibre_351"/>，<img src="images/00759.jpg" class="calibre_47"/>∊R，则f+<img src="images/00759.jpg" class="calibre_47"/>g∊<img src="images/00855.jpg" class="calibre_351"/>，于是，</p><p class="calibre_20"><img src="images/00745.jpg" class="calibre_516"/></p><p class="calibre_22">其左端是<img src="images/00759.jpg" class="calibre_47"/>的二次三项式，非负，其判别式小于等于0，即</p><p class="calibre_20"><img src="images/00491.jpg" class="calibre_517"/></p><p class="calibre_22">于是式（7.79）得证。现证若f*f＝0，则f＝0。事实上，若</p><p class="calibre_20"><img src="images/00963.jpg" class="calibre_518"/></p><p class="calibre_22">则按运算*的定义式（7.73），对任意的x∊x，有</p><p class="calibre_20"><img src="images/00527.jpg" class="calibre_519"/></p><p class="calibre_22">于是，</p><p class="calibre_20"><img src="images/00004.jpg" class="calibre_520"/></p><p class="calibre_22">由式（7.79）和式（7.77）有</p><p class="calibre_20"><img src="images/00221.jpg" class="calibre_521"/></p><p class="calibre_22">由式（7.80）有</p><p class="calibre_20"><img src="images/00329.jpg" class="calibre_192"/></p><p class="calibre_22">此式表明，当f*f＝0时，对任意的x都有|f(x)|＝0。</p><p class="calibre_8">至此，证明了*为向量空间<img src="images/00855.jpg" class="calibre_351"/>的内积。赋予内积的向量空间为内积空间。因此<img src="images/00855.jpg" class="calibre_351"/>是一个内积空间。既然*为<img src="images/00855.jpg" class="calibre_351"/>的内积运算，那么仍然用·表示，即若</p><p class="calibre_20"><img src="images/00436.jpg" class="calibre_522"/></p><p class="calibre_22">则</p><p class="calibre_20"><img src="images/00555.jpg" class="calibre_523"/></p><p class="calibre_7"><span class="bold">3．将内积空间</span><img src="images/00855.jpg" class="calibre_351"/><span class="bold">完备化为希尔伯特空间</span></p><p class="calibre_7">现在将内积空间<img src="images/00855.jpg" class="calibre_351"/>完备化。由式（7.81）定义的内积可以得到范数</p><p class="calibre_20"><img src="images/00669.jpg" class="calibre_524"/></p><p class="calibre_22">因此，<img src="images/00855.jpg" class="calibre_351"/>是一个赋范向量空间。根据泛函分析理论，对于不完备的赋范向量空间<img src="images/00855.jpg" class="calibre_351"/>，一定可以使之完备化，得到完备的赋范向量空间<img src="images/00911.jpg" class="calibre_322"/>。一个内积空间，当作为一个赋范向量空间是完备的时候，就是希尔伯特空间。这样，就得到了希尔伯特空间<img src="images/00911.jpg" class="calibre_322"/>。</p><p class="calibre_8">这一希尔伯特空间<img src="images/00911.jpg" class="calibre_322"/>称为再生核希尔伯特空间（reproducing kernel Hilbert space，RKHS）。这是由于核K具有再生性，即满足</p><p class="calibre_20"><img src="images/00776.jpg" class="calibre_525"/></p><p class="calibre_22">及</p><p class="calibre_20"><img src="images/00880.jpg" class="calibre_526"/></p><p class="calibre_22">称为再生核。</p><p class="calibre_7"><span class="bold">4．正定核的充要条件</span></p><p class="calibre_7"><span class="bold">定理7.5（正定核的充要条件）　</span>设K:<img src="images/00991.jpg" class="calibre_322"/>×<img src="images/00991.jpg" class="calibre_322"/>→R是对称函数，则K(x,z)为正定核函数的充要条件是对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00991.jpg" class="calibre_322"/>，i＝1,2,…,m，K(x,z)对应的Gram矩阵：</p><p class="calibre_20"><img src="images/00033.jpg" class="calibre_527"/></p><p class="calibre_22">是半正定矩阵。</p><p class="calibre_8"><span class="bold">证明</span>　必要性。由于K(x,z)是<img src="images/00991.jpg" class="calibre_322"/>×<img src="images/00991.jpg" class="calibre_322"/>上的正定核，所以存在从<img src="images/00991.jpg" class="calibre_322"/>到希尔伯特空间<img src="images/00911.jpg" class="calibre_322"/>的映射Ø，使得</p><p class="calibre_20"><img src="images/00138.jpg" class="calibre_528"/></p><p class="calibre_22">于是，对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，构造K(x,z)关于x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的Gram矩阵</p><p class="calibre_20"><img src="images/00249.jpg" class="calibre_529"/></p><p class="calibre_22">对任意c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>∊R，有</p><p class="calibre_20"><img src="images/00359.jpg" class="calibre_530"/></p><p class="calibre_22">表明K(x,z)关于x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的Gram矩阵是半正定的。</p><p class="calibre_8">充分性。已知对称函数K(x,z)对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>∊x，K(x,z)关于x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的Gram矩阵是半正定的。根据前面的结果，对给定的K(x,z)，可以构造从<img src="images/00991.jpg" class="calibre_322"/>到某个希尔伯特空间<img src="images/00911.jpg" class="calibre_322"/>的映射：</p><p class="calibre_20"><img src="images/00467.jpg" class="calibre_531"/></p><p class="calibre_22">由式（7.83）可知，</p><p class="calibre_20"><img src="images/00587.jpg" class="calibre_532"/></p><p class="calibre_22">并且</p><p class="calibre_20"><img src="images/00751.jpg" class="calibre_533"/></p><p class="calibre_22">由式（7.86）即得</p><p class="calibre_20"><img src="images/00226.jpg" class="calibre_534"/></p><p class="calibre_22">表明K(x,z)是<img src="images/00991.jpg" class="calibre_322"/>×<img src="images/00991.jpg" class="calibre_322"/>上的核函数。</p><p class="calibre_8">定理给出了正定核的充要条件，因此可以作为正定核，即核函数的另一定义。</p><p class="calibre_8"><span class="bold">定义7.7（正定核的等价定义）</span>　设<img src="images/00991.jpg" class="calibre_322"/>⊂R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，K(x,z)是定义在<img src="images/00991.jpg" class="calibre_322"/>×<img src="images/00991.jpg" class="calibre_322"/>上的对称函数，如果对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00991.jpg" class="calibre_322"/>，i＝1,2,…,m，K(x,z)对应的Gram矩阵</p><p class="calibre_20"><img src="images/00909.jpg" class="calibre_527"/></p><p class="calibre_22">是半正定矩阵，则称K(x,z)是正定核。</p><p class="calibre3" style="margin:0pt; border:0pt; height:1em"> </p><p class="calibre_8">这一定义在构造核函数时很有用。但对于一个具体函数K(x,z)来说，检验它是否为正定核函数并不容易，因为要求对任意有限输入集{x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>}验证K对应的Gram矩阵是否为半正定的。在实际问题中往往应用已有的核函数。另外，由Mercer定理可以得到Mercer核（Mercer Kernel）<sup class="calibre5"><small class="calibre6"><span class="calibre7">[11]</span></small></sup>，正定核比Mercer核更具一般性。下面介绍一些常用的核函数。</p><p id="filepos403427" class="calibre_6"><span class="calibre2"><span class="bold">7.3.3　常用核函数</span></span></p><p class="calibre_7"><span class="bold">1．多项式核函数（polynomial kernel function）</span></p><p class="calibre_6"><img src="images/00336.jpg" class="calibre_35"/></p><p class="calibre_22">对应的支持向量机是一个p次多项式分类器。在此情形下，分类决策函数成为</p><p class="calibre_20"><img src="images/00060.jpg" class="calibre_535"/></p><p class="calibre_24"><span class="bold">2。高斯核函数（Gaussian kernel function）</span></p><p class="calibre_20"><img src="images/00166.jpg" class="calibre_536"/></p><p class="calibre_22">对应的支持向量机是高斯径向基函数（radial basis function）分类器。在此情形下，分类决策函数成为</p><p class="calibre_20"><img src="images/00274.jpg" class="calibre_537"/></p><p class="calibre_7"><span class="bold">3．字符串核函数（string kernel function）</span></p><p class="calibre_7">核函数不仅可以定义在欧氏空间上，还可以定义在离散数据的集合上。比如，字符串核是定义在字符串集合上的核函数。字符串核函数在文本分类、信息检索、生物信息学等方面都有应用。</p><p class="calibre_8">考虑一个有限字符表<img src="images/00387.jpg" class="calibre_538"/>。字符串s是从<img src="images/00387.jpg" class="calibre_538"/>中取出的有限个字符的序列，包括空字符串。字符串s的长度用|s|表示，它的元素记作s(1)s(2)…s(|s|)。两个字符串s和t的连接记作st。所有长度为n的字符串的集合记作<img src="images/00387.jpg" class="calibre_538"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，所有字符串的集合记作<img src="images/00498.jpg" class="calibre_539"/></p><p class="calibre_8">考虑字符串s的子串u。给定一个指标序列i＝(i<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，i<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,i<sub class="calibre8"><small class="calibre6"><span class="calibre7">|u|</span></small></sub>)，1≤i<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>&lt;i<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>&lt;…&lt;i<sub class="calibre8"><small class="calibre6"><span class="calibre7">|u|</span></small></sub>≤|s|，s的子串定义为u＝s(i)＝s(i<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>)s(i<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)…s(i<sub class="calibre8"><small class="calibre6"><span class="calibre7">|u|</span></small></sub>)，其长度记作l(i)＝i<sub class="calibre8"><small class="calibre6"><span class="calibre7">|u|</span></small></sub>-i<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+1。如果i是连续的，则l(i)＝|u|；否则，l(i)&gt;|u|。</p><p class="calibre_8">假设<img src="images/00855.jpg" class="calibre_351"/>是长度大于或等于n字符串的集合，s是<img src="images/00855.jpg" class="calibre_351"/>的元素。现在建立字符串集合<img src="images/00855.jpg" class="calibre_351"/>到特征空间<img src="images/00911.jpg" class="calibre_322"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>＝<img src="images/00616.jpg" class="calibre_540"/>的映射Øn(s)。<img src="images/00616.jpg" class="calibre_540"/>表示定义在<img src="images/00387.jpg" class="calibre_538"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>上的实数空间，其每一维对应一个字符串u∊<img src="images/00387.jpg" class="calibre_538"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，映射Øn(s)将字符串s对应于空间<img src="images/00616.jpg" class="calibre_540"/>的一个向量，其在u维上的取值为</p><p class="calibre_20"><img src="images/00721.jpg" class="calibre_541"/></p><p class="calibre_22">这里，0&lt;<img src="images/00759.jpg" class="calibre_47"/>≤1是一个衰减参数，l(i)表示字符串i的长度，求和在s中所有与u相同的子串上进行。</p><p class="calibre_8">例如，假设<img src="images/00387.jpg" class="calibre_538"/>为英文字符集，n为3，<img src="images/00855.jpg" class="calibre_351"/>为长度大于或等于3的字符串的集合。考虑将字符集<img src="images/00855.jpg" class="calibre_351"/>映射到特征空间H<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>。H<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>的一维对应于字符串asd。这时，字符串“Nasdaq”与“lass das”在这一维上的值分别是[Ø<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>(Nasdaq)]<sub class="calibre8"><small class="calibre6"><span class="calibre7">asd</span></small></sub>＝<img src="images/00759.jpg" class="calibre_47"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">3</span></small></sup>和[Ø<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>(lass□das)]<sub class="calibre8"><small class="calibre6"><span class="calibre7">asd</span></small></sub>＝2<img src="images/00759.jpg" class="calibre_47"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">5</span></small></sup>（□为空格）。在第1个字符串里，asd是连续的子串。在第2个字符串里，asd是长度为5的不连续子串，共出现2次。</p><p class="calibre_8">两个字符串s和t上的字符串核函数是基于映射Øn的特征空间中的内积：</p><p class="calibre_20"><img src="images/00832.jpg" class="calibre_542"/></p><p class="calibre_22">字符串核函数k<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>(s,t)给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度（cosine similarity）。直观上，两个字符串相同的子串越多，它们就越相似，字符串核函数的值就越大。字符串核函数可以由动态规划快速地计算。</p><p id="filepos409516" class="calibre_6"><span class="calibre2"><span class="bold">7.3.4　非线性支持向量分类机</span></span></p><p class="calibre_7">如上所述，利用核技巧，可以将线性分类的学习方法应用到非线性分类问题中去。将线性支持向量机扩展到非线性支持向量机，只需将线性支持向量机对偶形式中的内积换成核函数。</p><p class="calibre_8"><span class="bold">定义7.8（非线性支持向量机）</span>　从非线性分类训练集，通过核函数与软间隔最大化，或凸二次规划（7.95）～（7.97），学习得到的分类决策函数</p><p class="calibre_20"><img src="images/00211.jpg" class="calibre_543"/></p><p class="calibre_22">称为非线性支持向量，K(x,z)是正定核函数。</p><p class="calibre3" style="margin:0pt; border:0pt; height:1em"> </p><p class="calibre_8">下面叙述非线性支持向量机学习算法。</p><p class="calibre_146"><span class="bold">算法7.4（非线性支持向量机学习算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N；</p><p class="calibre_8">输出：分类决策函数。</p><p class="calibre_8">（1）选取适当的核函数K(x,z)和适当的参数C，构造并求解最优化问题</p><p class="calibre_20"><img src="images/00318.jpg" class="calibre_544"/></p><p class="calibre_24">求得最优解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝(<img src="images/00515.jpg" class="calibre_433"/>,<img src="images/00554.jpg" class="calibre_434"/>,…,<img src="images/00429.jpg" class="calibre_545"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。</p><p class="calibre_8">（2）选择a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的一个正分量0&lt;<img src="images/00544.jpg" class="calibre_219"/>&lt;C，计算</p><p class="calibre_20"><img src="images/00660.jpg" class="calibre_546"/></p><p class="calibre_24">（3）构造决策函数：</p><p class="calibre_20"><img src="images/00763.jpg" class="calibre_547"/></p><p class="calibre_22">当K(x,z)是正定核函数时，问题（7.95）～（7.97）是凸二次规划问题，解是存在的。</p><p id="filepos412451" class="calibre_6"><span class="calibre9"><span class="bold">7.4　序列最小最优化算法</span></span></p><p class="calibre_7">本节讨论支持向量机学习的实现问题。我们知道，支持向量机的学习问题可以形式化为求解凸二次规划问题。这样的凸二次规划问题具有全局最优解，并且有许多最优化算法可以用于这一问题的求解。但是当训练样本容量很大时，这些算法往往变得非常低效，以致无法使用。所以，如何高效地实现支持向量机学习就成为一个重要的问题。目前人们已提出许多快速实现算法。本节讲述其中的序列最小最优化（sequential minimal optimization，SMO）算法，这种算法1998年由Platt提出。</p><p class="calibre_8">SMO算法要解如下凸二次规划的对偶问题：</p><p class="calibre_20"><img src="images/00870.jpg" class="calibre_548"/></p><p class="calibre_22">在这个问题中，变量是拉格朗日乘子，一个变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>对应于一个样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)；变量的总数等于训练样本容量N。</p><p class="calibre_8">SMO算法是一种启发式算法，其基本思路是：如果所有变量的解都满足此最优化问题的KKT条件（Karush-Kuhn-Tucker conditions），那么这个最优化问题的解就得到了。因为KKT条件是该最优化问题的充分必要条件。否则，选择两个变量，固定其他变量，针对这两个变量构建一个二次规划问题。这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解，因为这会使得原始二次规划问题的目标函数值变得更小。重要的是，这时子问题可以通过解析方法求解，这样就可以大大提高整个算法的计算速度。子问题有两个变量，一个是违反KKT条件最严重的那一个，另一个由约束条件自动确定。如此，SMO算法将原问题不断分解为子问题并对子问题求解，进而达到求解原问题的目的。</p><p class="calibre_8">注意，子问题的两个变量中只有一个是自由变量。假设a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>为两个变量，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>固定，那么由等式约束（7.99）可知</p><p class="calibre_20"><img src="images/00058.jpg" class="calibre_549"/></p><p class="calibre_22">如果a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>确定，那么a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>也随之确定。所以子问题中同时更新两个变量。</p><p class="calibre_8">整个SMO算法包括两个部分：求解两个变量二次规划的解析方法和选择变量的启发式方法。</p><p id="filepos415574" class="calibre_6"><span class="calibre2"><span class="bold">7.4.1　两个变量二次规划的求解方法</span></span></p><p class="calibre_7">不失一般性，假设选择的两个变量是a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，其他变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(i＝3,4,…,N)是固定的。于是SMO的最优化问题（7.98）～（7.100）的子问题可以写成：</p><p class="calibre_20"><img src="images/00020.jpg" class="calibre_550"/></p><p class="calibre_20"><img src="images/00128.jpg" class="calibre_551"/></p><p class="calibre_22">其中，K<sub class="calibre8"><small class="calibre6"><span class="calibre7">ij</span></small></sub>＝K(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>),i，j＝1,2,…,N，<img src="images/00642.jpg" class="calibre_73"/>是常数，目标函数式（7.101）中省略了不含a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的常数项。</p><p class="calibre_8">为了求解两个变量的二次规划问题(7.101)～(7.103)，首先分析约束条件，然后在此约束条件下求极小。</p><p class="calibre_8">由于只有两个变量(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)，约束可以用二维空间中的图形表示（如图7.8所示）。</p><p class="calibre_20"><img src="images/00345.jpg" class="calibre_552"/></p><p class="calibre_20"><span class="calibre4">图7.8　二变量优化问题图示</span></p><p class="calibre_27">不等式约束（7.103）使得(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)在盒子[0,C ]×[0,C ]内，等式约束（7.102）使(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)在平行于盒子[0,C ]× [0,C ]的对角线的直线上。因此要求的是目标函数在一条平行于对角线的线段上的最优值。这使得两个变量的最优化问题成为实质上的单变量的最优化问题，不妨考虑为变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的最优化问题。</p><p class="calibre_8">假设问题（7.101)～(7.103）的初始可行解为<img src="images/00454.jpg" class="calibre_553"/>,<img src="images/00574.jpg" class="calibre_553"/>，最优解为<img src="images/00686.jpg" class="calibre_554"/>,<img src="images/00794.jpg" class="calibre_555"/>，并且假设在沿着约束方向未经剪辑时a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的最优解为<img src="images/00899.jpg" class="calibre_556"/>。</p><p class="calibre_8">由于<img src="images/00794.jpg" class="calibre_555"/>需满足不等式约束（7.103），所以最优值<img src="images/00794.jpg" class="calibre_555"/>的取值范围必须满足条件</p><p class="calibre_20"><img src="images/01008.jpg" class="calibre_557"/></p><p class="calibre_22">其中，L与H是<img src="images/00794.jpg" class="calibre_555"/>所在的对角线段端点的界。如果y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>≠y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>（如图7.8左图所示），则</p><p class="calibre_20"><img src="images/00051.jpg" class="calibre_558"/></p><p class="calibre_22">如果y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>（如图7.8右图所示），则</p><p class="calibre_20"><img src="images/00157.jpg" class="calibre_559"/></p><p class="calibre_24">下面，首先求沿着约束方向未经剪辑即未考虑不等式约束（7.103）时a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的最优解<img src="images/00899.jpg" class="calibre_556"/>；然后再求剪辑后a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的解<img src="images/00794.jpg" class="calibre_555"/>。我们用定理来叙述这个结果。为了叙述简单，记</p><p class="calibre_20"><img src="images/00267.jpg" class="calibre_560"/></p><p class="calibre_22">令</p><p class="calibre_20"><img src="images/00375.jpg" class="calibre_561"/></p><p class="calibre_22">当i＝1,2时，E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为函数g(x)对输入x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的预测值与真实输出y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>之差。</p><p class="calibre_8"><span class="bold">定理7.6　</span>最优化问题（7.101）～（7.103）沿着约束方向未经剪辑时的解是</p><p class="calibre_20"><img src="images/00484.jpg" class="calibre_49"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/00606.jpg" class="calibre_562"/></p><p class="calibre_22">Φ(x)是输入空间到特征空间的映射，E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，i＝1,2，由式（7.105）给出。</p><p class="calibre_8">经剪辑后a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的解是</p><p class="calibre_20"><img src="images/00709.jpg" class="calibre_563"/></p><p class="calibre_22">由<img src="images/00794.jpg" class="calibre_555"/>求得<img src="images/00686.jpg" class="calibre_554"/>是</p><p class="calibre_20"><img src="images/00307.jpg" class="calibre_564"/></p><p class="calibre_24"><span class="bold">证明</span>　引进记号</p><p class="calibre_20"><img src="images/00140.jpg" class="calibre_565"/></p><p class="calibre_22">目标函数可写成</p><p class="calibre_20"><img src="images/01035.jpg" class="calibre_566"/></p><p class="calibre_22">由a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝<img src="images/00642.jpg" class="calibre_73"/>-a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>及<img src="images/00889.jpg" class="calibre_567"/>＝1，可将a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>表示为</p><p class="calibre_20"><img src="images/00184.jpg" class="calibre_568"/></p><p class="calibre_22">代入式（7.110），得到只是a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的函数的目标函数：</p><p class="calibre_20"><img src="images/00292.jpg" class="calibre_569"/></p><p class="calibre_22">对a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>求导数</p><p class="calibre_20"><img src="images/00404.jpg" class="calibre_570"/></p><p class="calibre_22">令其为0，得到</p><p class="calibre_20"><img src="images/00518.jpg" class="calibre_571"/></p><p class="calibre_22">将<img src="images/00642.jpg" class="calibre_73"/>＝<img src="images/00454.jpg" class="calibre_553"/>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+<img src="images/00574.jpg" class="calibre_553"/>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>代入，得到</p><p class="calibre_20"><img src="images/00425.jpg" class="calibre_572"/></p><p class="calibre_22">将<img src="images/00636.jpg" class="calibre_18"/>＝K<sub class="calibre8"><small class="calibre6"><span class="calibre7">11</span></small></sub>+K<sub class="calibre8"><small class="calibre6"><span class="calibre7">22</span></small></sub>-2K<sub class="calibre8"><small class="calibre6"><span class="calibre7">12</span></small></sub>代入，于是得到</p><p class="calibre_20"><img src="images/00975.jpg" class="calibre_573"/></p><p class="calibre_24">要使其满足不等式约束必须将其限制在区间[L，H]内，从而得到<img src="images/00794.jpg" class="calibre_555"/>的表达式（7.108）。由等式约束（7.102），得到<img src="images/00686.jpg" class="calibre_554"/>的表达式（7.109）。于是得到最优化问题（7.101）～（7.103）的解(<img src="images/00686.jpg" class="calibre_554"/>,<img src="images/00794.jpg" class="calibre_555"/>)。</p><p id="filepos424642" class="calibre_6"><span class="calibre2"><span class="bold">7.4.2　变量的选择方法</span></span></p><p class="calibre_7">SMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。</p><p class="calibre_7"><span class="bold">1．第1个变量的选择</span></p><p class="calibre_7">SMO称选择第1个变量的过程为外层循环。外层循环在训练样本中选取违反KKT条件最严重的样本点，并将其对应的变量作为第1个变量。具体地，检验训练样本点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)是否满足KKT条件，即</p><p class="calibre_20"><img src="images/00449.jpg" class="calibre_574"/></p><p class="calibre_20"><img src="images/00272.jpg" class="calibre_575"/></p><p class="calibre_22">其中，<img src="images/00708.jpg" class="calibre_576"/>。</p><p class="calibre_8">该检验是在<img src="images/00860.jpg" class="calibre_76"/>范围内进行的。在检验过程中，外层循环首先遍历所有满足条件0&lt;a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>&lt;C的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件。如果这些样本点都满足KKT条件，那么遍历整个训练集，检验它们是否满足KKT条件。</p><p class="calibre_7"><span class="bold">2．第2个变量的选择</span></p><p class="calibre_7">SMO称选择第2个变量的过程为内层循环。假设在外层循环中已经找到第1个变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，现在要在内层循环中找第2个变量a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>。第2个变量选择的标准是希望能使a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>有足够大的变化。</p><p class="calibre_8">由式（7.106）和式（7.108）可知，<img src="images/00794.jpg" class="calibre_555"/>是依赖于|E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>-E<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>|的，为了加快计算速度，一种简单的做法是选择a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，使其对应的|E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>-E<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>|最大。因为a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>已定，E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>也确定了。如果E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>是正的，那么选择最小的E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>作为E<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>；如果E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>是负的，那么选择最大的E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>作为E<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>。为了节省计算时间，将所有E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>值保存在一个列表中。</p><p class="calibre_8">在特殊情况下，如果内层循环通过以上方法选择的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>不能使目标函数有足够的下降，那么采用以下启发式规则继续选择a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>。遍历在间隔边界上的支持向量点，依次将其对应的变量作为a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>试用，直到目标函数有足够的下降。若找不到合适的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，那么遍历训练数据集；若仍找不到合适的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，则放弃第1个a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，再通过外层循环寻求另外的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>。</p><p class="calibre_7"><span class="bold">3．计算阈值b和差值E<sub class="calibre8"><small class="calibre6"><span class="calibre7"><span class="bold">i</span></span></small></sub></span></p><p class="calibre_7">在每次完成两个变量的优化后，都要重新计算阈值b。当0&lt;<img src="images/00686.jpg" class="calibre_554"/>&lt;C时，由KKT条件（7.112）可知：</p><p class="calibre_20"><img src="images/00363.jpg" class="calibre_577"/></p><p class="calibre_22">于是，</p><p class="calibre_20"><img src="images/00470.jpg" class="calibre_578"/></p><p class="calibre_22">由E<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的定义式（7.105）有</p><p class="calibre_20"><img src="images/00590.jpg" class="calibre_579"/></p><p class="calibre_22">式（7.114）的前两项可写成：</p><p class="calibre_20"><img src="images/00696.jpg" class="calibre_580"/></p><p class="calibre_22">代入式（7.114），可得</p><p class="calibre_20"><img src="images/00808.jpg" class="calibre_581"/></p><p class="calibre_22">同样，如果0&lt;<img src="images/00794.jpg" class="calibre_555"/>&lt;C，那么，</p><p class="calibre_20"><img src="images/00913.jpg" class="calibre_582"/></p><p class="calibre_24">如果<img src="images/00686.jpg" class="calibre_554"/>,<img src="images/00794.jpg" class="calibre_555"/>同时满足条件0&lt;<img src="images/01021.jpg" class="calibre_554"/>&lt;C，i＝1,2，那么<img src="images/00063.jpg" class="calibre_583"/>＝<img src="images/00169.jpg" class="calibre_584"/>。如果<img src="images/00686.jpg" class="calibre_554"/>,<img src="images/00794.jpg" class="calibre_555"/>是0或者C，那么<img src="images/00063.jpg" class="calibre_583"/>和<img src="images/00169.jpg" class="calibre_584"/>以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为b<sup class="calibre5"><small class="calibre6"><span class="calibre7">new</span></small></sup>。</p><p class="calibre_8">在每次完成两个变量的优化之后，还必须更新对应的E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>值，并将它们保存在列表中。E<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>值的更新要用到b<sup class="calibre5"><small class="calibre6"><span class="calibre7">new</span></small></sup>值，以及所有支持向量对应的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>：</p><p class="calibre_20"><img src="images/00276.jpg" class="calibre_585"/></p><p class="calibre_22">其中，S是所有支持向量jx的集合。</p><p id="filepos431442" class="calibre_6"><span class="calibre2"><span class="bold">7.4.3　SMO算法</span></span></p><p class="calibre_7"><span class="bold">算法7.5（SMO算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N，精度<img src="images/00860.jpg" class="calibre_76"/>；</p><p class="calibre_8">输出：近似解<img src="images/00390.jpg" class="calibre_586"/>。</p><p class="calibre_8">（1）取初值a<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0，令k＝0；</p><p class="calibre_8">（2）选取优化变量<img src="images/00501.jpg" class="calibre_587"/>,<img src="images/00618.jpg" class="calibre_588"/>，解析求解两个变量的最优化问题(7.101)～(7.103)，求得最优解<img src="images/00723.jpg" class="calibre_589"/>,<img src="images/00378.jpg" class="calibre_589"/>，更新a为<img src="images/00943.jpg" class="calibre_590"/>；</p><p class="calibre_8">（3）若在精度<img src="images/00860.jpg" class="calibre_76"/>范围内满足停机条件</p><p class="calibre_20"><img src="images/01052.jpg" class="calibre_591"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/00036.jpg" class="calibre_592"/></p><p class="calibre_22">则转（4）；否则令k＝k+1，转（2）；</p><p class="calibre_8">（4）取<img src="images/00390.jpg" class="calibre_586"/>＝<img src="images/00943.jpg" class="calibre_590"/>。</p><p id="filepos433722" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．支持向量机最简单的情况是线性可分支持向量机，或硬间隔支持向量机。构建它的条件是训练数据线性可分。其学习策略是最大间隔法。可以表示为凸二次规划问题，其原始最优化问题为</p><p class="calibre_20"><img src="images/00199.jpg" class="calibre_593"/></p><p class="calibre_22">求得最优化问题的解为w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，得到线性可分支持向量机，分离超平面是</p><p class="calibre_20"><img src="images/00305.jpg" class="calibre_402"/></p><p class="calibre_24">分类决策函数是</p><p class="calibre_20"><img src="images/00418.jpg" class="calibre_594"/></p><p class="calibre_24">最大间隔法中，函数间隔与几何间隔是重要的概念。</p><p class="calibre_8">线性可分支持向量机的最优解存在且唯一。位于间隔边界上的实例点为支持向量。最优分离超平面由支持向量完全决定。</p><p class="calibre_8">二次规划问题的对偶问题是</p><p class="calibre_20"><img src="images/00931.jpg" class="calibre_595"/></p><p class="calibre_24">通常，通过求解对偶问题学习线性可分支持向量机，即首先求解对偶问题的最优值a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，然后求最优值w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>和b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，得出分离超平面和分类决策函数。</p><p class="calibre_8">2．现实中训练数据是线性可分的情形较少，训练数据往往是近似线性可分的，这时使用线性支持向量机，或软间隔支持向量机。线性支持向量机是最基本的支持向量机。</p><p class="calibre_8">对于噪声或例外，通过引入松弛变量<img src="images/01009.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，使其“可分”，得到线性支持向量机学习的凸二次规划问题，其原始最优化问题是</p><p class="calibre_20"><img src="images/00647.jpg" class="calibre_596"/></p><p class="calibre_24">求解原始最优化问题的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，得到线性支持向量机，其分离超平面为</p><p class="calibre_20"><img src="images/00752.jpg" class="calibre_597"/></p><p class="calibre_24">分类决策函数为</p><p class="calibre_20"><img src="images/00859.jpg" class="calibre_598"/></p><p class="calibre_24">线性可分支持向量机的解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>唯一但b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>不唯一。</p><p class="calibre_8">对偶问题是</p><p class="calibre_20"><img src="images/00969.jpg" class="calibre_599"/></p><p class="calibre_24">线性支持向量机的对偶学习算法，首先求解对偶问题得到最优解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，然后求原始问题最优解w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>和b<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，得出分离超平面和分类决策函数。</p><p class="calibre_8">对偶问题的解a<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>中满足<img src="images/00955.jpg" class="calibre_449"/>&gt;0的实例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>称为支持向量。支持向量可在间隔边界上，也可在间隔边界与分离超平面之间，或者在分离超平面误分一侧。最优分离超平面由支持向量完全决定。</p><p class="calibre_8">线性支持向量机学习等价于最小化二阶范数正则化的合页函数</p><p class="calibre_20"><img src="images/00010.jpg" class="calibre_600"/></p><p class="calibre_24">3．非线性支持向量机</p><p class="calibre_8">对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个高维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都只涉及实例与实例之间的内积，所以不需要显式地指定非线性变换，而是用核函数来替换当中的内积。核函数表示，通过一个非线性转换后的两个实例间的内积。具体地，K(x,z)是一个核函数，或正定核，意味着存在一个从输入空间x到特征空间<img src="images/00991.jpg" class="calibre_322"/>的映射Ø(x):<img src="images/00991.jpg" class="calibre_322"/>→<img src="images/00911.jpg" class="calibre_322"/>，对任意x,z∊<img src="images/00991.jpg" class="calibre_322"/>，有</p><p class="calibre_20"><img src="images/00118.jpg" class="calibre_601"/></p><p class="calibre_22">对称函数K(x,z)为正定核的充要条件如下：对任意x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00991.jpg" class="calibre_322"/>，i＝1,2,…,m，任意正整数m，对称函数K(x,z)对应的Gram矩阵是半正定的。</p><p class="calibre_8">所以，在线性支持向量机学习的对偶问题中，用核函数K(x,z)替代内积，求解得到的就是非线性支持向量机</p><p class="calibre_20"><img src="images/00228.jpg" class="calibre_602"/></p><p class="calibre_24">4．SMO算法</p><p class="calibre_8">SMO算法是支持向量机学习的一种快速算法，其特点是不断地将原二次规划问题分解为只有两个变量的二次规划子问题，并对子问题进行解析求解，直到所有变量满足KKT条件为止。这样通过启发式的方法得到原二次规划问题的最优解。因为子问题有解析解，所以每次计算子问题都很快，虽然计算子问题次数很多，但在总体上还是高效的。</p><p id="filepos440339" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">线性支持向量机（软间隔）由Cortes与Vapnik提出<sup class="calibre5"><small class="calibre6"><span class="calibre7">[1]</span></small></sup>。同时，Boser,Guyon与Vapnik又引入核技巧，提出非线性支持向量机<sup class="calibre5"><small class="calibre6"><span class="calibre7">[2]</span></small></sup>。Drucker等人将其扩展到支持向量回归<sup class="calibre5"><small class="calibre6"><span class="calibre7">[3]</span></small></sup>。Vapnik Vladimir在他的统计学习理论<sup class="calibre5"><small class="calibre6"><span class="calibre7">[4]</span></small></sup>一书中对支持向量机的泛化能力进行了论述。</p><p class="calibre_8">Platt提出了支持向量机的快速学习算法SMO<sup class="calibre5"><small class="calibre6"><span class="calibre7">[5]</span></small></sup>，Joachims实现的SVM Light，以及Chang与Lin实现的LIBSVM软件包被广泛使用。<sup class="calibre5"><small id="filepos441188" class="calibre6"><a href="#filepos446403"><span class="calibre7">[2]</span></a></small></sup></p><p class="calibre_8">原始的支持向量机是二类分类模型，又被推广到多类分类支持向量机<sup class="calibre5"><small class="calibre6"><span class="calibre7">[6,7]</span></small></sup>，以及用于结构预测的结构支持向量机<sup class="calibre5"><small class="calibre6"><span class="calibre7">[8]</span></small></sup>。</p><p class="calibre_8">关于支持向量机的文献很多。支持向量机的介绍可参照文献[9～12]。核方法被认为是比支持向量机更具一般性的机器学习方法。核方法的介绍可参考文献[13～15]。</p><p id="filepos441788" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">1.1　比较感知机的对偶形式与线性可分支持向量机的对偶形式。</p><p class="calibre_8">1.2　已知正例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(1,2)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(2,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，负例点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>＝(2,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">5</span></small></sub>＝(3,2)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试求最大间隔分离超平面和分类决策函数，并在图上画出分离超平面、间隔边界及支持向量。</p><p class="calibre_8">1.3　线性支持向量机还可以定义为以下形式：</p><p class="calibre_20"><img src="images/00337.jpg" class="calibre_603"/></p><p class="calibre_24">试求其对偶形式。</p><p class="calibre_8">1.4　证明内积的正整数幂函数：</p><p class="calibre_20"><img src="images/00444.jpg" class="calibre_604"/></p><p class="calibre_24">是正定核函数，这里p是正整数，x,z∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>。</p><p id="filepos443310" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Cortes C,Vapnik V. Support-vector networks. Machine Learning,1995,20</p><p class="calibre_8">[2]　Boser BE,Guyon IM,Vapnik VN. A training algorithm for optimal margin classifiers. In: Haussler D,ed. Proc of the 5th Annual ACM Workshop on COLT. Pittsburgh,PA,1992,144–152</p><p class="calibre_8">[3]　Drucker H,Burges CJC,Kaufman L,Smola A,Vapnik V. Support vector regression machines. In: Advances in Neural Information Processing Systems 9,NIPS 1996. MIT Press,155–161</p><p class="calibre_8">[4]　Vapnik Vladimir N. The Nature of Statistical Learning Theory. Berlin: Springer-Verlag,1995（中译本：张学工，译。统计学习理论的本质。北京：清华大学出版社，2000）</p><p class="calibre_8">[5]　Platt JC. Fast training of support vector machines using sequential minimal optimization. Microsoft Research,http://research.microsoft.com/apps/pubs/?id=68391</p><p class="calibre_8">[6]　Weston JAE,Watkins C. Support vector machines for multi-class pattern recognition. In: Proceedings of the 7th European Symposium on Articial Neural Networks. 1999</p><p class="calibre_8">[7]　Crammer K,Singer Y. On the algorithmic implementation of multiclass kernel-based machines. Journal of Machine Learning Research,2001,2(Dec): 265–292</p><p class="calibre_8">[8]　Tsochantaridis I,Joachims T,Hofmann T,Altun Y. Large margin methods for structured and interdependent output variables. JMLR,2005,6: 1453–1484</p><p class="calibre_8">[9]　Burges JC. A tutorial on support vector machines for pattern recognition. Bell Laboratories,Lucent Technologies. 1997</p><p class="calibre_8">[10]　Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines and Othre KernerBased Learning Methods. Cambridge University Press，2000（中译本：李国正，等译。支持向量机导论。北京：电子工业出版社，2004）</p><p class="calibre_8">[11]　邓乃扬，田英杰。数据挖掘中的新方法——支持向量机。北京：科学出版社，2004</p><p class="calibre_8">[12]　邓乃扬，田英杰。支持向量机——理论，算法与拓展。北京：科学出版社，2009</p><p class="calibre_8">[13]　Scholkpf B,Smola AJ. Learning with Kernels: Support Vector Machines,Regularization,Optimization,and Beyond. MIT Press,2002</p><p class="calibre_8">[14]　Herbrich R. Learning Kernel Classifiers,Theory and Algorithms. The MIT Press,2002</p><p class="calibre_8">[15]　Hofmann T,Scholkopf B,Smola AJ. Kernel methods in machine learning. The Annals of Statistics,2008,36(3): 1171–1220</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos446165" class="calibre_99"><a href="#filepos326859"><span class="calibre4">[1]</span></a><span class="calibre4">　f(x)称为仿射函数，如果它满足f(x)＝a·x+b，a∊R<sup class="calibre10"><small class="calibre6"><span class="calibre7">n</span></small></sup>，b∊R，x∊Rn。</span></p><p id="filepos446403" class="calibre_8"><a href="#filepos441188"><span class="calibre4">[2]</span></a><span class="calibre4">　SVM Light：http://svmlight.joachims.org/.LIBSVM：http://www.csie.ntu.edu.tw/~cjlin/libsvm/。</span></p><div class="mbp_pagebreak" id="calibre_pb_12"></div>
</body></html>
