<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos197056" class="calibre_"><span class="calibre1"><span class="bold">第5章　决策树</span></span></p><p class="calibre_13">决策树（decision tree）是一种基本的分类与回归方法。本章主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。预测时，对新的数据，利用决策树模型进行分类。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的修剪。这些决策树学习的思想主要来源于由Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及由Breiman等人在1984年提出的CART算法。</p><p class="calibre_8">本章首先介绍决策树的基本概念，然后通过ID3和C4.5介绍特征的选择、决策树的生成以及决策树的修剪，最后介绍CART算法。</p><p id="filepos198202" class="calibre_6"><span class="calibre9"><span class="bold">5.1　决策树模型与学习</span></span></p><p id="filepos198308" class="calibre_6"><span class="calibre2"><span class="bold">5.1.1　决策树模型</span></span></p><p class="calibre_7"><span class="bold">定义5.1（决策树）　</span>分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点有两种类型：内部结点（internal node）和叶结点（leaf node）。内部结点表示一个特征或属性，叶结点表示一个类。</p><p class="calibre_8">用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值。如此递归地对实例进行测试并分配，直至达到叶结点。最后将实例分到叶结点的类中。</p><p class="calibre_8">图5.1是一个决策树的示意图。图中圆和方框分别表示内部结点和叶结点。</p><p class="calibre_20"><img src="images/00049.jpg" class="calibre_213"/></p><p class="calibre_20"><span class="calibre4">图5.1　决策树模型</span></p><p id="filepos199452" class="calibre_6"><span class="calibre2"><span class="bold">5.1.2　决策树与if-then规则</span></span></p><p class="calibre_7">可以将决策树看成一个if-then规则的集合。将决策树转换成if-then规则的过程是这样的：由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的if-then规则集合具有一个重要的性质：互斥并且完备。这就是说，每一个实例都被一条路径或一条规则所覆盖，而且只被一条路径或一条规则所覆盖。这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件。</p><p id="filepos200211" class="calibre_6"><span class="calibre2"><span class="bold">5.1.3　决策树与条件概率分布</span></span></p><p class="calibre_7">决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition）上。将特征空间划分为互不相交的单元（cell）或区域（region），并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为P(Y|X)。X取值于给定划分下单元的集合，Y取值于类的集合。各叶结点（单元）上的条件概率往往偏向某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。</p><p class="calibre_8">图5.2（a）示意地表示了特征空间的一个划分。图中的大正方形表示特征空间。这个大正方形被若干个小矩形分割，每个小矩形表示一个单元。特征空间划分上的单元构成了一个集合，X取值为单元的集合。为简单起见，假设只有两类：正类和负类，即Y取值为+1和–1。小矩形中的数字表示单元的类。图5.2（b）示意地表示特征空间划分确定时，特征（单元）给定条件下类的条件概率分布。图5.2（b）中条件概率分布对应于图5.2（a）的划分。当某个单元c的条件概率满足P(Y＝+1|X＝c)&gt;0.5时，则认为这个单元属于正类，即落在这个单元的实例都被视为正例。图5.2（c）为对应于图5.2（b）中条件概率分布的决策树。</p><p class="calibre_20"><img src="images/00053.jpg" class="calibre_214"/></p><p class="calibre_20"><img src="images/00054.jpg" class="calibre_215"/></p><p class="calibre_20"><img src="images/00059.jpg" class="calibre_216"/></p><p class="calibre_20"><span class="calibre4">图5.2　决策树对应于条件概率分布</span></p><p id="filepos202516" class="calibre_6"><span class="calibre2"><span class="bold">5.1.4　决策树学习</span></span></p><p class="calibre_7">决策树学习，假设给定训练数据集</p><p class="calibre_20"><img src="images/00062.jpg" class="calibre_217"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝(<img src="images/00214.jpg" class="calibre_91"/>,<img src="images/00068.jpg" class="calibre_218"/>,…,<img src="images/00072.jpg" class="calibre_219"/>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>为输入实例（特征向量）,n为特征个数，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊{1,2,…,K}为类标记，i＝1,2,…,N，N为样本容量．学习的目标是根据给定的训练数据集构建一个决策树模型，使它能够对实例进行正确的分类。</p><p class="calibre_8">决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。从另一个角度看，决策树学习是由训练数据集估计条件概率模型。基于特征空间划分的类的条件概率模型有无穷多个。我们选择的条件概率模型应该不仅对训练数据有很好的拟合，而且对未知数据有很好的预测。</p><p class="calibre_8">决策树学习用损失函数表示这一目标。如下所述，决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是以损失函数为目标函数的最小化。</p><p class="calibre_8">当损失函数确定以后，学习问题就变为在损失函数意义下选择最优决策树的问题。因为从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优（sub-optimal）的。</p><p class="calibre_8">决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。</p><p class="calibre_8">以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><p class="calibre_8">如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。</p><p class="calibre_8">可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。</p><p class="calibre_8">决策树学习常用的算法有ID3、C4.5与CART，下面结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。</p><p id="filepos207037" class="calibre_6"><span class="calibre9"><span class="bold">5.2　特征选择</span></span></p><p id="filepos207131" class="calibre_6"><span class="calibre2"><span class="bold">5.2.1　特征选择问题</span></span></p><p class="calibre_7">特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的准则是信息增益或信息增益比。</p><p class="calibre_8">首先通过一个例子来说明特征选择问题。</p><p class="calibre_8">例5.1<sup class="calibre5"><small id="filepos207774" class="calibre6"><a href="#filepos261718"><span class="calibre7">[1]</span></a></small></sup>　表5.1是一个由15个样本组成的贷款申请训练数据。数据包括贷款申请人的4个特征（属性）：第1个特征是年龄，有3个可能值：青年，中年，老年；第2个特征是有工作，有2个可能值：是，否；第3个特征是有自己的房子，有2个可能值：是，否；第4个特征是信贷情况，有3个可能值：非常好，好，一般。表的最后一列是类别，是否同意贷款，取2个值：是，否。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表5.1　贷款申请样本数据表</span></span></p><p class="calibre_20"><img src="images/00075.jpg" class="calibre_220"/></p><p class="calibre_24">希望通过所给的训练数据学习一个贷款申请的决策树，用以对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p><p class="calibre_8">特征选择是决定用哪个特征来划分特征空间。</p><p class="calibre_8">图5.3表示从表5.1数据学习到的两个可能的决策树，分别由两个不同特征的根结点构成。图5.3（a）所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。图5.3（b）所示的根结点的特征是有工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益（information gain）就能够很好地表示这一直观的准则。</p><p class="calibre_20"><img src="images/00079.jpg" class="calibre_221"/></p><p class="calibre_20"><span class="calibre4">图5.3　不同特征决定的不同决策树</span></p><p id="filepos209959" class="calibre_6"><span class="calibre2"><span class="bold">5.2.2　信息增益</span></span></p><p class="calibre_7">为了便于说明，先给出熵与条件熵的定义。</p><p class="calibre_8">在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为</p><p class="calibre_20"><img src="images/00083.jpg" class="calibre_222"/></p><p class="calibre_22">则随机变量X的熵定义为</p><p class="calibre_20"><img src="images/00087.jpg" class="calibre_223"/></p><p class="calibre_22">在式（5.1）中，若p<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝0，则定义0log0＝0。通常，式（5.1）中的对数以2为底或以e为底（自然对数），这时熵的单位分别称作比特（bit）或纳特（nat）。由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作H(p)，即</p><p class="calibre_20"><img src="images/00091.jpg" class="calibre_224"/></p><p class="calibre_24">熵越大，随机变量的不确定性就越大。从定义可验证</p><p class="calibre_20"><img src="images/00093.jpg" class="calibre_225"/></p><p class="calibre_24">当随机变量只取两个值，例如1，0时，即X的分布为</p><p class="calibre_20"><img src="images/00097.jpg" class="calibre_226"/></p><p class="calibre_22">熵为</p><p class="calibre_20"><img src="images/00101.jpg" class="calibre_227"/></p><p class="calibre_22">这时，熵H(p)随概率p变化的曲线如图5.4所示（单位为比特）。</p><p class="calibre_20"><img src="images/00103.jpg" class="calibre_228"/></p><p class="calibre_20"><span class="calibre4">图5.4　分布为贝努利分布时熵与概率的关系</span></p><p class="calibre_27">当p＝0或p＝1时H(p)＝0，随机变量完全没有不确定性。当p＝0.5时，H(p)＝1，熵取值最大，随机变量不确定性最大。</p><p class="calibre_8">设有随机变量(X,Y)，其联合概率分布为</p><p class="calibre_20"><img src="images/00106.jpg" class="calibre_229"/></p><p class="calibre_22">条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy）H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望</p><p class="calibre_20"><img src="images/00109.jpg" class="calibre_230"/></p><p class="calibre_22">这里，p<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝P(X＝x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)，i＝1,2,…,n。</p><p class="calibre_8">当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。此时，如果有0概率，令0log0＝0。</p><p class="calibre_8">信息增益（information gain）表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p><p class="calibre_8"><span class="bold">定义5.2（信息增益）</span>　特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即</p><p class="calibre_20"><img src="images/00114.jpg" class="calibre_231"/></p><p class="calibre_24">一般地，熵H(Y)与条件熵H(Y|X)之差称为互信息（mutual information）。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p class="calibre_8">决策树学习应用信息增益准则选择特征。给定训练数据集D和特征A，经验熵H(D)表示对数据集D进行分类的不确定性。而经验条件熵H(D|A)表示在特征A给定的条件下对数据集D进行分类的不确定性。那么它们的差，即信息增益，就表示由于特征A而使得对数据集D的分类的不确定性减少的程度。显然，对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益。信息增益大的特征具有更强的分类能力。</p><p class="calibre_8">根据信息增益准则的特征选择方法是：对训练数据集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。</p><p class="calibre_8">设训练数据集为D，|D|表示其样本容量，即样本个数。设有K个类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，k＝1,2,…,K，|C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>|为属于类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>的样本个数，<img src="images/00116.jpg" class="calibre_232"/>。设特征A有n个不同的取值{a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>}，根据特征A的取值将D划分为n个子集D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,D<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>，|D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>|为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的样本个数，<img src="images/01069.jpg" class="calibre_233"/>。记子集D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>中属于类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>的样本的集合为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">ik</span></small></sub>，即D<sub class="calibre8"><small class="calibre6"><span class="calibre7">ik</span></small></sub>＝D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>⋂C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，|D<sub class="calibre8"><small class="calibre6"><span class="calibre7">ik</span></small></sub>|为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">ik</span></small></sub>的样本个数。于是信息增益的算法如下：</p><p class="calibre_146"><span class="bold">算法5.1（信息增益的算法）</span></p><p class="calibre_146">输入：训练数据集D和特征A；</p><p class="calibre_8">输出：特征A对训练数据集D的信息增益g(D,A)。</p><p class="calibre_8">（1）计算数据集D的经验熵H(D)</p><p class="calibre_20"><img src="images/00122.jpg" class="calibre_234"/></p><p class="calibre_24">（2）计算特征A对数据集D的经验条件熵H(D|A)</p><p class="calibre_20"><img src="images/00126.jpg" class="calibre_235"/></p><p class="calibre_24">（3）计算信息增益</p><p class="calibre_20"><img src="images/00130.jpg" class="calibre_236"/></p><p class="calibre_24"><span class="bold">例5.2　</span>对表5.1所给的训练数据集D，根据信息增益准则选择最优特征。</p><p class="calibre_8"><span class="bold">解</span>　首先计算经验熵H(D)。</p><p class="calibre_20"><img src="images/00236.jpg" class="calibre_237"/></p><p class="calibre_24">然后计算各特征对数据集D的信息增益。分别以A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>表示年龄、有工作、有自己的房子和信贷情况4个特征，则</p><p class="calibre_20"><img src="images/00343.jpg" class="calibre_238"/></p><p class="calibre_22">这里D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，D<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>分别是D中A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>（年龄）取值为青年、中年和老年的样本子集。类似地，</p><p class="calibre_20"><img src="images/00534.jpg" class="calibre_239"/></p><p class="calibre_20"><img src="images/00275.jpg" class="calibre_240"/></p><p class="calibre_24">最后，比较各特征的信息增益值。由于特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>（有自己的房子）的信息增益值最大，所以选择特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>作为最优特征。</p><p id="filepos218750" class="calibre_6"><span class="calibre2"><span class="bold">5.2.3　信息增益比</span></span></p><p class="calibre_7">信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比（information gain ratio）可以对这一问题进行校正。这是特征选择的另一准则。</p><p class="calibre_8"><span class="bold">定义5.3（信息增益比）</span>　特征A对训练数据集D的信息增益比g<sub class="calibre8"><small class="calibre6"><span class="calibre7">R</span></small></sub>(D,A)定义为其信息增益g(D,A)与训练数据集D的经验熵H(D)之比：</p><p class="calibre_20"><img src="images/00657.jpg" class="calibre_241"/></p><p id="filepos219597" class="calibre_6"><span class="calibre9"><span class="bold">5.3　决策树的生成</span></span></p><p class="calibre_7">本节将介绍决策树学习的生成算法。首先介绍ID3的生成算法，然后再介绍C4.5中的生成算法。这些都是决策树学习的经典算法。</p><p id="filepos219895" class="calibre_6"><span class="calibre2"><span class="bold">5.3.1　ID3算法</span></span></p><p class="calibre_7">ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点（root node）开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。</p><p class="calibre_146"><span class="bold">算法5.2（ID3算法）</span></p><p class="calibre_146">输入：训练数据集D，特征集A，阈值<img src="images/00860.jpg" class="calibre_76"/>；</p><p class="calibre_8">输出：决策树T。</p><p class="calibre_8">（1）若D中所有实例属于同一类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，则T为单结点树，并将类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类标记，返回T；</p><p class="calibre_8">（2）若A＝Ø，则T为单结点树，并将D中实例数最大的类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类标记，返回T；</p><p class="calibre_8">（3）否则，按算法5.1计算A中各特征对D的信息增益，选择信息增益最大的特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>；</p><p class="calibre_8">（4）如果A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>的信息增益小于阈值<img src="images/00860.jpg" class="calibre_76"/>，则置T为单结点树，并将D中实例数最大的类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类标记，返回T；</p><p class="calibre_8">（5）否则，对A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>的每一可能值a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，依A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>将D分割为若干非空子集D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，将D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；</p><p class="calibre_8">（6）对第i个子结点，以D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为训练集，以A-{A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>}为特征集，递归地调用步（1）～步（5），得到子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，返回T<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。</p><p class="calibre_8"><span class="bold">例5.3</span>　对表5.1的训练数据集，利用ID3算法建立决策树。</p><p class="calibre_8"><span class="bold">解</span>　利用例5.2的结果，由于特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>（有自己的房子）的信息增益值最大，所以选择特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>作为根结点的特征。它将训练数据集D划分为两个子集D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>（A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>取值为“是”）和D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>（A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>取值为“否”）。由于D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。</p><p class="calibre_8">对D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>则需从特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>（年龄），A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>（有工作）和A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>（信贷情况）中选择新的特征。计算各个特征的信息增益：</p><p class="calibre_20"><img src="images/00676.jpg" class="calibre_242"/></p><p class="calibre_22">选择信息增益最大的特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>（有工作）作为结点的特征。由于A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>有两个可能取值，从这一结点引出两个子结点：一个对应“是”（有工作）的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为“是”；另一个是对应“否”（无工作）的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为“否”。</p><p class="calibre_8">这样生成一个如图5.5所示的决策树。该决策树只用了两个特征（有两个内部结点）。</p><p class="calibre_20"><img src="images/00289.jpg" class="calibre_243"/></p><p class="calibre_20"><span class="calibre4">图5.5　决策树的生成</span></p><p class="calibre_27">ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。</p><p id="filepos225055" class="calibre_6"><span class="calibre2"><span class="bold">5.3.2　C4.5的生成算法</span></span></p><p class="calibre_7">C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。</p><p class="calibre_146"><span class="bold">算法5.3（C4.5的生成算法）</span></p><p class="calibre_146">输入：训练数据集D，特征集A，阈值<img src="images/00860.jpg" class="calibre_76"/>；</p><p class="calibre_8">输出：决策树T。</p><p class="calibre_8">（1）如果D中所有实例属于同一类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，则置T为单结点树，并将C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类，返回T；</p><p class="calibre_8">（2）如果A＝Ø，则置T为单结点树，并将D中实例数最大的类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类，返回T；</p><p class="calibre_8">（3）否则，按式（5.10）计算A中各特征对D的信息增益比，选择信息增益比最大的特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>；</p><p class="calibre_8">（4）如果A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>的信息增益比小于阈值<img src="images/00860.jpg" class="calibre_76"/>，则置T为单结点树，并将D中实例数最大的类C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>作为该结点的类，返回T；</p><p class="calibre_8">（5）否则，对A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>的每一可能值a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，依A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>将D分割为子集若干非空D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，将D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T；</p><p class="calibre_8">（6）对结点i，以D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为训练集，以A-{A<sub class="calibre8"><small class="calibre6"><span class="calibre7">g</span></small></sub>}为特征集，递归地调用步(1)～步(5)，得到子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，返回T<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。</p><p id="filepos227437" class="calibre_6"><span class="calibre9"><span class="bold">5.4　决策树的剪枝</span></span></p><p class="calibre_7">决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。</p><p class="calibre_8">在决策树学习中将已生成的树进行简化的过程称为剪枝（pruning）。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。</p><p class="calibre_8">本节介绍一种简单的决策树学习的剪枝算法。</p><p class="calibre_8">决策树的剪枝往往通过极小化决策树整体的损失函数（loss function）或代价函数（cost function）来实现。设树T的叶结点个数为||T，t是树T的叶结点，该叶结点有N<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>个样本点，其中k类的样本点有N<sub class="calibre8"><small class="calibre6"><span class="calibre7">tk</span></small></sub>个，k＝1,2,…,K，H<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>(T)为叶结点t上的经验熵，a≥0为参数，则决策树学习的损失函数可以定义为</p><p class="calibre_20"><img src="images/00293.jpg" class="calibre_244"/></p><p class="calibre_22">其中经验熵为</p><p class="calibre_20"><img src="images/00047.jpg" class="calibre_245"/></p><p class="calibre_24">在损失函数中，将式（5.11）右端的第1项记作</p><p class="calibre_20"><img src="images/00760.jpg" class="calibre_246"/></p><p class="calibre_22">这时有</p><p class="calibre_20"><img src="images/00262.jpg" class="calibre_247"/></p><p class="calibre_22">式（5.14）中，C(T)表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数a≥0控制两者之间的影响。较大的a促使选择较简单的模型（树），较小的a促使选择较复杂的模型（树）。a＝0意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。</p><p class="calibre_8">剪枝，就是当a确定时，选择损失函数最小的模型，即损失函数最小的子树。当a值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越高；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡。</p><p class="calibre_8">可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。</p><p class="calibre_8">式（5.11）或式（5.14）定义的损失函数的极小化等价于正则化的极大似然估计。所以，利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择。</p><p class="calibre_8">图5.6是决策树剪枝过程的示意图。下面介绍剪枝算法。</p><p class="calibre_20"><img src="images/00306.jpg" class="calibre_248"/></p><p class="calibre_20"><span class="calibre4">图5.6　决策树的剪枝</span></p><p class="calibre_27"><span class="bold">算法5.4（树的剪枝算法）</span></p><p class="calibre_146">输入：生成算法产生的整个树T，参数a；</p><p class="calibre_8">输出：修剪后的子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。</p><p class="calibre_8">（1）计算每个结点的经验熵。</p><p class="calibre_8">（2）递归地从树的叶结点向上回缩。</p><p class="calibre_8">设一组叶结点回缩到其父结点之前与之后的整体树分别为T<sub class="calibre8"><small class="calibre6"><span class="calibre7">B</span></small></sub>与T<sub class="calibre8"><small class="calibre6"><span class="calibre7">A</span></small></sub>，其对应的损失函数值分别是C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T<sub class="calibre8"><small class="calibre6"><span class="calibre7">B</span></small></sub>)与C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T<sub class="calibre8"><small class="calibre6"><span class="calibre7">A</span></small></sub>)，如果</p><p class="calibre_20"><img src="images/00826.jpg" class="calibre_249"/></p><p class="calibre_22">则进行剪枝，即将父结点变为新的叶结点。</p><p class="calibre_8">（3）返回（2），直至不能继续为止，得到损失函数最小的子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。</p><p class="calibre_8">注意，式（5.15）只需考虑两个树的损失函数的差，其计算可以在局部进行。所以，决策树的剪枝算法可以由一种动态规划的算法实现。类似的动态规划算法可参见文献[10]。</p><p id="filepos232793" class="calibre_6"><span class="calibre9"><span class="bold">5.5　CART算法</span></span></p><p class="calibre_7">分类与回归树（classification and regression tree，CART）模型由Breiman等人在1984年提出，是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。以下将用于分类与回归的树统称为决策树。</p><p class="calibre_8">CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p><p class="calibre_8">CART算法由以下两步组成：</p><p class="calibre_8">（1）决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；</p><p class="calibre_8">（2）决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。</p><p id="filepos234137" class="calibre_6"><span class="calibre2"><span class="bold">5.5.1　CART生成</span></span></p><p class="calibre_7">决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。</p><p class="calibre_7"><span class="bold">1．回归树的生成</span></p><p class="calibre_7">假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集</p><p class="calibre_20"><img src="images/00601.jpg" class="calibre_250"/></p><p class="calibre_22">考虑如何生成回归树。</p><p class="calibre_8">一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,R<sub class="calibre8"><small class="calibre6"><span class="calibre7">M</span></small></sub>，并且在每个单元R<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>上有一个固定的输出值c<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，于是回归树模型可表示为</p><p class="calibre_20"><img src="images/00867.jpg" class="calibre_251"/></p><p class="calibre_24">当输入空间的划分确定时，可以用平方误差<img src="images/00323.jpg" class="calibre_252"/>来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。易知，单元R<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>上的c<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的最优值<img src="images/00326.jpg" class="calibre_31"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>是R<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>上的所有输入实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>对应的输出y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的均值，即</p><p class="calibre_20"><img src="images/00330.jpg" class="calibre_253"/></p><p class="calibre_24">问题是怎样对输入空间进行划分。这里采用启发式的方法，选择第j个变量x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(j)</span></small></sup>和它取的值s，作为切分变量（splitting variable）和切分点（splitting point），并定义两个区域：</p><p class="calibre_20"><img src="images/00956.jpg" class="calibre_254"/></p><p class="calibre_22">然后寻找最优切分变量j和最优切分点s。具体地，求解</p><p class="calibre_20"><img src="images/00338.jpg" class="calibre_255"/></p><p class="calibre_22">对固定输入变量j可以找到最优切分点s。</p><p class="calibre_20"><img src="images/00997.jpg" class="calibre_256"/></p><p class="calibre_22">遍历所有输入变量，找到最优的切分变量j，构成一个对(j,s)。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树（least squares regression tree），现将算法叙述如下：</p><p class="calibre_146"><span class="bold">算法5.5（最小二乘回归树生成算法）</span></p><p class="calibre_146">输入：训练数据集D；</p><p class="calibre_8">输出：回归树f(x)。</p><p class="calibre_8">在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p><p class="calibre_8">（1）选择最优切分变量j与切分点s，求解</p><p class="calibre_20"><img src="images/00344.jpg" class="calibre_257"/></p><p class="calibre_24">遍历变量j，对固定的切分变量j扫描切分点s，选择使式（5.21）达到最小值的对（j,s）。</p><p class="calibre_8">（2）用选定的对（j,s）划分区域并决定相应的输出值：</p><p class="calibre_20"><img src="images/01041.jpg" class="calibre_258"/></p><p class="calibre_24">（3）继续对两个子区域调用步骤（1），（2），直至满足停止条件。</p><p class="calibre_8">（4）将输入空间划分为M个区域R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…R<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，生成决策树：</p><p class="calibre_20"><img src="images/00351.jpg" class="calibre_259"/></p><p class="calibre_7"><span class="bold">2．分类树的生成</span></p><p class="calibre_7">分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p><p class="calibre_8"><span class="bold">定义5.4（基尼指数）</span>　分类问题中，假设有K个类，样本点属于第k类的概率为p<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，则概率分布的基尼指数定义为</p><p class="calibre_20"><img src="images/00357.jpg" class="calibre_260"/></p><p class="calibre_22">对于二类分类问题，若样本点属于第1个类的概率是p，则概率分布的基尼指数为</p><p class="calibre_20"><img src="images/00361.jpg" class="calibre_261"/></p><p class="calibre_24">对于给定的样本集合D，其基尼指数为</p><p class="calibre_20"><img src="images/00365.jpg" class="calibre_262"/></p><p class="calibre_22">这里，C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>是D中属于第k类的样本子集，K是类的个数。</p><p class="calibre_8">如果样本集合D根据特征A是否取某一可能值a被分割成D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>两部分，即</p><p class="calibre_20"><img src="images/00084.jpg" class="calibre_263"/></p><p class="calibre_22">则在特征A的条件下，集合D的基尼指数定义为</p><p class="calibre_20"><img src="images/00371.jpg" class="calibre_264"/></p><p class="calibre_22">基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A＝a分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。</p><p class="calibre_8">图5.7显示二类分类问题中基尼指数Gini(p)、熵（单位比特）之半<img src="images/00811.jpg" class="calibre_61"/>H(p)和分类误差率的关系。横坐标表示概率p，纵坐标表示损失。可以看出基尼指数和熵之半的曲线很接近，都可以近似地代表分类误差率。</p><p class="calibre_20"><img src="images/00374.jpg" class="calibre_265"/></p><p class="calibre_20"><span class="calibre4">图5.7　二类分类中基尼指数、熵之半和分类误差率的关系</span></p><p class="calibre_27"><span class="bold">算法5.6（CART生成算法）</span></p><p class="calibre_146">输入：训练数据集D，停止计算的条件；</p><p class="calibre_8">输出：CART决策树。</p><p class="calibre_8">根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：</p><p class="calibre_8">（1）设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A＝a的测试为“是”或“否”将D分割成D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>两部分，利用式（5.25）计算A＝a时的基尼指数。</p><p class="calibre_8">（2）在所有可能的特征A以及它们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</p><p class="calibre_8">（3）对两个子结点递归地调用（1），（2），直至满足停止条件。</p><p class="calibre_8">（4）生成CART决策树。</p><p class="calibre_8">算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。</p><p class="calibre_8"><span class="bold">例5.4</span>　根据表5.1所给训练数据集，应用CART算法生成决策树。</p><p class="calibre_8"><span class="bold">解</span>　首先计算各特征的基尼指数，选择最优特征以及其最优切分点。仍采用例5.2的记号，分别以A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>表示年龄、有工作、有自己的房子和信贷情况4个特征，并以1，2，3表示年龄的值为青年、中年和老年，以1，2表示有工作和有自己的房子的值为是和否，以1，2，3表示信贷情况的值为非常好、好和一般。</p><p class="calibre_8">求特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的基尼指数:</p><p class="calibre_20"><img src="images/00144.jpg" class="calibre_266"/></p><p class="calibre_24">由于Gini(D，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝1)和Gini(D，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝3)相等，且最小，所以A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝1和A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝3都可以选作A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的最优切分点。</p><p class="calibre_8">求特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>和A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>的基尼指数：</p><p class="calibre_20"><img src="images/00167.jpg" class="calibre_267"/></p><p class="calibre_24">由于A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>和A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>只有一个切分点，所以它们就是最优切分点。</p><p class="calibre_8">求特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>的基尼指数：</p><p class="calibre_20"><img src="images/00385.jpg" class="calibre_268"/></p><p class="calibre_24">Gini(D，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>＝3)最小，所以A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>＝3为A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>的最优切分点。</p><p class="calibre_8">在A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>几个特征中，Gini(D，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝1)＝0.27最小，所以选择特征A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>为最优特征，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝1为其最优切分点。于是根结点生成两个子结点，一个是叶结点。对另一个结点继续使用以上方法在A<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，A<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>中选择最优特征及其最优切分点，结果是A<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝1。依此计算得知，所得结点都是叶结点。</p><p class="calibre_8">对于本问题，按照CART算法所生成的决策树与按照ID3算法所生成的决策树完全一致。</p><p id="filepos246454" class="calibre_6"><span class="calibre2"><span class="bold">5.5.2　CART剪枝</span></span></p><p class="calibre_7">CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测。CART剪枝算法由两步组成：首先从生成算法产生的决策树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>底端开始不断剪枝，直到T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>的根结点，形成一个子树序列{T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>}；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。</p><p class="calibre_7"><span class="bold">1．剪枝，形成一个子树序列</span></p><p class="calibre_7">在剪枝过程中，计算子树的损失函数：</p><p class="calibre_20"><img src="images/00389.jpg" class="calibre_269"/></p><p class="calibre_22">其中，T为任意子树，C(T)为对训练数据的预测误差（如基尼指数），|T|为子树的叶结点个数，a≥0为参数，C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T)为参数是a时的子树T的整体损失。参数a权衡训练数据的拟合程度与模型的复杂度。</p><p class="calibre_8">对固定的a，一定存在使损失函数C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T)最小的子树，将其表示为T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>在损失函数C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T)最小的意义下是最优的。容易验证这样的最优子树是唯一的。当a大的时候，最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>偏小；当a小的时候，最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>偏大。极端情况，当a＝0时，整体树是最优的。当a→<img src="images/00633.jpg" class="calibre_169"/>时，根结点组成的单结点树是最优的。</p><p class="calibre_8">Breiman等人证明：可以用递归的方法对树进行剪枝。将a从小增大，0＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>&lt;a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>&lt;…&lt;a<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>&lt;+<img src="images/00633.jpg" class="calibre_169"/>，产生一系列的区间[a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i+1</span></small></sub>),i＝0,1,…,n；剪枝得到的子树序列对应着区间a∊[a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i+1</span></small></sub>)，i＝0,1,…,n的最优子树序列{T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>}，序列中的子树是嵌套的。</p><p class="calibre_8">具体地，从整体树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>开始剪枝。对T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>的任意内部结点t，以t为单结点树的损失函数是</p><p class="calibre_20"><img src="images/00394.jpg" class="calibre_270"/></p><p class="calibre_22">以t为根结点的子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>的损失函数是</p><p class="calibre_20"><img src="images/00399.jpg" class="calibre_271"/></p><p class="calibre_24">当a＝0及a充分小时，有不等式</p><p class="calibre_20"><img src="images/00678.jpg" class="calibre_272"/></p><p class="calibre_24">当a增大时，在某一a有</p><p class="calibre_20"><img src="images/00849.jpg" class="calibre_273"/></p><p class="calibre_24">当a再增大时，不等式（5.29）反向。只要<img src="images/00687.jpg" class="calibre_274"/>，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>与t有相同的损失函数值，而t的结点少，因此t比T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>更可取，对T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>进行剪枝。</p><p class="calibre_8">为此，对T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>中每一内部结点t，计算</p><p class="calibre_20"><img src="images/00893.jpg" class="calibre_275"/></p><p class="calibre_22">它表示剪枝后整体损失函数减少的程度。在T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>中剪去g(t)最小的T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>，将得到的子树作为T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，同时将最小的g(t)设为a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>。T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>为区间[a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)的最优子树。</p><p class="calibre_8">如此剪枝下去，直至得到根结点。在这一过程中，不断地增加a的值，产生新的区间。</p><p class="calibre_7"><span class="bold">2．在剪枝得到的子树序列T<sub class="calibre8"><small class="calibre6"><span class="calibre7"><span class="bold">0</span></span></small></sub>,T<sub class="calibre8"><small class="calibre6"><span class="calibre7"><span class="bold">1</span></span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7"><span class="bold">n</span></span></small></sub>中通过交叉验证选取最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7"><span class="bold">a</span></span></small></sub></span></p><p class="calibre_7">具体地，利用独立的验证数据集，测试子树序列T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>都对应于一个参数a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>。所以，当最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>确定时，对应的ak也确定了，即得到最优决策树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。</p><p class="calibre_8">现在写出CART剪枝算法。</p><p class="calibre_146"><span class="bold">算法5.7（CART剪枝算法）</span></p><p class="calibre_146">输入：CART算法生成的决策树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>；</p><p class="calibre_8">输出：最优决策树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。</p><p class="calibre_8">（1）设k＝0，T＝T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>。</p><p class="calibre_8">（2）设a＝+<img src="images/00633.jpg" class="calibre_169"/>。</p><p class="calibre_8">（3）自下而上地对各内部结点t计算C(T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>)，|T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>|以及</p><p class="calibre_20"><img src="images/00691.jpg" class="calibre_276"/></p><p class="calibre_22">这里，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>表示以t为根结点的子树，C(T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>)是对训练数据的预测误差，|T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>|是T<sub class="calibre8"><small class="calibre6"><span class="calibre7">t</span></small></sub>的叶结点个数。</p><p class="calibre_8">（4）自上而下地访问内部结点t，如果有g(t)＝a，进行剪枝，并对叶结点t以多数表决法决定其类，得到树T。</p><p class="calibre_8">（5）设k＝k+1，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>＝a，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>＝T。</p><p class="calibre_8">（6）如果T不是由根结点单独构成的树，则回到步骤（4）。</p><p class="calibre_8">（7）采用交叉验证法在子树序列T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>中选取最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>。</p><p id="filepos255107" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．分类决策树模型是表示基于特征对实例进行分类的树形结构。决策树可以转换成一个if-then规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。</p><p class="calibre_8">2．决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策树。因为从可能的决策树中直接选取最优决策树是NP完全问题。现实中采用启发式方法学习次优的决策树。</p><p class="calibre_8">决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、C4.5和CART。</p><p class="calibre_8">3．特征选择的目的在于选取对训练数据能够分类的特征。特征选择的关键是其准则。常用的准则如下：</p><p class="calibre_8">（1）样本集合D对特征A的信息增益（ID3）</p><p class="calibre_20"><img src="images/00694.jpg" class="calibre_277"/></p><p class="calibre_22">其中，H(D)是数据集D的熵，H(D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)是数据集D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的熵，H(D|A)是数据集D对特征A的条件熵。 D<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>是D中特征A取第i个值的样本子集，C<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>是D中属于第k类的样本子集。n是特征A取值的个数，K是类的个数。</p><p class="calibre_8">（2）样本集合D对特征A的信息增益比（C4.5）</p><p class="calibre_20"><img src="images/00959.jpg" class="calibre_278"/></p><p class="calibre_22">其中，g(D,A)是信息增益，H(D)是数据集D的熵。</p><p class="calibre_8">（3）样本集合D的基尼指数（CART）</p><p class="calibre_20"><img src="images/00701.jpg" class="calibre_279"/></p><p class="calibre_24">特征A条件下集合D的基尼指数：</p><p class="calibre_20"><img src="images/01003.jpg" class="calibre_280"/></p><p class="calibre_24">4．决策树的生成。通常使用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。决策树的生成往往通过计算信息增益或其他指标，从根结点开始，递归地产生决策树。这相当于用信息增益或其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。</p><p class="calibre_8">5．决策树的剪枝。由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。</p><p id="filepos258145" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">介绍决策树学习方法的文献很多，关于ID3可见文献[1]，C4.5可见文献[2]，CART可见文献[3,4]。决策树学习一般性介绍可见文献[5～7]。与决策树类似的分类方法还有决策列表（decision list）。决策列表与决策树可以相互转换<sup class="calibre5"><small class="calibre6"><span class="calibre7">[8]</span></small></sup>，决策列表的学习方法可参见文献[9]。</p><p id="filepos258652" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">5.1　根据表5.1所给的训练数据集，利用信息增益比（C4.5算法）生成决策树。</p><p class="calibre_8">5.2　已知如表5.2所示的训练数据，试用平方误差损失准则生成一个二叉回归树。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表5.2　训练数据表</span></span></p><p class="calibre_20"><img src="images/00706.jpg" class="calibre_281"/></p><p class="calibre_24">5.3　证明CART剪枝算法中，当a确定时，存在唯一的最小子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>使损失函数C<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>(T)最小。</p><p class="calibre_8">5.4　证明CART剪枝算法中求出的子树序列{T<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>，T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,…,T<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>}分别是区间a∊[a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i+1</span></small></sub>)的最优子树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">a</span></small></sub>，这里i＝0,1,…,n，0＝a<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>&lt;a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>&lt;…&lt;a<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>&lt;+<img src="images/00633.jpg" class="calibre_169"/>。</p><p id="filepos260156" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Quinlan JR. Induction of decision trees. Machine Learning,1986,1(1): 81–106</p><p class="calibre_8">[2]　Quinlan JR. C4. 5: Programs for Machine Learning. Morgan Kaufmann,1992</p><p class="calibre_8">[3]　Breiman L,Friedman J,Stone C. Classification and Regression Trees. Wadsworth,1984</p><p class="calibre_8">[4]　Ripley B. Pattern Recognition and Neural Networks. Cambridge University Press,1996</p><p class="calibre_8">[5]　Liu B. Web Data Mining: Exploring Hyperlinks,Contents and Usage Data. Springer-Verlag,2006</p><p class="calibre_8">[6]　Hyafil L,Rivest RL. Constructing Optimal Binary Decision Trees is NP-complete. Information Processing Letters,1976,5(1): 15–17</p><p class="calibre_8">[7]　Hastie T,Tibshirani R,Friedman JH. The Elements of Statistical Learning: Data Mining,Inference,and Prediction. New York: Springer-Verlag,2001</p><p class="calibre_8">[8]　Yamanishi K. A learning criterion for stochastic rules. Machine Learning,1992</p><p class="calibre_8">[9]　Li H,Yamanishi K. Text classification using ESC-based stochastic decision lists. Information Processing &amp; Management,2002,38(3): 343–361</p><p class="calibre_8">[10]　Li H,Abe N. Generalizing case frames using a thesaurus and the MDL principle. Computational Linguistics,1998,24(2): 217–244</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos261718" class="calibre_99"><a href="#filepos207774"><span class="calibre4">[1]</span></a><span class="calibre4">　此例取自参考文献[5]。</span></p><div class="mbp_pagebreak" id="calibre_pb_10"></div>
</body></html>
