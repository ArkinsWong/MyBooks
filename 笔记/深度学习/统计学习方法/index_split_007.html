<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos114167" class="calibre_"><span class="calibre1"><span class="bold">第2章　感知机</span></span></p><p class="calibre_13">感知机（perceptron）是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和–1二值。感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。感知机学习算法具有简单而易于实现的优点，分为原始形式和对偶形式。感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机1957年由Rosenblatt提出，是神经网络与支持向量机的基础。</p><p class="calibre_8">本章首先介绍感知机模型；然后叙述感知机的学习策略，特别是损失函数；最后介绍感知机学习算法，包括原始形式和对偶形式，并证明算法的收敛性。</p><p id="filepos115271" class="calibre_6"><span class="calibre9"><span class="bold">2.1　感知机模型</span></span></p><p class="calibre_7"><span class="bold">定义2.1（感知机）</span>　假设输入空间（特征空间）是x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7"><span class="italic">n</span></span></small></sup>，输出空间是<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}。输入x∊x表示实例的特征向量，对应于输入空间（特征空间）的点；输出y∊<img src="images/00940.jpg" class="calibre_29"/>表示实例的类别。由输入空间到输出空间的如下函数</p><p class="calibre_20"><img src="images/00407.jpg" class="calibre_100"/></p><p class="calibre_22">称为感知机。其中，w和b为感知机模型参数，w∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>叫作权值（weight）或权值向量（weight vector），b∊R叫作偏置（bias），w·x表示w和x的内积。sign是符号函数，即</p><p class="calibre_20"><img src="images/00414.jpg" class="calibre_101"/></p><p class="calibre_24">感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器(linear classifier)，即函数集合{f|f(x)＝w·x+b}。</p><p class="calibre_8">感知机有如下几何解释：线性方程</p><p class="calibre_20"><img src="images/00376.jpg" class="calibre_102"/></p><p class="calibre_22">对应于特征空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>中的一个超平面S，其中w是超平面的法向量，b是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点（特征向量）分别被分为正、负两类。因此，超平面S称为分离超平面（separating hyperplane），如图2.1所示。</p><p class="calibre_20"><img src="images/00420.jpg" class="calibre_103"/></p><p class="calibre_20"><span class="calibre4">图2.1　感知机模型</span></p><p class="calibre_27">感知机学习，由训练数据集（实例的特征向量及类别）</p><p class="calibre_20"><img src="images/00422.jpg" class="calibre_28"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}，i＝1,2,…,N，求得感知机模型（2.1），即求得模型参数w,b。感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。</p><p id="filepos118167" class="calibre_6"><span class="calibre9"><span class="bold">2.2　感知机学习策略</span></span></p><p id="filepos118270" class="calibre_6"><span class="calibre2"><span class="bold">2.2.1　数据集的线性可分性</span></span></p><p class="calibre_7"><span class="bold">定义2.2（数据集的线性可分性）</span>　给定一个数据集</p><p class="calibre_20"><img src="images/00440.jpg" class="calibre_104"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}，i＝1,2,…,N，如果存在某个超平面S</p><p class="calibre_20"><img src="images/00432.jpg" class="calibre_105"/></p><p class="calibre_22">能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1的实例i，有w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b&gt;0，对所有y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1的实例i，有w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b&lt;0，则称数据集T为线性可分数据集（linearly separable data set）；否则，称数据集T线性不可分。</p><p id="filepos119575" class="calibre_6"><span class="calibre2"><span class="bold">2.2.2　感知机学习策略</span></span></p><p class="calibre_7">假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。为了找出这样的超平面，即确定感知机模型参数w,b，需要确定一个学习策略，即定义（经验）损失函数并将损失函数极小化。</p><p class="calibre_8">损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数w,b的连续可导函数，不易优化。损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的。为此，首先写出输入空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>中任一点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>到超平面S的距离：</p><p class="calibre_20"><img src="images/00486.jpg" class="calibre_106"/></p><p class="calibre_22">这里，||w||是w的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>范数。</p><p class="calibre_8">其次，对于误分类的数据（x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>）来说，</p><p class="calibre_20"><img src="images/00434.jpg" class="calibre_107"/></p><p class="calibre_22">成立。因为当w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b&gt;0时，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝-1，而当w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b&lt;0时，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝+1。因此，误分类点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>到超平面S的距离是</p><p class="calibre_20"><img src="images/00536.jpg" class="calibre_108"/></p><p class="calibre_24">这样，假设超平面S的误分类点集合为M，那么所有误分类点到超平面S的总距离为</p><p class="calibre_20"><img src="images/00559.jpg" class="calibre_109"/></p><p class="calibre_22">不考虑<img src="images/00447.jpg" class="calibre_110"/>，就得到感知机学习的损失函数<sup class="calibre5"><small id="filepos121937" class="calibre6"><a href="#filepos152765"><span class="calibre7">[1]</span></a></small></sup>。</p><p class="calibre_8">给定训练数据集</p><p class="calibre_20"><img src="images/00452.jpg" class="calibre_28"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{+1,-1}，i＝1,2,…,N。感知机sign(w·x+b)学习的损失函数定义为</p><p class="calibre_20"><img src="images/00455.jpg" class="calibre_111"/></p><p class="calibre_22">其中M为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。</p><p class="calibre_8">显然，损失函数L(w,b)是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。一个特定的样本点的损失函数：在误分类时是参数w,b的线性函数，在正确分类时是0。因此，给定训练数据集T，损失函数L(w,b)是w,b的连续可导函数。</p><p class="calibre_8">感知机学习的策略是在假设空间中选取使损失函数式（2.4）最小的模型参数w,b，即感知机模型。</p><p id="filepos123345" class="calibre_6"><span class="calibre9"><span class="bold">2.3　感知机学习算法</span></span></p><p class="calibre_7">感知机学习问题转化为求解损失函数式（2.4）的最优化问题，最优化的方法是随机梯度下降法。本节叙述感知机学习的具体算法，包括原始形式和对偶形式，并证明在训练数据线性可分条件下感知机学习算法的收敛性。</p><p id="filepos123768" class="calibre_6"><span class="calibre2"><span class="bold">2.3.1　感知机学习算法的原始形式</span></span></p><p class="calibre_7">感知机学习算法是对以下最优化问题的算法。给定一个训练数据集</p><p class="calibre_20"><img src="images/00652.jpg" class="calibre_89"/></p><p class="calibre_24">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,1}，i＝1,2,…,N，求参数w,b，使其为以下损失函数极小化问题的解</p><p class="calibre_20"><img src="images/00461.jpg" class="calibre_112"/></p><p class="calibre_22">其中M为误分类点的集合。</p><p class="calibre_8">感知机学习算法是误分类驱动的，具体采用随机梯度下降法（stochastic gradient descent）。首先，任意选取一个超平面,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,b<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>，然后用梯度下降法不断地极小化目标函数（2.5）。极小化过程中不是一次使M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p><p class="calibre_8">假设误分类点集合M是固定的，那么损失函数L(w,b)的梯度由</p><p class="calibre_20"><img src="images/00466.jpg" class="calibre_113"/></p><p class="calibre_22">给出。</p><p class="calibre_8">随机选取一个误分类点(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)，对w,b进行更新：</p><p class="calibre_20"><img src="images/00471.jpg" class="calibre_114"/></p><p class="calibre_22">式中<img src="images/00636.jpg" class="calibre_18"/>(0&lt;<img src="images/00636.jpg" class="calibre_18"/>≤1)是步长，在统计学习中又称为学习率（learning rate）。这样，通过迭代可以期待损失函数L(w,b)不断减小，直到为0。综上所述，得到如下算法：</p><p class="calibre_8"><span class="bold">算法2.1（感知机学习算法的原始形式）</span></p><p class="calibre_8">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N；学习率<img src="images/00636.jpg" class="calibre_18"/>(0&lt;<img src="images/00636.jpg" class="calibre_18"/>≤1)；</p><p class="calibre_8">输出：w,b；感知机模型f(x)＝sign(w·x+b)。</p><p class="calibre_8">（1）选取初值w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,b<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub></p><p class="calibre_8">（2）在训练集中选取数据(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)</p><p class="calibre_8">（3）如果y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b)≤0</p><p class="calibre_20"><img src="images/00474.jpg" class="calibre_115"/></p><p class="calibre_24">（4）转至（2），直至训练集中没有误分类点。</p><p class="calibre_8">这种学习算法直观上有如下解释：当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整w,b的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。</p><p class="calibre_8">算法2.1是感知机学习的基本算法，对应于后面的对偶形式，称为原始形式。感知机学习算法简单且易于实现。</p><p class="calibre_8"><span class="bold">例2.1</span>　如图2.2所示的训练数据集，其正实例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(4,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，负实例点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试用感知机学习算法的原始形式求感知机模型f(x)＝sign(w·x+b)。这里，w＝(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x＝(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>)<sup class="calibre5"><small class="calibre6"></small></sup><sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。</p><p class="calibre_20"><img src="images/00755.jpg" class="calibre_116"/></p><p class="calibre_20"><span class="calibre4">图2.2　感知机示例</span></p><p class="calibre_27"><span class="bold">解</span>　构建最优化问题：</p><p class="calibre_20"><img src="images/00778.jpg" class="calibre_117"/></p><p class="calibre_22">按照算法2.1求解w,b。<img src="images/00636.jpg" class="calibre_18"/>＝1。</p><p class="calibre_8">（1）取初值w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>＝0，b<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>＝0</p><p class="calibre_8">（2）对x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>)＝0，未能被正确分类，更新w,b</p><p class="calibre_20"><img src="images/00802.jpg" class="calibre_118"/></p><p class="calibre_24">得到线性模型</p><p class="calibre_20"><img src="images/00823.jpg" class="calibre_119"/></p><p class="calibre_24">（3）对x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，显然，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>)&gt;0，被正确分类，不修改w,b；对x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>)&lt;0，被误分类，更新w,b。</p><p class="calibre_20"><img src="images/00843.jpg" class="calibre_120"/></p><p class="calibre_24">得到线性模型</p><p class="calibre_20"><img src="images/00862.jpg" class="calibre_121"/></p><p class="calibre_22">如此继续下去，直到</p><p class="calibre_20"><img src="images/00503.jpg" class="calibre_122"/></p><p class="calibre_22">对所有数据点y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">7</span></small></sub>·x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">7</span></small></sub>)&gt;0，没有误分类点，损失函数达到极小。</p><p class="calibre_8">分离超平面为x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>+x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>-3＝0</p><p class="calibre_8">感知机模型为f(x)＝sign(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>+x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>-3)</p><p class="calibre_8">迭代过程见表2.1。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表2.1　例2.1求解的迭代过程</span></span></p><p class="calibre_20"><img src="images/00905.jpg" class="calibre_123"/></p><p class="calibre_24">这是在计算中误分类点先后取x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>得到的分离超平面和感知机模型。如果在计算中误分类点依次取x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>，那么得到的分离超平面是x<sup class="calibre5"><small class="calibre6"><span class="calibre7">2</span></small></sup><sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>+x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>-5＝0。</p><p class="calibre_8">可见，感知机学习算法由于采用不同的初值或选取不同的误分类点，解可以不同。</p><p id="filepos134079" class="calibre_6"><span class="calibre2"><span class="bold">2.3.2　算法的收敛性</span></span></p><p class="calibre_7">现在证明，对于线性可分数据集感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。</p><p class="calibre_8">为了便于叙述与推导，将偏置b并入权重向量w，记作<img src="images/00513.jpg" class="calibre_19"/>＝(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>,b)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，同样也将输入向量加以扩充，加进常数1，记作<img src="images/00952.jpg" class="calibre_124"/>＝(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。这样，<img src="images/00952.jpg" class="calibre_124"/>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>，<img src="images/00513.jpg" class="calibre_19"/>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>。显然，<img src="images/00513.jpg" class="calibre_19"/>·<img src="images/00952.jpg" class="calibre_124"/>＝w·x+b。</p><p class="calibre_8"><span class="bold">定理2.1（Novikoff）</span>　设训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}是线性可分的，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x＝R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，i＝1,2,…,N，则</p><p class="calibre_8">（1）存在满足条件||<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>||＝1的超平面<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>·<img src="images/00952.jpg" class="calibre_124"/>＝w<sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>·x+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>＝0将训练数据集完全正确分开；且存在<img src="images/00972.jpg" class="calibre_125"/>&gt;0，对所有i＝1,2,…,N</p><p class="calibre_20"><img src="images/00993.jpg" class="calibre_126"/></p><p class="calibre_24">（2）令<img src="images/00528.jpg" class="calibre_127"/>，则感知机算法2.1在训练数据集上的误分类次数k满足不等式</p><p class="calibre_20"><img src="images/01037.jpg" class="calibre_128"/></p><p class="calibre_24"><span class="bold">证明</span>　（1）由于训练数据集是线性可分的，按照定义2.2，存在超平面可将训练数据集完全正确分开，取此超平面为<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>·<img src="images/00952.jpg" class="calibre_124"/>＝w<sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>·x+b<sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>＝0，使||<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">opt</span></small></sub>||＝1。由于对有限的i＝1,2,…,N，均有</p><p class="calibre_20"><img src="images/00535.jpg" class="calibre_129"/></p><p class="calibre_22">所以存在</p><p class="calibre_20"><img src="images/00288.jpg" class="calibre_130"/></p><p class="calibre_22">使</p><p class="calibre_20"><img src="images/00406.jpg" class="calibre_131"/></p><p class="calibre_24">（2）感知机算法从<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>=0开始，如果实例被误分类，则更新权重。令<img src="images/00513.jpg" class="calibre_19"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k-1</span></small></sub>是第k个误分类实例之前的扩充权重向量，即</p><p class="calibre_20"><img src="images/00332.jpg" class="calibre_132"/></p><p class="calibre_22">则第k个误分类实例的条件是</p><p class="calibre_20"><img src="images/00356.jpg" class="calibre_133"/></p><p class="calibre_22">若(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)是被<img src="images/00416.jpg" class="calibre_134"/>误分类的数据，则w和b的更新是</p><p class="calibre_20"><img src="images/00401.jpg" class="calibre_135"/></p><p class="calibre_22">即</p><p class="calibre_20"><img src="images/00426.jpg" class="calibre_136"/></p><p class="calibre_24">下面推导两个不等式：</p><p class="calibre_8">（1）</p><p class="calibre_20"><img src="images/00428.jpg" class="calibre_137"/></p><p class="calibre_24">由式（2.11）及式（2.8）得</p><p class="calibre_20"><img src="images/00463.jpg" class="calibre_138"/></p><p class="calibre_22">由此递推即得不等式（2.12）</p><p class="calibre_20"><img src="images/00433.jpg" class="calibre_139"/></p><p class="calibre_24">（2）</p><p class="calibre_20"><img src="images/00514.jpg" class="calibre_140"/></p><p class="calibre_24">由式（2.11）及式（2.10）得</p><p class="calibre_20"><img src="images/00439.jpg" class="calibre_141"/></p><p class="calibre_24">结合不等式（2.12）及式（2.13）即得</p><p class="calibre_20"><img src="images/00443.jpg" class="calibre_142"/></p><p class="calibre_22">于是</p><p class="calibre_20"><img src="images/00584.jpg" class="calibre_143"/></p><p class="calibre_24">定理表明，误分类的次数<span class="italic">k</span>是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。但是例2.1说明，感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。这就是第7章将要讲述的线性支持向量机的想法。当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。</p><p id="filepos141359" class="calibre_6"><span class="calibre2"><span class="bold">2.3.3　感知机学习算法的对偶形式</span></span></p><p class="calibre_7">现在考虑感知机学习算法的对偶形式。感知机学习算法的原始形式和对偶形式与第7章中支持向量机学习算法的原始形式和对偶形式相对应。</p><p class="calibre_8">对偶形式的基本想法是，将w和b表示为实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>和标记y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的线性组合的形式，通过求解其系数而求得w和b。不失一般性，在算法2.1中可假设初始值w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,b<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>均为0。对误分类点（x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>）通过</p><p class="calibre_20"><img src="images/00608.jpg" class="calibre_144"/></p><p class="calibre_22">逐步修改w,b，设修改n次，则w,b关于（x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>）的增量分别是a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>和a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，这里a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝n<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub><img src="images/00636.jpg" class="calibre_18"/>。这样，从学习过程不难看出，最后学习到的w,b可以分别表示为</p><p class="calibre_20"><img src="images/00630.jpg" class="calibre_145"/></p><p class="calibre_22">这里，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>≥0，i＝1,2,…,N，当<img src="images/00636.jpg" class="calibre_18"/>＝1时，表示第i个实例点由于误分而进行更新的次数。实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类。换句话说，这样的实例对学习结果影响最大。</p><p class="calibre_8">下面对照原始形式来叙述感知机学习算法的对偶形式。</p><p class="calibre_146"><span class="bold">算法2.2（感知机学习算法的对偶形式）</span></p><p class="calibre_146">输入：线性可分的数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊{-1,+1}，<span class="italic">i</span>＝1,2,…,<span class="italic">N</span>；学习率<img src="images/00636.jpg" class="calibre_18"/>（0&lt;<img src="images/00636.jpg" class="calibre_18"/>≤1）；</p><p class="calibre_8">输出：a,b；感知机模型<img src="images/00458.jpg" class="calibre_147"/></p><p class="calibre_8">其中a＝(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup> 。</p><p class="calibre_8">（1）a←0，b←0</p><p class="calibre_8">（2）在训练集中选取数据(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)</p><p class="calibre_8">（3）如果<img src="images/00671.jpg" class="calibre_148"/></p><p class="calibre_20"><img src="images/00446.jpg" class="calibre_149"/></p><p class="calibre_24">（4）转至（2）直到没有误分类数据。</p><p class="calibre_8">对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram矩阵（Gram matrix）</p><p class="calibre_20"><img src="images/00119.jpg" class="calibre_150"/></p><p class="calibre_24"><span class="bold">例2.2</span>　数据同例2.1，正样本点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(3,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(4,3)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，负样本点是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试用感知机学习算法对偶形式求感知机模型。</p><p class="calibre_8"><span class="bold">解</span>　按照算法2.2，</p><p class="calibre_8">（1）取a<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝0，i＝1,2,3，b＝0，<img src="images/00636.jpg" class="calibre_18"/>＝1</p><p class="calibre_8">（2）计算Gram矩阵</p><p class="calibre_20"><img src="images/00733.jpg" class="calibre_151"/></p><p class="calibre_24">（3）误分条件</p><p class="calibre_20"><img src="images/00478.jpg" class="calibre_152"/></p><p class="calibre_24">参数更新</p><p class="calibre_20"><img src="images/00480.jpg" class="calibre_153"/></p><p class="calibre_24">（4）迭代。过程从略，结果列于表2.2。</p><p class="calibre_8">（5）</p><p class="calibre_20"><img src="images/00485.jpg" class="calibre_154"/></p><p class="calibre_24">分离超平面</p><p class="calibre_20"><img src="images/00489.jpg" class="calibre_155"/></p><p class="calibre_24">感知机模型</p><p class="calibre_20"><img src="images/00494.jpg" class="calibre_40"/></p><p class="calibre_20"><span class="calibre2"><span class="bold">表2.2　例2.2求解的迭代过程</span></span></p><p class="calibre_20"><img src="images/00499.jpg" class="calibre_156"/></p><p class="calibre_24">对照例2.1，结果一致，迭代步骤也是互相对应的。</p><p class="calibre_8">与原始形式一样，感知机学习算法的对偶形式迭代是收敛的，存在多个解。</p><p id="filepos148101" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．感知机是根据输入实例的特征向量x对其进行二类分类的线性分类模型：</p><p class="calibre_20"><img src="images/00882.jpg" class="calibre_157"/></p><p class="calibre_22">感知机模型对应于输入空间（特征空间）中的分离超平面w·x+b＝0。</p><p class="calibre_8">2．感知机学习的策略是极小化损失函数：</p><p class="calibre_20"><img src="images/00508.jpg" class="calibre_158"/></p><p class="calibre_22">损失函数对应于误分类点到分离超平面的总距离。</p><p class="calibre_8">3．感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法，有原始形式和对偶形式。算法简单且易于实现。原始形式中，首先任意选取一个超平面，然后用梯度下降法不断极小化目标函数。在这个过程中一次随机选取一个误分类点使其梯度下降。</p><p class="calibre_8">4．当训练数据集线性可分时，感知机学习算法是收敛的。感知机算法在训练数据集上的误分类次数k满足不等式：</p><p class="calibre_20"><img src="images/00929.jpg" class="calibre_159"/></p><p class="calibre_24">当训练数据集线性可分时，感知机学习算法存在无穷多个解，其解由于不同的初值或不同的迭代顺序而可能有所不同。</p><p id="filepos149712" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">感知机最早在1957年由Rosenblatt提出<sup class="calibre5"><small class="calibre6"><span class="calibre7">[1]</span></small></sup>。Novikoff<sup class="calibre5"><small class="calibre6"><span class="calibre7"> [2]</span></small></sup>，Minsky与Papert<sup class="calibre5"><small class="calibre6"><span class="calibre7"> [3]</span></small></sup>等人对感知机进行了一系列理论研究。感知机的扩展学习方法包括口袋算法（pocket algorithm）<sup class="calibre5"><small class="calibre6"><span class="calibre7">[4]</span></small></sup>、表决感知机（voted perceptron）<sup class="calibre5"><small class="calibre6"><span class="calibre7">[5]</span></small></sup>、带边缘感知机（perceptron with margin）<sup class="calibre5"><small class="calibre6"><span class="calibre7">[6]</span></small></sup>。关于感知机的介绍可进一步参考文献[7,8]。</p><p id="filepos150482" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">2.1　Minsky与Papert指出：感知机因为是线性模型，所以不能表示复杂的函数，如异或（XOR）。验证感知机为什么不能表示异或。</p><p class="calibre_8">2.2　模仿例题2.1，构建从训练数据集求解感知机模型的例子。</p><p class="calibre_8">2.3　证明以下定理：样本集线性可分的充分必要条件是正实例点集所构成的凸壳<sup class="calibre5"><small id="filepos151012" class="calibre6"><a href="#filepos152934"><span class="calibre7">[2]</span></a></small></sup>与负实例点集所构成的凸壳互不相交。</p><p id="filepos151139" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Rosenblatt F. The Perceptron: A probabilistic model for information storage and organization in the Brain. Cornell Aeronautical Laboratory. Psychological Review,1958,65(6): 386–408</p><p class="calibre_8">[2]　Novikoff AB. On convergence proofs on perceptrons. Symposium on the Mathematical Theory of Automata,Polytechnic Institute of Brooklyn,1962,12,615–622</p><p class="calibre_8">[3]　Minsky ML,Papert SA. Perceptrons. Cambridge,MA: MIT Press. 1969</p><p class="calibre_8">[4]　Gallant SI. Perceptron-based learning algorithms. IEEE Transactions on Neural Networks,1990,1(2): 179–191</p><p class="calibre_8">[5]　Freund Y,Schapire RE. Large margin classification using the perceptron algorithm. In: Proceedings of the 11th Annual Conference on Computational Learning Theory(COLT’ 98). ACM Press,1998</p><p class="calibre_8">[6]　Li YY,Zaragoza H,Herbrich R,Shawe-Taylor J,Kandola J. The Perceptron algorithm with uneven margins. In: Proceedings of the 19th International Conference on Machine Learning. 2002,379–386</p><p class="calibre_8">[7]　Widrow B,Lehr MA. 30 years of adaptive neural networks: Perceptron,madaline,and backpropagation. Proc. IEEE,1990,78(9): 1415–1442</p><p class="calibre_8">[8]　Cristianini N,Shawe-Taylor J. An Introduction to Support Vector Machines and Other Kernelbased Learning Methods. Cambridge University Press,2000</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos152765" class="calibre_99"><a href="#filepos121937"><span class="calibre4">[1]</span></a><span class="calibre4">　第7章中会介绍y(w·x+b)称为样本点的函数间隔。</span></p><p id="filepos152934" class="calibre_8"><a href="#filepos151012"><span class="calibre4">[2]</span></a><span class="calibre4">　设集合S⊂R<sup class="calibre10"><small class="calibre6"><span class="calibre7">n</span></small></sup>是由R<sup class="calibre10"><small class="calibre6"><span class="calibre7">n</span></small></sup>中的k个点所组成的集合，即S＝{x<sub class="calibre11"><small class="calibre6"><span class="calibre7">1</span></small></sub>，x<sub class="calibre11"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre11"><small class="calibre6"><span class="calibre7">k</span></small></sub>}。定义S的凸壳conv(S)为</span></p><p class="calibre_8"><img src="images/00516.jpg" class="calibre_160"/></p><div class="mbp_pagebreak" id="calibre_pb_7"></div>
</body></html>
