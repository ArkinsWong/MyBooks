<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos446622" class="calibre_"><span class="calibre1"><span class="bold">第8章　提升方法</span></span></p><p class="calibre_13">提升（boosting）方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。</p><p class="calibre_8">本章首先介绍提升方法的思路和代表性的提升算法AdaBoost；然后通过训练误差分析探讨AdaBoost为什么能够提高学习精度；并且从前向分步加法模型的角度解释AdaBoost；最后叙述提升方法更具体的实例——提升树（boosting tree）。AdaBoost算法是1995年由Freund和Schapire提出的，提升树是2000年由Friedman等人提出的。</p><p id="filepos447422" class="calibre_6"><span class="calibre9"><span class="bold">8.1　提升方法AdaBoost算法</span></span></p><p id="filepos447530" class="calibre_6"><span class="calibre2"><span class="bold">8.1.1　提升方法的基本思路</span></span></p><p class="calibre_7">提升方法基于这样一种思想：对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。实际上，就是“三个臭皮匠顶个诸葛亮”的道理。</p><p class="calibre_8">历史上，Kearns和Valiant首先提出了“强可学习（strongly learnable）”和“弱可学习（weakly learnable）”的概念。指出：在概率近似正确（probably approximately correct，PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的；一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。</p><p class="calibre_8">这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为“强学习算法”。大家知道，发现弱学习算法通常要比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。关于提升方法的研究很多，有很多算法被提出。最具代表性的是AdaBoost算法（AdaBoost algorithm）。</p><p class="calibre_8">对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易得多。提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。</p><p class="calibre_8">这样，对提升方法来说，有两个问题需要回答：一是在每一轮如何改变训练数据的权值或概率分布；二是如何将弱分类器组合成一个强分类器。关于第1个问题，AdaBoost的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。这样一来，那些没有得到正确分类的数据，由于其权值的加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器“分而治之”。至于第2个问题，即弱分类器的组合，AdaBoost采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。</p><p class="calibre_8">AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里。</p><p id="filepos450762" class="calibre_6"><span class="calibre2"><span class="bold">8.1.2　AdaBoost算法</span></span></p><p class="calibre_7">现在叙述AdaBoost算法。假设给定一个二类分类的训练数据集</p><p class="calibre_20"><img src="images/00564.jpg" class="calibre_28"/></p><p class="calibre_22">其中，每个样本点由实例与标记组成。实例x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，标记y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}，x是实例空间，<img src="images/00940.jpg" class="calibre_29"/>是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些弱分类器线性组合成为一个强分类器。</p><p class="calibre_146"><span class="bold">算法8.1（AdaBoost）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}；弱学习算法；</p><p class="calibre_8">输出：最终分类器G(x)。</p><p class="calibre_8">（1）初始化训练数据的权值分布</p><p class="calibre_20"><img src="images/00675.jpg" class="calibre_605"/></p><p class="calibre_24">（2）对M＝1,2,…,m</p><p class="calibre_8">（a）使用具有权值分布D<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的训练数据集学习，得到基本分类器</p><p class="calibre_20"><img src="images/00705.jpg" class="calibre_606"/></p><p class="calibre_24">（b）计算G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)在训练数据集上的分类误差率</p><p class="calibre_20"><img src="images/00355.jpg" class="calibre_607"/></p><p class="calibre_24">（c）计算G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)的系数</p><p class="calibre_20"><img src="images/00462.jpg" class="calibre_608"/></p><p class="calibre_22">这里的对数是自然对数。</p><p class="calibre_8">（d）更新训练数据集的权值分布</p><p class="calibre_20"><img src="images/00583.jpg" class="calibre_609"/></p><p class="calibre_22">这里，Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>是规范化因子</p><p class="calibre_20"><img src="images/00731.jpg" class="calibre_610"/></p><p class="calibre_22">它使D<sub class="calibre8"><small class="calibre6"><span class="calibre7">m+1</span></small></sub>成为一个概率分布。</p><p class="calibre_8">（3）构建基本分类器的线性组合</p><p class="calibre_20"><img src="images/00207.jpg" class="calibre_611"/></p><p class="calibre_22">得到最终分类器</p><p class="calibre_20"><img src="images/00698.jpg" class="calibre_612"/></p><p class="calibre_24">对AdaBoost算法作如下说明：</p><p class="calibre_8">步骤（1）　假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同，这一假设保证第1步能够在原始数据上学习基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)。</p><p class="calibre_8">步骤（2）　AdaBoost反复学习基本分类器，在每一轮m＝1,2,…,M顺次地执行下列操作：</p><p class="calibre_8">（a）使用当前分布D<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>加权的训练数据集，学习基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)。</p><p class="calibre_8">（b）计算基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)在加权训练数据集上的分类误差率：</p><p class="calibre_20"><img src="images/00082.jpg" class="calibre_613"/></p><p class="calibre_22">这里，w<sub class="calibre8"><small class="calibre6"><span class="calibre7">mi</span></small></sub>表示第m轮中第i个实例的权值，<img src="images/00057.jpg" class="calibre_614"/>。这表明，G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)在加权的训练数据集上的分类误差率是被G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)误分类样本的权值之和，由此可以看出数据权值分布D<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>与基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)的分类误差率的关系。</p><p class="calibre_8">（c）计算基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)的系数am。am表示G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)在最终分类器中的重要性。由式（8.2）可知，当e<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>≤<img src="images/00811.jpg" class="calibre_61"/>时，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>≥0，并且a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>随着e<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。</p><p class="calibre_8">（d）更新训练数据的权值分布为下一轮作准备。式（8.4）可以写成：</p><p class="calibre_20"><img src="images/00264.jpg" class="calibre_615"/></p><p class="calibre_22">由此可知，被基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)误分类样本的权值得以扩大，而被正确分类样本的权值却得以缩小。两相比较，误分类样本的权值被放大<img src="images/00819.jpg" class="calibre_616"/>倍。因此，误分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用，这是AdaBoost的一个特点。</p><p class="calibre_8">步骤（3）　线性组合f(x)实现M个基本分类器的加权表决。系数a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>表示了基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)的重要性，这里，所有a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>之和并不为1.f(x)的符号决定实例x的类，f(x)的绝对值表示分类的确信度。利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点。</p><p id="filepos457959" class="calibre_6"><span class="calibre2"><span class="bold">8.1.3　AdaBoost的例子<sup class="calibre15"><small id="filepos458050" class="calibre6"><a href="#filepos498861"><span class="calibre7"><span class="bold">[1]</span></span></a></small></sup></span></span></p><p class="calibre_7"><span class="bold">例8.1　</span>给定如表8.1所示训练数据。假设弱分类器由x&lt;v或x&gt;v产生，其阈值v使该分类器在训练数据集上分类误差率最低。试用AdaBoost算法学习一个强分类器。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表8.1　训练数据表</span></span></p><p class="calibre_20"><img src="images/00382.jpg" class="calibre_617"/></p><p class="calibre_24"><span class="bold">解</span>　初始化数据权值分布</p><p class="calibre_20"><img src="images/00493.jpg" class="calibre_618"/></p><p class="calibre_22">对m＝1，</p><p class="calibre_8">（a）在权值分布为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的训练数据上，阈值v取2.5时分类误差率最低，故基本分类器为</p><p class="calibre_20"><img src="images/00613.jpg" class="calibre_619"/></p><p class="calibre_24">（b）G<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)在训练数据集上的误差率e<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝P(G<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)≠y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)＝0.3。</p><p class="calibre_8">（c）计算G<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)的系数：<img src="images/00718.jpg" class="calibre_620"/>。</p><p class="calibre_8">（d）更新训练数据的权值分布：</p><p class="calibre_20"><img src="images/00829.jpg" class="calibre_431"/></p><p class="calibre_24">分类器sign[f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)]在训练数据集上有3个误分类点。</p><p class="calibre_9">对m＝2，</p><p class="calibre_8">（a）在权值分布为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的训练数据上，阈值v是8.5时分类误差率最低，基本分类器为</p><p class="calibre_20"><img src="images/00936.jpg" class="calibre_621"/></p><p class="calibre_24">（b）G<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>(x)在训练数据集上的误差率e<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝0.2143。</p><p class="calibre_8">（c）计算a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝0.6496。</p><p class="calibre_8">（d）更新训练数据权值分布：</p><p class="calibre_20"><img src="images/01045.jpg" class="calibre_622"/></p><p class="calibre_24">分类器sign[f<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>(x)]在训练数据集上有3个误分类点。</p><p class="calibre_9">对m＝3，</p><p class="calibre_8">（a）在权值分布为D<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>的训练数据上，阈值v是5.5时分类误差率最低，基本分类器为</p><p class="calibre_20"><img src="images/00088.jpg" class="calibre_623"/></p><p class="calibre_24">（b）G<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>(x)在训练样本集上的误差率e<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝0.1820。</p><p class="calibre_8">（c）计算a<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝0.7514。</p><p class="calibre_8">（d）更新训练数据的权值分布：</p><p class="calibre_8">D<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>＝(0.125,0.125,0.125,0.102,0.102,0.102,0.065,0.065,0.065,0.125)</p><p class="calibre_9">于是得到：</p><p class="calibre_20"><img src="images/00405.jpg" class="calibre_624"/></p><p class="calibre_24">分类器sign[f<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>(x)]在训练数据集上误分类点个数为0。</p><p class="calibre_8">于是最终分类器为</p><p class="calibre_20"><img src="images/00957.jpg" class="calibre_625"/></p><p id="filepos462313" class="calibre_6"><span class="calibre9"><span class="bold">8.2　AdaBoost算法的训练误差分析</span></span></p><p class="calibre_7">AdaBoost最基本的性质是它能在学习过程中不断减少训练误差，即在训练数据集上的分类误差率。关于这个问题有下面的定理：</p><p class="calibre_8"><span class="bold">定理8.1（AdaBoost的训练误差界）</span>　AdaBoost算法最终分类器的训练误差界为</p><p class="calibre_20"><img src="images/00145.jpg" class="calibre_626"/></p><p class="calibre_22">这里,G(x),f(x)和Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>分别由式（8.7）、式（8.6）和式（8.5）给出。</p><p class="calibre_8"><span class="bold">证明</span>　当G(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)≠y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>时，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>f(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)&lt;0，因而exp(-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>f(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>))≥1。由此直接推导出前半部分。</p><p class="calibre_8">后半部分的推导要用到Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的定义式（8.5）及式（8.4）的变形：</p><p class="calibre_20"><img src="images/00982.jpg" class="calibre_627"/></p><p class="calibre_22">现推导如下：</p><p class="calibre_20"><img src="images/00294.jpg" class="calibre_628"/></p><p class="calibre_24">这一定理说明，可以在每一轮选取适当的G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>使得Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>最小，从而使训练误差下降最快。对二类分类问题，有如下结果：</p><p class="calibre_146"><span class="bold">定理8.2（二类分类问题AdaBoost的训练误差界）</span></p><p class="calibre_20"><img src="images/01010.jpg" class="calibre_629"/></p><p class="calibre_24">这里，<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>＝<img src="images/00811.jpg" class="calibre_61"/>-e<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>。</p><p class="calibre_8"><span class="bold">证明</span>　由Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的定义式（8.5）及式（8.8）得</p><p class="calibre_20"><img src="images/00487.jpg" class="calibre_630"/></p><p class="calibre_22">至于不等式</p><p class="calibre_20"><img src="images/01038.jpg" class="calibre_631"/></p><p class="calibre_22">则可先由e<sup class="calibre5"><small class="calibre6"><span class="calibre7">x</span></small></sup>和<img src="images/00714.jpg" class="calibre_632"/>在点x＝0的泰勒展开式推出不等式<img src="images/00001.jpg" class="calibre_633"/>进而得到。</p><p class="calibre_8"><span class="bold">推论8.1　</span>如果存在<img src="images/00021.jpg" class="calibre_18"/>&gt;0，对所有m有<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>≥<img src="images/00021.jpg" class="calibre_18"/>，则</p><p class="calibre_20"><img src="images/00550.jpg" class="calibre_260"/></p><p class="calibre_24">这表明在此条件下AdaBoost的训练误差是以指数速率下降的。这一性质当然是很有吸引力的。</p><p class="calibre_8">注意，AdaBoost算法不需要知道下界·。这正是Freund与Schapire设计AdaBoost时所考虑的。与一些早期的提升方法不同，AdaBoost具有适应性，即它能适应弱分类器各自的训练误差率。这也是它的名称（适应的提升）的由来，Ada是Adaptive的简写。</p><p id="filepos466336" class="calibre_6"><span class="calibre9"><span class="bold">8.3　AdaBoost算法的解释</span></span></p><p class="calibre_7">AdaBoost算法还有另一个解释，即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法。</p><p id="filepos466663" class="calibre_6"><span class="calibre2"><span class="bold">8.3.1　前向分步算法</span></span></p><p class="calibre_7">考虑加法模型（additive model）</p><p class="calibre_20"><img src="images/00029.jpg" class="calibre_634"/></p><p class="calibre_22">其中，b(x;<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>)为基函数，<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>为基函数的参数，β<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>为基函数的系数。显然，式（8.6）是一个加法模型。</p><p class="calibre_8">在给定训练数据及损失函数L(Y,f(X))的条件下，学习加法模型f(x)成为经验风险极小化即损失函数极小化问题：</p><p class="calibre_20"><img src="images/00854.jpg" class="calibre_635"/></p><p class="calibre_24">通常这是一个复杂的优化问题。前向分步算法（forward stagewise algorithm）求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（8.14），那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：</p><p class="calibre_20"><img src="images/00400.jpg" class="calibre_636"/></p><p class="calibre_24">给定训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)},x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{-1,+1}。损失函数L(Y,f(X))和基函数的集合{b(x;<img src="images/00021.jpg" class="calibre_18"/>)}，学习加法模型f(x)的前向分步算法如下：</p><p class="calibre_146"><span class="bold">算法8.2（前向分步算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}；损失函数L(Y,f(X))；基函数集{b(x;<img src="images/00021.jpg" class="calibre_18"/>)}；</p><p class="calibre_8">输出：加法模型f(x)。</p><p class="calibre_8">（1）初始化f<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>(x)＝0</p><p class="calibre_8">（2）对m＝1,2,…,M</p><p class="calibre_8">（a）极小化损失函数</p><p class="calibre_20"><img src="images/00612.jpg" class="calibre_637"/></p><p class="calibre_24">得到参数β<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub></p><p class="calibre_8">（b）更新</p><p class="calibre_20"><img src="images/00086.jpg" class="calibre_638"/></p><p class="calibre_24">（3）得到加法模型</p><p class="calibre_20"><img src="images/00637.jpg" class="calibre_639"/></p><p class="calibre_24">这样，前向分步算法将同时求解从m＝1到M所有参数β<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的优化问题简化为逐次求解各个β<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>的优化问题。</p><p id="filepos470999" class="calibre_6"><span class="calibre2"><span class="bold">8.3.2　前向分步算法与AdaBoost</span></span></p><p class="calibre_7">由前向分步算法可以推导出AdaBoost，用定理叙述这一关系。</p><p class="calibre_8"><span class="bold">定理8.3　</span>AdaBoost算法是前向分歩加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。</p><p class="calibre_8"><span class="bold">证明</span>　前向分步算法学习的是加法模型，当基函数为基本分类器时，该加法模型等价于AdaBoost的最终分类器</p><p class="calibre_20"><img src="images/00110.jpg" class="calibre_640"/></p><p class="calibre_22">由基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)及其系数a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>组成，m＝1,2,…,M。前向分步算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。下面证明前向分步算法的损失函数是指数损失函数（exponential loss function）</p><p class="calibre_20"><img src="images/00030.jpg" class="calibre_153"/></p><p class="calibre_22">时，其学习的具体操作等价于AdaBoost算法学习的具体操作。</p><p class="calibre_8">假设经过m-1轮迭代前向分步算法已经得到f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x)：</p><p class="calibre_20"><img src="images/00117.jpg" class="calibre_641"/></p><p class="calibre_22">在第m轮迭代得到a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)和f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)。</p><p class="calibre_20"><img src="images/00240.jpg" class="calibre_642"/></p><p class="calibre_22">目标是使前向分步算法得到的am和G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)使f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)在训练数据集T上的指数损失最小，即</p><p class="calibre_20"><img src="images/00353.jpg" class="calibre_643"/></p><p class="calibre_22">式（8.20）可以表示为</p><p class="calibre_20"><img src="images/00699.jpg" class="calibre_644"/></p><p class="calibre_22">其中，<img src="images/00582.jpg" class="calibre_645"/>＝exp[-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)]。因为<img src="images/00582.jpg" class="calibre_645"/>既不依赖a也不依赖于G，所以与最小化无关。但<img src="images/00582.jpg" class="calibre_645"/>依赖于f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x)，随着每一轮迭代而发生改变。</p><p class="calibre_8">现证使式（8.21）达到最小的<img src="images/00690.jpg" class="calibre_646"/>和<img src="images/00800.jpg" class="calibre_647"/>就是AdaBoost算法所得到的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>和G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)。求解式（8.21）可分两步：</p><p class="calibre_8">首先，求<img src="images/00800.jpg" class="calibre_647"/>。对任意a&gt;0，使式（8.21）最小的G(x)由下式得到：</p><p class="calibre_20"><img src="images/00904.jpg" class="calibre_648"/></p><p class="calibre_22">其中，<img src="images/00582.jpg" class="calibre_645"/>＝exp[-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub> f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)]。</p><p class="calibre_8">此分类器<img src="images/00800.jpg" class="calibre_647"/>即为AdaBoost算法的基本分类器G<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)，因为它是使第m轮加权训练数据分类误差率最小的基本分类器。</p><p class="calibre_8">之后，求<img src="images/00690.jpg" class="calibre_646"/>。参照式（8.11），式（8.21）中</p><p class="calibre_20"><img src="images/01014.jpg" class="calibre_649"/></p><p class="calibre_22">将已求得的<img src="images/00800.jpg" class="calibre_647"/>(x)代入式（8.22），对a求导并使导数为0，即得到使式（8.21）最小的a。</p><p class="calibre_20"><img src="images/00056.jpg" class="calibre_650"/></p><p class="calibre_22">其中，e<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>是分类误差率：</p><p class="calibre_20"><img src="images/00162.jpg" class="calibre_651"/></p><p class="calibre_24">这里的<img src="images/00690.jpg" class="calibre_646"/>与AdaBoost算法第2(c)步的a<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>完全一致。</p><p class="calibre_8">最后来看每一轮样本权值的更新。由</p><p class="calibre_20"><img src="images/00271.jpg" class="calibre_652"/></p><p class="calibre_22">以及<img src="images/00582.jpg" class="calibre_645"/>＝exp[-y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub> f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)]，可得</p><p class="calibre_20"><img src="images/00381.jpg" class="calibre_653"/></p><p class="calibre_24">这与AdaBoost算法第2(d)步的样本权值的更新，只相差规范化因子，因而等价。</p><p id="filepos477011" class="calibre_6"><span class="calibre9"><span class="bold">8.4　提升树</span></span></p><p class="calibre_7">提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。</p><p id="filepos477272" class="calibre_6"><span class="calibre2"><span class="bold">8.4.1　提升树模型</span></span></p><p class="calibre_7">提升方法实际采用加法模型（即基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题决策树是二叉回归树。在例8.1中看到的基本分类器x&lt;v或x&gt;v，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即所谓的决策树桩（decision stump）。提升树模型可以表示为决策树的加法模型：</p><p class="calibre_20"><img src="images/00492.jpg" class="calibre_510"/></p><p class="calibre_22">其中，T(x;Θ<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>)表示决策树；Θ<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>为决策树的参数；M为树的个数。</p><p id="filepos478226" class="calibre_6"><span class="calibre2"><span class="bold">8.4.2　提升树算法</span></span></p><p class="calibre_7">提升树算法采用前向分步算法。首先确定初始提升树f<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>(x)＝0，第m歩的模型是</p><p class="calibre_20"><img src="images/00312.jpg" class="calibre_654"/></p><p class="calibre_22">其中，f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x)为当前模型，通过经验风险极小化确定下一棵决策树的参数Θ<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，</p><p class="calibre_20"><img src="images/00717.jpg" class="calibre_655"/></p><p class="calibre_24">由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法。</p><p class="calibre_8">下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题。</p><p class="calibre_8">对于二类分类问题，提升树算法只需将AdaBoost算法8.1中的基本分类器限制为二类分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况，这里不再细述。下面叙述回归问题的提升树。</p><p class="calibre_8">已知一个训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，x为输入空间，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>⊆R，<img src="images/00940.jpg" class="calibre_29"/>为输出空间。在5.5节中已经讨论了回归树的问题。如果将输入空间x划分为J个互不相交的区域R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,R<sub class="calibre8"><small class="calibre6"><span class="calibre7">J</span></small></sub>，并且在每个区域上确定输出的常量c<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>，那么树可表示为</p><p class="calibre_20"><img src="images/00768.jpg" class="calibre_656"/></p><p class="calibre_22">其中，参数Θ＝{(R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(R<sub class="calibre8"><small class="calibre6"><span class="calibre7">J</span></small></sub>,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">J</span></small></sub>)}表示树的区域划分和各区域上的常数。J是回归树的复杂度即叶结点个数。</p><p class="calibre_8">回归问题提升树使用以下前向分步算法：</p><p class="calibre_20"><img src="images/00935.jpg" class="calibre_657"/></p><p class="calibre_22">在前向分步算法的第m步，给定当前模型f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x)，需求解</p><p class="calibre_20"><img src="images/01044.jpg" class="calibre_658"/></p><p class="calibre_22">得到<img src="images/00922.jpg" class="calibre_79"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，即第m棵树的参数。</p><p class="calibre_8">当采用平方误差损失函数时，</p><p class="calibre_20"><img src="images/00190.jpg" class="calibre_659"/></p><p class="calibre_22">其损失变为</p><p class="calibre_20"><img src="images/00953.jpg" class="calibre_660"/></p><p class="calibre_22">这里，</p><p class="calibre_20"><img src="images/00410.jpg" class="calibre_661"/></p><p class="calibre_22">是当前模型拟合数据的残差（residual）。所以，对回归问题的提升树算法来说，只需简单地拟合当前模型的残差。这样，算法是相当简单的。现将回归问题的提升树算法叙述如下。</p><p class="calibre_146"><span class="bold">算法8.3（回归问题的提升树算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>⊆R；</p><p class="calibre_8">输出：提升树f<sub class="calibre8"><small class="calibre6"><span class="calibre7">M</span></small></sub>(x)。</p><p class="calibre_8">（1）初始化f<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>(x)＝0</p><p class="calibre_8">（2）对m＝1,2,…,M</p><p class="calibre_8">（a）按式（8.27）计算残差</p><p class="calibre_20"><img src="images/00525.jpg" class="calibre_662"/></p><p class="calibre_24">（b）拟合残差r<sub class="calibre8"><small class="calibre6"><span class="calibre7">mi</span></small></sub>学习一个回归树，得到T(x ;Θ<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>)</p><p class="calibre_8">（c）更新f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>(x)＝f<sub class="calibre8"><small class="calibre6"><span class="calibre7">m-1</span></small></sub>(x)+T(x ;Θ<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>)</p><p class="calibre_8">（3）得到回归问题提升树</p><p class="calibre_20"><img src="images/00639.jpg" class="calibre_663"/></p><p class="calibre_24"><span class="bold">例8.2</span>　已知如表8.2所示的训练数据，x的取值范围为区间[0.5,10.5]，y的取值范围为区间[5.0,10.0]，学习这个回归问题的提升树模型，考虑只用树桩作为基函数。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表8.2　训练数据表</span></span></p><p class="calibre_20"><img src="images/00744.jpg" class="calibre_664"/></p><p class="calibre_24"><span class="bold">解</span>　按照算法8.3，第1步求f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)即回归树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)。</p><p class="calibre_8">首先通过以下优化问题：</p><p class="calibre_20"><img src="images/00852.jpg" class="calibre_665"/></p><p class="calibre_22">求解训练数据的切分点s：</p><p class="calibre_20"><img src="images/00962.jpg" class="calibre_28"/></p><p class="calibre_24">容易求得在R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>内部使平方损失误差达到最小值的c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>为</p><p class="calibre_20"><img src="images/00003.jpg" class="calibre_666"/></p><p class="calibre_22">这里N<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，N<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>是R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>的样本点数。</p><p class="calibre_8">求训练数据的切分点。根据所给数据，考虑如下切分点：</p><p class="calibre_8">1.5，2.5，3.5，4.5，5.5，6.5，7.5，8.5，9.5</p><p class="calibre_9">对各切分点，不难求出相应的R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>及</p><p class="calibre_20"><img src="images/00112.jpg" class="calibre_667"/></p><p class="calibre_22">例如，当s＝1.5时，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝{1}，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝{2,3,…,10}，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝5.56，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝7.50，</p><p class="calibre_20"><img src="images/00546.jpg" class="calibre_668"/></p><p class="calibre_24">现将s及m(s)的计算结果列表如下（见表8.3）。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表8.3　计算数据表</span></span></p><p class="calibre_20"><img src="images/00024.jpg" class="calibre_669"/></p><p class="calibre_24">由表8.3可知，当s＝6.5时m(s)达到最小值，此时R<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝{1,2,…,6}，R<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝ {7,8,9,10}，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝6.24，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝8.91，所以回归树T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)为</p><p class="calibre_20"><img src="images/00575.jpg" class="calibre_670"/></p><p class="calibre_24">用f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)拟合训练数据的残差见表8.4，表中r<sub class="calibre8"><small class="calibre6"><span class="calibre7">2i</span></small></sub>＝y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>-f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)，i＝1,2,…,10。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表8.4　残差表</span></span></p><p class="calibre_20"><img src="images/00052.jpg" class="calibre_671"/></p><p class="calibre_24">用f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)拟合训练数据的平方损失误差：</p><p class="calibre_20"><img src="images/00607.jpg" class="calibre_672"/></p><p class="calibre_24">第2步求T<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>(x)。方法与求T<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>(x)一样，只是拟合的数据是表8.4的残差。可以得到：</p><p class="calibre_20"><img src="images/00594.jpg" class="calibre_673"/></p><p class="calibre_24">用f<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>(x)拟合训练数据的平方损失误差是</p><p class="calibre_20"><img src="images/00700.jpg" class="calibre_674"/></p><p class="calibre_22">继续求得</p><p class="calibre_20"><img src="images/00812.jpg" class="calibre_675"/></p><p class="calibre_20"><img src="images/00917.jpg" class="calibre_676"/></p><p class="calibre_24">用f<sub class="calibre8"><small class="calibre6"><span class="calibre7">6</span></small></sub>(x)拟合训练数据的平方损失误差是</p><p class="calibre_20"><img src="images/01024.jpg" class="calibre_677"/></p><p class="calibre_24">假设此时已满足误差要求，那么f(x)＝f<sub class="calibre8"><small class="calibre6"><span class="calibre7">6</span></small></sub>(x)即为所求提升树。</p><p id="filepos490230" class="calibre_6"><span class="calibre2"><span class="bold">8.4.3　梯度提升</span></span></p><p class="calibre_7">提升树利用加法模型与前向分歩算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，Freidman提出了梯度提升（gradient boosting）算法。这是利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值</p><p class="calibre_20"><img src="images/00069.jpg" class="calibre_678"/></p><p class="calibre_22">作为回归问题提升树算法中的残差的近似值，拟合一个回归树。</p><p class="calibre_146"><span class="bold">算法8.4（梯度提升算法）</span></p><p class="calibre_146">输入：训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>⊆R；损失函数L(Y,f(X))；</p><p class="calibre_8">输出：回归树<img src="images/00981.jpg" class="calibre_31"/>(x)。</p><p class="calibre_8">（1）初始化</p><p class="calibre_20"><img src="images/00175.jpg" class="calibre_679"/></p><p class="calibre_24">（2）对m＝1,2,…,M</p><p class="calibre_8">（a）对i＝1,2,…,N，计算</p><p class="calibre_20"><img src="images/00282.jpg" class="calibre_680"/></p><p class="calibre_24">（b）对r<sub class="calibre8"><small class="calibre6"><span class="calibre7">mi</span></small></sub>拟合一个回归树，得到第m棵树的叶结点区域R<sub class="calibre8"><small class="calibre6"><span class="calibre7">mj</span></small></sub>，j＝1,2,…,J</p><p class="calibre_8">（c）对j＝1,2,…,J，计算</p><p class="calibre_20"><img src="images/00395.jpg" class="calibre_681"/></p><p class="calibre_24">（d）更新<img src="images/00506.jpg" class="calibre_682"/></p><p class="calibre_8">（3）得到回归树</p><p class="calibre_20"><img src="images/00622.jpg" class="calibre_565"/></p><p class="calibre_24">算法第1步初始化，估计使损失函数极小化的常数值，它是只有一个根结点的树。第2(a)步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。第2(b)步估计回归树叶结点区域，以拟合残差的近似值。第2(c)步利用线性搜索估计叶结点区域的值，使损失函数极小化。第2(d)步更新回归树。第3步得到输出的最终模型<img src="images/00981.jpg" class="calibre_31"/>(X)。</p><p id="filepos493579" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。</p><p class="calibre_8">AdaBoost模型是弱分类器的线性组合：</p><p class="calibre_20"><img src="images/00728.jpg" class="calibre_683"/></p><p class="calibre_24">2．AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值。</p><p class="calibre_8">3．AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。</p><p class="calibre_8">4．AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。</p><p class="calibre_8">每一步中极小化损失函数</p><p class="calibre_20"><img src="images/00398.jpg" class="calibre_684"/></p><p class="calibre_22">得到参数β<sub class="calibre8"><small class="calibre6"><span class="calibre7">m</span></small></sub>，<img src="images/00021.jpg" class="calibre_18"/>m。</p><p class="calibre_8">5．提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。</p><p id="filepos495606" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">提升方法的介绍可参见文献[1,2]。PAC学习可参见文献[3]。强可学习与弱可学习的关系可参见文献[4]。关于AdaBoost的最初论文是文献[5]。关于AdaBoost的前向分步加法模型解释参见文献[6]，提升树与梯度提升可参见文献[6,7]。AdaBoost只是用于二类分类，Schapire与Singer将它扩展到多类分类问题<sup class="calibre5"><small class="calibre6"><span class="calibre7">[8]</span></small></sup>。AdaBoost与逻辑斯谛回归的关系也有相关研究<sup class="calibre5"><small class="calibre6"><span class="calibre7">[9]</span></small></sup>。</p><p id="filepos496262" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">8.1　某公司招聘职员考查身体、业务能力、发展潜力这3项。身体分为合格1、不合格0两级，业务能力和发展潜力分为上1、中2、下3三级。分类为合格1、不合格–1两类。已知10个人的数据，如下表所示。假设弱分类器为决策树桩。试用AdaBoost算法学习一个强分类器。</p><p class="calibre_20"><span class="calibre2"><span class="bold">应聘人员情况数据表</span></span></p><p class="calibre_20"><img src="images/00947.jpg" class="calibre_685"/></p><p class="calibre_24">8.2　比较支持向量机、AdaBoost、逻辑斯谛回归模型的学习策略与算法。</p><p id="filepos497068" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Freund Y，Schapire RE. A short introduction to boosting. Journal of Japanese Society for Artificial Intelligence,1999,14(5): 771–780</p><p class="calibre_8">[2]　Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: Data Mining,Inference,and Prediction. Springer-Verlag,2001（中译本：统计学习基础——数据挖掘、推理与预测。范明，柴玉梅，昝红英，等译。北京：电子工业出版社，2004）</p><p class="calibre_8">[3]　Valiant LG. A theory of the learnable. Communications of the ACM,1984,27(11): 1134–1142</p><p class="calibre_8">[4]　Schapire R. The strength of weak learnability. Machine Learning,1990,5(2): 197–227</p><p class="calibre_8">[5]　Freund Y,Schapire RE. A decision-theoretic generalization of on-line learning and an application to boosting. Computational Learning Theory. Lecture Notes in Computer Science,Vol. 904,1995,23–37</p><p class="calibre_8">[6]　Friedman J,Hastie T,Tibshirani R. Additive logistic regression: a statistical view of boosting(with discussions). Annals of Statistics,2000,28: 337–407</p><p class="calibre_8">[7]　Friedman J. Greedy function approximation: a gradient boosting machine. Annals of Statistics,2001,29(5)</p><p class="calibre_8">[8]　Schapire RE,Singer Y. Improved boosting algorithms using confidence-rated predictions. Machine Learning,1999,37(3): 297–336</p><p class="calibre_8">[9]　Collins M,Schapire R E,Singer Y. Logistic regression,AdaBoost and Bregman distances. Machine Learning Journal,2004</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos498861" class="calibre_99"><a href="#filepos458050"><span class="calibre4">[1]</span></a><span class="calibre4">　例题来源于http://www.csie.edu.tw。</span></p><div class="mbp_pagebreak" id="calibre_pb_13"></div>
</body></html>
