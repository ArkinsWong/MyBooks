<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos686997" class="calibre_"><span class="calibre1"><span class="bold">第12章　统计学习方法总结</span></span></p><p class="calibre_13">本书共介绍了10种主要的统计学习方法：感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场。这10种统计学习方法的特点概括总结在表12.1中。</p><p class="calibre_20"><span class="calibre2"><span class="bold">表12.1　10种统计学习方法特点的概括总结</span></span></p><p class="calibre_20"><img src="images/01000.jpg" class="calibre_950"/></p><p class="calibre_20"><span class="calibre4">① EM算法在这里有些特殊，它是个一般方法，不具有具体模型。</span></p><p class="calibre_27">下面对各种方法的特点及其关系进行简单的讨论。</p><p class="calibre_7"><span class="bold">1．适用问题</span></p><p class="calibre_7">本书主要介绍监督学习方法。监督学习可以认为是学习一个模型，使它能对给定的输入预测相应的输出。监督学习包括分类、标注、回归。本书主要考虑前两者的学习方法。分类问题是从实例的特征向量到类标记的预测问题，标注问题是从观测序列到标记序列（或状态序列）的预测问题。可以认为分类问题是标注问题的特殊情况。分类问题中可能的预测结果是二类或多类。而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。</p><p class="calibre_8">感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法是分类方法。原始的感知机、支持向量机以及提升方法是针对二类分类的，可以将它们扩展到多类分类。隐马尔可夫模型、条件随机场是标注方法。EM算法是含有隐变量的概率模型的一般学习算法，可以用于生成模型的非监督学习。</p><p class="calibre_8">感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点。逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高。隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更高。</p><p class="calibre_7"><span class="bold">2．模型</span></p><p class="calibre_7">分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射。它们可以写成条件概率分布P(Y|X)或决策函数Y＝f(X)的形式。前者表示给定输入条件下输出的概率模型，后者表示输入到输出的非概率模型。有时，模型更直接地表示为概率模型，或者非概率模型；但有时模型兼有两种解释。</p><p class="calibre_8">朴素贝叶斯法、隐马尔可夫模型是概率模型。感知机、k近邻法、支持向量机、提升方法是非概率模型。而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型。</p><p class="calibre_8">直接学习条件概率分布P(Y|X)或决策函数Y＝f(X)的方法为判别方法，对应的模型是判别模型。感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。首先学习联合概率分布P(X,Y)，从而求得条件概率分布P(Y|X)的方法是生成方法，对应的模型是生成模型。朴素贝叶斯法、隐马尔可夫模型是生成方法。图12.1给出部分模型之间的关系。</p><p class="calibre_8">可以用非监督学习的方法学习生成模型。具体地，应用EM算法可以学习朴素贝叶斯模型以及隐马尔可夫模型。</p><p class="calibre_8">决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间（更一般地，是希尔伯特空间）。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。</p><p class="calibre_8">感知机模型是线性模型，而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型。k近邻法、决策树、支持向量机（包含核函数）、提升方法使用的是非线性模型。</p><p class="calibre_8">图12.1从生成与判别、分类与标注两个方面描述了几个统计学习方法之间的关系。</p><p class="calibre_20"><img src="images/00383.jpg" class="calibre_951"/></p><p class="calibre_20"><span class="calibre4">图12.1　部分模型之间的关系</span></p><p class="calibre_7"><span class="bold">3．学习策略</span></p><p class="calibre_7">在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、逻辑斯谛损失函数、指数损失函数。3种损失函数分别写为</p><p class="calibre_20"><img src="images/01028.jpg" class="calibre_952"/></p><p class="calibre_22">这3种损失函数都是0-1损失函数的上界，具有相似的形状，如图12.2所示。所以，可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数（surrogate loss function）表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数：</p><p class="calibre_20"><img src="images/00511.jpg" class="calibre_486"/></p><p class="calibre_22">这里，第1项为经验风险（经验损失），第2项为正则化项，L(Y,f(X))为损失函数，J(f)为模型的复杂度，<img src="images/00759.jpg" class="calibre_47"/>≥0为系数。</p><p class="calibre_8">支持向量机用L<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上L<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>范数正则化项。提升方法没有显式的正则化项，通常通过早停止（early stopping）的方法达到正则化的效果。</p><p class="calibre_20"><img src="images/00321.jpg" class="calibre_953"/></p><p class="calibre_20"><span class="calibre4">图12.2　0-1损失函数、合页损失函数、逻辑斯谛损失函数、指数损失函数的关系</span></p><p class="calibre_27">以上二类分类的学习方法可以扩展到多类分类学习以及标注问题，比如标注问题的条件随机场可以看作是分类问题的最大熵模型的推广。</p><p class="calibre_8">概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。这时，学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。对数似然损失可以写成</p><p class="calibre_20"><img src="images/00431.jpg" class="calibre_954"/></p><p class="calibre_22">极大后验概率估计时，正则化项是先验概率的负对数。</p><p class="calibre_8">决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。</p><p class="calibre_8">逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计（或正则化的极大似然估计），又可以看成是极小化逻辑斯谛损失（或正则化的逻辑斯谛损失）。</p><p class="calibre_8">朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。</p><p class="calibre_7"><span class="bold">4．学习算法</span></p><p class="calibre_7">统计学习的问题有了具体的形式以后，就变成了最优化问题。有时，最优化问题比较简单，解析解存在，最优解可以由公式简单计算。但在多数情况下，最优化问题没有解析解，需要用数值计算的方法或启发式的方法求解。</p><p class="calibre_8">朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即极大似然估计值，可以由概率计算公式直接计算。</p><p class="calibre_8">感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等。这些都是一般的无约束最优化问题的解法。</p><p class="calibre_8">支持向量机学习，可以解凸二次规划的对偶问题。有序列最小最优化算法等方法。</p><p class="calibre_8">决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。</p><p class="calibre_8">提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。</p><p class="calibre_8">EM算法是一种迭代的求解含隐变量概率模型参数的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。</p><p class="calibre_8">支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题，全局最优解保证存在。而其他学习问题则不是凸优化问题。</p><div class="mbp_pagebreak" id="calibre_pb_17"></div>
</body></html>
