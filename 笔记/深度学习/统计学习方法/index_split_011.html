<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos261872" class="calibre_"><span class="calibre1"><span class="bold">第6章　逻辑斯谛回归与最大熵模型</span></span></p><p class="calibre_13">逻辑斯谛回归（logistic regression）是统计学习中的经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型（maximum entropy model）。逻辑斯谛回归模型与最大熵模型都属于对数线性模型。本章首先介绍逻辑斯谛回归模型，然后介绍最大熵模型，最后讲述逻辑斯谛回归与最大熵模型的学习算法，包括改进的迭代尺度算法和拟牛顿法。</p><p id="filepos262501" class="calibre_6"><span class="calibre9"><span class="bold">6.1　逻辑斯谛回归模型</span></span></p><p id="filepos262607" class="calibre_6"><span class="calibre2"><span class="bold">6.1.1　逻辑斯谛分布</span></span></p><p class="calibre_7">首先介绍逻辑斯谛分布（logistic distribution）。</p><p class="calibre_8"><span class="bold">定义6.1（逻辑斯谛分布）</span>　设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：</p><p class="calibre_20"><img src="images/00710.jpg" class="calibre_282"/></p><p class="calibre_22">式中，<img src="images/01071.jpg" class="calibre_283"/>为位置参数，<img src="images/00021.jpg" class="calibre_18"/>&gt;0为形状参数。</p><p class="calibre_8">逻辑斯谛分布的密度函数f(x)和分布函数F(x)的图形如图6.1所示。分布函数属于逻辑斯谛函数，其图形是一条S形曲线（sigmoid curve）。该曲线以点<img src="images/00044.jpg" class="calibre_198"/>为中心对称，即满足</p><p class="calibre_20"><img src="images/00726.jpg" class="calibre_284"/></p><p class="calibre_20"><img src="images/00730.jpg" class="calibre_285"/></p><p class="calibre_20"><span class="calibre4">图6.1　逻辑斯谛分布的密度函数与分布函数</span></p><p class="calibre_60">曲线在中心附近增长速度较快，在两端增长速度较慢。形状参数<img src="images/00021.jpg" class="calibre_18"/>的值越小，曲线在中心附近增长得越快。</p><p id="filepos264217" class="calibre_6"><span class="calibre2"><span class="bold">6.1.2　二项逻辑斯谛回归模型</span></span></p><p class="calibre_7">二项逻辑斯谛回归模型（binomial logistic regression model）是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的逻辑斯谛分布。这里，随机变量X取值为实数，随机变量Y取值为1或0。我们通过监督学习的方法来估计模型参数。</p><p class="calibre_8"><span class="bold">定义6.2（逻辑斯谛回归模型）</span>　二项逻辑斯谛回归模型是如下的条件概率分布：</p><p class="calibre_20"><img src="images/00732.jpg" class="calibre_286"/></p><p class="calibre_22">这里，x∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>是输入，Y∊{0,1}是输出，w∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>和b∊R是参数，w称为权值向量，b称为偏置，w·x为w和x的内积。</p><p class="calibre_8">对于给定的输入实例x，按照式（6.3）和式（6.4）可以求得P(Y＝1|x)和P(Y＝0|x)。逻辑斯谛回归比较两个条件概率值的大小，将实例x分到概率值较大的那一类。</p><p class="calibre_8">有时为了方便，将权值向量和输入向量加以扩充，仍记作w，x，即w＝(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>,…,w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(n)</span></small></sup>,b)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，x＝(x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>,x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>,…,x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(n)</span></small></sup>,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>。这时，逻辑斯谛回归模型如下：</p><p class="calibre_20"><img src="images/00737.jpg" class="calibre_287"/></p><p class="calibre_24">现在考查逻辑斯谛回归模型的特点。一个事件的几率（odds）是指该事件发生的概率与该事件不发生的概率的比值。如果事件发生的概率是p，那么该事件的几率是<img src="images/00741.jpg" class="calibre_288"/>，该事件的对数几率（log odds）或logit函数是</p><p class="calibre_20"><img src="images/00742.jpg" class="calibre_289"/></p><p class="calibre_24">对逻辑斯谛回归而言，由式（6.5）与式（6.6）得</p><p class="calibre_20"><img src="images/00747.jpg" class="calibre_290"/></p><p class="calibre_22">这就是说，在逻辑斯谛回归模型中，输出Y＝1的对数几率是输入x的线性函数。或者说，输出Y＝1的对数几率是由输入x的线性函数表示的模型，即逻辑斯谛回归模型。</p><p class="calibre_8">换一个角度看，考虑对输入x进行分类的线性函数w·x，其值域为实数域。注意，这里x∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>,w∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>。通过逻辑斯谛回归模型定义式（6.5）可以将线性函数w·x转换为概率：</p><p class="calibre_20"><img src="images/00750.jpg" class="calibre_291"/></p><p class="calibre_22">这时，线性函数的值越接近正无穷，概率值就越接近1；线性函数的值越接近负无穷，概率值就越接近0（如图6.1所示）。这样的模型就是逻辑斯谛回归模型。</p><p id="filepos267791" class="calibre_6"><span class="calibre2"><span class="bold">6.1.3　模型参数估计</span></span></p><p class="calibre_7">逻辑斯谛回归模型学习时，对于给定的训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}，其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊{0,1}，可以应用极大似然估计法估计模型参数，从而得到逻辑斯谛回归模型。</p><p class="calibre_20"><img src="images/00754.jpg" class="calibre_292"/></p><p class="calibre_22">似然函数为</p><p class="calibre_20"><img src="images/00260.jpg" class="calibre_293"/></p><p class="calibre_22">对数似然函数为</p><p class="calibre_20"><img src="images/00279.jpg" class="calibre_294"/></p><p class="calibre_22">对L(w)求极大值，得到w的估计值。</p><p class="calibre_8">这样，问题就变成了以对数似然函数为目标函数的最优化问题。逻辑斯谛回归学习中通常采用的方法是梯度下降法及拟牛顿法。</p><p class="calibre_8">假设w的极大似然估计值是<img src="images/00513.jpg" class="calibre_19"/>，那么学到的逻辑斯谛回归模型为</p><p class="calibre_20"><img src="images/00767.jpg" class="calibre_295"/></p><p id="filepos269607" class="calibre_6"><span class="calibre2"><span class="bold">6.1.4　多项逻辑斯谛回归</span></span></p><p class="calibre_7">上面介绍的逻辑斯谛回归模型是二项分类模型，用于二类分类。可以将其推广为多项逻辑斯谛回归模型（multi-nominal logistic regression model），用于多类分类。假设离散型随机变量Y的取值集合是{1,2,…,K}，那么多项逻辑斯谛回归模型是</p><p class="calibre_20"><img src="images/00772.jpg" class="calibre_296"/></p><p class="calibre_22">这里，x∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">N+1</span></small></sup>。</p><p class="calibre_8">二项逻辑斯谛回归的参数估计法也可以推广到多项逻辑斯谛回归。</p><p id="filepos270492" class="calibre_6"><span class="calibre9"><span class="bold">6.2　最大熵模型</span></span></p><p class="calibre_7">最大熵模型（maximum entropy model）由最大熵原理推导实现。这里首先叙述一般的最大熵原理，然后讲解最大熵模型的推导，最后给出最大熵模型学习的形式。</p><p id="filepos270825" class="calibre_6"><span class="calibre2"><span class="bold">6.2.1　最大熵原理</span></span></p><p class="calibre_7">最大熵原理是概率模型学习的一个准则。最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以，最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。</p><p class="calibre_8">假设离散随机变量X的概率分布是P(X)，则其熵（参照5.2.2节）是</p><p class="calibre_20"><img src="images/00775.jpg" class="calibre_297"/></p><p class="calibre_22">熵满足下列不等式：</p><p class="calibre_20"><img src="images/00370.jpg" class="calibre_298"/></p><p class="calibre_22">式中，|X|是X的取值个数，当且仅当X的分布是均匀分布时右边的等号成立。这就是说，当X服从均匀分布时，熵最大。</p><p class="calibre_8">直观地，最大熵原理认为要选择的概率模型首先必须满足已有的事实，即约束条件。在没有更多信息的情况下，那些不确定的部分都是“等可能的”。最大熵原理通过熵的最大化来表示等可能性。“等可能”不容易操作，而熵则是一个可优化的数值指标。</p><p class="calibre_8">首先，通过一个简单的例子来介绍一下最大熵原理<sup class="calibre5"><small id="filepos272364" class="calibre6"><a href="#filepos310526"><span class="calibre7">[1]</span></a></small></sup>。</p><p class="calibre_8"><span class="bold">例6.1</span>　假设随机变量X有5个取值{A，B，C，D，E}，要估计取各个值的概率P(A),P(B),P(C),P(D),P(E)。</p><p class="calibre_8"><span class="bold">解</span>　这些概率值满足以下约束条件：</p><p class="calibre_20"><img src="images/00782.jpg" class="calibre_299"/></p><p class="calibre_22">满足这个约束条件的概率分布有无穷多个。如果没有任何其他信息，仍要对概率分布进行估计，一个办法就是认为这个分布中取各个值的概率是相等的：</p><p class="calibre_20"><img src="images/00786.jpg" class="calibre_300"/></p><p class="calibre_22">等概率表示了对事实的无知。因为没有更多的信息，这种判断是合理的。</p><p class="calibre_8">有时，能从一些先验知识中得到一些对概率值的约束条件，例如：</p><p class="calibre_20"><img src="images/00791.jpg" class="calibre_301"/></p><p class="calibre_22">满足这两个约束条件的概率分布仍然有无穷多个。在缺少其他信息的情况下，可以认为A与B是等概率的，C，D与E是等概率的，于是，</p><p class="calibre_20"><img src="images/00795.jpg" class="calibre_302"/></p><p class="calibre_24">如果还有第3个约束条件：</p><p class="calibre_20"><img src="images/00797.jpg" class="calibre_303"/></p><p class="calibre_22">可以继续按照满足约束条件下求等概率的方法估计概率分布。这里不再继续讨论。以上概率模型学习的方法正是遵循了最大熵原理。</p><p class="calibre_8">图6.2提供了用最大熵原理进行概率模型选择的几何解释。概率模型集合P可由欧氏空间中的单纯形（simplex）<sup class="calibre5"><small id="filepos274401" class="calibre6"><a href="#filepos310664"><span class="calibre7">[2]</span></a></small></sup>表示，如左图的三角形（2-单纯形）。一个点代表一个模型，整个单纯形代表模型集合。右图上的一条直线对应于一个约束条件，直线的交集对应于满足所有约束条件的模型集合。一般地，这样的模型仍有无穷多个。学习的目的是在可能的模型集合中选择最优模型，而最大熵原理则给出最优模型选择的一个准则。</p><p class="calibre_20"><img src="images/00801.jpg" class="calibre_304"/></p><p class="calibre_20"><span class="calibre4">图6.2　概率模型集合</span></p><p id="filepos275103" class="calibre_6"><span class="calibre2"><span class="bold">6.2.2　最大熵模型的定义</span></span></p><p class="calibre_7">最大熵原理是统计学习的一般原理，将它应用到分类得到最大熵模型。</p><p class="calibre_8">假设分类模型是一个条件概率分布P(Y|X)，X∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>表示输入，Y∊<img src="images/00940.jpg" class="calibre_29"/>表示输出，x和<img src="images/00940.jpg" class="calibre_29"/>分别是输入和输出的集合。这个模型表示的是对于给定的输入X，以条件概率P(Y|X)输出Y。</p><p class="calibre_8">给定一个训练数据集</p><p class="calibre_20"><img src="images/00805.jpg" class="calibre_305"/></p><p class="calibre_22">学习的目标是用最大熵原理选择最好的分类模型。</p><p class="calibre_8">首先考虑模型应该满足的条件。给定训练数据集，可以确定联合分布P(X,Y)的经验分布和边缘分布P(X)的经验分布，分别以<img src="images/00541.jpg" class="calibre_306"/>(X,Y)和<img src="images/00541.jpg" class="calibre_306"/>(X)表示。这里，</p><p class="calibre_20"><img src="images/00545.jpg" class="calibre_307"/></p><p class="calibre_22">其中，v(X＝x,Y＝y)表示训练数据中样本（X,Y）出现的频数，v(X＝x)表示训练数据中输入x出现的频数，N表示训练样本容量。</p><p class="calibre_8">用特征函数（feature function）f(X,Y)描述输入x和输出y之间的某一个事实。其定义是</p><p class="calibre_20"><img src="images/00547.jpg" class="calibre_308"/></p><p class="calibre_22">它是一个二值函数<sup class="calibre5"><small id="filepos277007" class="calibre6"><a href="#filepos310854"><span class="calibre7">[3]</span></a></small></sup>，当x和y满足这个事实时取值为1，否则取值为0。</p><p class="calibre_8">特征函数f(X,Y)关于经验分布<img src="images/00541.jpg" class="calibre_306"/>(X,Y)的期望值，用E<sub class="calibre8"><small class="calibre6"><img src="images/00541.jpg" class="calibre_306"/></small></sub>(f)表示。</p><p class="calibre_20"><img src="images/00551.jpg" class="calibre_309"/></p><p class="calibre_24">特征函数f(X,Y)关于模型P(Y|X)与经验分布<img src="images/00541.jpg" class="calibre_306"/>(X)的期望值，用E<sub class="calibre8"><small class="calibre6"><span class="calibre7">P</span></small></sub>(f)表示。</p><p class="calibre_20"><img src="images/00556.jpg" class="calibre_310"/></p><p class="calibre_24">如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即</p><p class="calibre_20"><img src="images/00561.jpg" class="calibre_311"/></p><p class="calibre_24">或</p><p class="calibre_20"><img src="images/00566.jpg" class="calibre_312"/></p><p class="calibre_22">我们将式（6.10）或式（6.11）作为模型学习的约束条件。假如有n个特征函数f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(X,Y)，i＝1,2,…,n，那么就有n个约束条件。</p><p class="calibre_8"><span class="bold">定义6.3（最大熵模型）</span>　假设满足所有约束条件的模型集合为</p><p class="calibre_20"><img src="images/00569.jpg" class="calibre_313"/></p><p class="calibre_22">定义在条件概率分布P(Y|X)上的条件熵为</p><p class="calibre_20"><img src="images/00573.jpg" class="calibre_314"/></p><p class="calibre_22">则模型集合<img src="images/00576.jpg" class="calibre_306"/>中条件熵H(P)最大的模型称为最大熵模型。式中的对数为自然对数。</p><p id="filepos279146" class="calibre_6"><span class="calibre2"><span class="bold">6.2.3　最大熵模型的学习</span></span></p><p class="calibre_7">最大熵模型的学习过程就是求解最大熵模型的过程。最大熵模型的学习可以形式化为约束最优化问题。</p><p class="calibre_8">对于给定的训练数据集T＝{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}以及特征函数f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(X,Y)，i＝1,2,…,n，最大熵模型的学习等价于约束最优化问题：</p><p class="calibre_20"><img src="images/00580.jpg" class="calibre_315"/></p><p class="calibre_24">按照最优化问题的习惯，将求最大值问题改写为等价的求最小值问题：</p><p class="calibre_20"><img src="images/00585.jpg" class="calibre_316"/></p><p class="calibre_24">求解约束最优化问题（6.14）～（6.16），所得出的解，就是最大熵模型学习的解。下面给出具体推导。</p><p class="calibre_8">这里，将约束最优化的原始问题转换为无约束最优化的对偶问题<sup class="calibre5"><small id="filepos280599" class="calibre6"><a href="#filepos311016"><span class="calibre7">[4]</span></a></small></sup>。通过求解对偶问题求解原始问题。</p><p class="calibre_8">首先，引进拉格朗日乘子w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>，定义拉格朗日函数L(P,w)：</p><p class="calibre_20"><img src="images/00589.jpg" class="calibre_317"/></p><p class="calibre_24">最优化的原始问题是</p><p class="calibre_20"><img src="images/00593.jpg" class="calibre_318"/></p><p class="calibre_24">对偶问题是</p><p class="calibre_20"><img src="images/00596.jpg" class="calibre_319"/></p><p class="calibre_24">由于拉格朗日函数L(P,w)是P的凸函数，原始问题（6.18）的解与对偶问题（6.19）的解是等价的。这样，可以通过求解对偶问题（6.19）来求解原始问题（6.18）。</p><p class="calibre_8">首先，求解对偶问题（6.19）内部的极小化问题<img src="images/00600.jpg" class="calibre_320"/>是w的函数，将其记作</p><p class="calibre_20"><img src="images/00605.jpg" class="calibre_321"/></p><p class="calibre_22"><img src="images/00609.jpg" class="calibre_322"/>(w)称为对偶函数。同时，将其解记作</p><p class="calibre_20"><img src="images/00611.jpg" class="calibre_323"/></p><p class="calibre_24">具体地，求L(P,w)对P(Y|X)的偏导数</p><p class="calibre_20"><img src="images/00615.jpg" class="calibre_324"/></p><p class="calibre_22">令偏导数等于0，在<img src="images/00541.jpg" class="calibre_306"/>(x)&gt;0的情况下，解得</p><p class="calibre_20"><img src="images/00617.jpg" class="calibre_325"/></p><p class="calibre_22">由于<img src="images/00621.jpg" class="calibre_326"/>，得</p><p class="calibre_20"><img src="images/00627.jpg" class="calibre_327"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/00631.jpg" class="calibre_328"/></p><p class="calibre_22">Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>(x)称为规范化因子；f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(X,Y)是特征函数；w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>是特征的权值。由式（6.22）、式（6.23）表示的模型P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>＝P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>(Y|X)就是最大熵模型。这里，w是最大熵模型中的参数向量。</p><p class="calibre_8">之后，求解对偶问题外部的极大化问题</p><p class="calibre_20"><img src="images/00634.jpg" class="calibre_329"/></p><p class="calibre_22">将其解记为w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，即</p><p class="calibre_20"><img src="images/00635.jpg" class="calibre_330"/></p><p class="calibre_24">这就是说，可以应用最优化算法求对偶函数<img src="images/00609.jpg" class="calibre_322"/>(w)的极大化，得到w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，用来表示P<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>∊<img src="images/00576.jpg" class="calibre_306"/>。这里，P<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w<sup class="calibre14"><small class="calibre13"><span class="calibre13">*</span></small></sup></span></small></sub>＝P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w<sup class="calibre14"><small class="calibre13"><span class="calibre13">*</span></small></sup></span></small></sub>(Y|X)是学习到的最优模型（最大熵模型）。也就是说，最大熵模型的学习归结为对偶函数<img src="images/00609.jpg" class="calibre_322"/>(w)的极大化。</p><p class="calibre_8"><span class="bold">例6.2</span>　学习例6.1中的最大熵模型。</p><p class="calibre_8"><span class="bold">解</span>　为了方便，分别以y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">4</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">5</span></small></sub>表示A，B，C，D和E，于是最大熵模型学习的最优化问题是</p><p class="calibre_20"><img src="images/00640.jpg" class="calibre_331"/></p><p class="calibre_24">引进拉格朗日乘子w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，定义拉格朗日函数</p><p class="calibre_20"><img src="images/00644.jpg" class="calibre_332"/></p><p class="calibre_24">根据拉格朗日对偶性，可以通过求解对偶最优化问题得到原始最优化问题的解，所以求解</p><p class="calibre_20"><img src="images/00648.jpg" class="calibre_333"/></p><p class="calibre_24">首先求解L(P,w)关于P的极小化问题。为此，固定w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，求偏导数：</p><p class="calibre_20"><img src="images/00654.jpg" class="calibre_334"/></p><p class="calibre_24">令各偏导数等于0，解得</p><p class="calibre_20"><img src="images/00658.jpg" class="calibre_335"/></p><p class="calibre_22">于是，</p><p class="calibre_20"><img src="images/00661.jpg" class="calibre_336"/></p><p class="calibre_24">再求解L(P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>,w)关于w的极大化问题：</p><p class="calibre_20"><img src="images/00664.jpg" class="calibre_337"/></p><p class="calibre_22">分别求L(P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>,w)对w<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的偏导数并令其为0，得到</p><p class="calibre_20"><img src="images/00666.jpg" class="calibre_338"/></p><p class="calibre_22">于是得到所要求的概率分布为</p><p class="calibre_20"><img src="images/00670.jpg" class="calibre_339"/></p><p id="filepos287467" class="calibre_6"><span class="calibre2"><span class="bold">6.2.4　极大似然估计</span></span></p><p class="calibre_7">从以上最大熵模型学习中可以看出，最大熵模型是由式（6.22）、式（6.23）表示的条件概率分布。下面证明对偶函数的极大化等价于最大熵模型的极大似然估计。</p><p class="calibre_8">已知训练数据的经验概率分布<img src="images/00541.jpg" class="calibre_306"/>(X,Y)，条件概率分布P(Y|X)的对数似然函数表示为</p><p class="calibre_20"><img src="images/00673.jpg" class="calibre_340"/></p><p class="calibre_22">当条件概率分布P(Y|X)是最大熵模型（6.22）和（6.23）时，对数似然函数L<img src="images/00541.jpg" class="calibre_306"/>(P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>)为</p><p class="calibre_20"><img src="images/00565.jpg" class="calibre_341"/></p><p class="calibre_24">再看对偶函数<img src="images/00609.jpg" class="calibre_322"/>(w)。由式（6.17）及式（6.20）可得<img src="images/00588.jpg" class="calibre_342"/></p><p class="calibre_20"><img src="images/00610.jpg" class="calibre_343"/></p><p class="calibre_22">最后一步用到<img src="images/00702.jpg" class="calibre_344"/>。</p><p class="calibre_8">比较式（6.26）和式（6.27），可得</p><p class="calibre_20"><img src="images/00281.jpg" class="calibre_345"/></p><p class="calibre_22">既然对偶函数<img src="images/00609.jpg" class="calibre_322"/>(w)等价于对数似然函数L<sub class="calibre8"><small class="calibre6"><img src="images/00541.jpg" class="calibre_306"/></small></sub>(P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>)，于是证明了最大熵模型学习中的对偶函数极大化等价于最大熵模型的极大似然估计这一事实。</p><p class="calibre_8">这样，最大熵模型的学习问题就转换为具体求解对数似然函数极大化或对偶函数极大化的问题。</p><p class="calibre_8">可以将最大熵模型写成更一般的形式。</p><p class="calibre_20"><img src="images/00285.jpg" class="calibre_346"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/01026.jpg" class="calibre_347"/></p><p class="calibre_22">这里，x∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>为输入，y∊{1,2,…,K}为输出，w∊R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>为权值向量，f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>(X,Y)，i＝1,2,…,n为任意实值特征函数。</p><p class="calibre_8">最大熵模型与逻辑斯谛回归模型有类似的形式，它们又称为对数线性模型（log linear model）。模型学习就是在给定的训练数据条件下对模型进行极大似然估计或正则化的极大似然估计。</p><p id="filepos290657" class="calibre_6"><span class="calibre9"><span class="bold">6.3　模型学习的最优化算法</span></span></p><p class="calibre_7">逻辑斯谛回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解。从最优化的观点看，这时的目标函数具有很好的性质。它是光滑的凸函数，因此多种最优化的方法都适用，保证能找到全局最优解。常用的方法有改进的迭代尺度法、梯度下降法、牛顿法或拟牛顿法。牛顿法或拟牛顿法一般收敛速度更快。</p><p class="calibre_8">下面介绍基于改进的迭代尺度法与拟牛顿法的最大熵模型学习算法。梯度下降法参阅附录A。</p><p id="filepos291410" class="calibre_6"><span class="calibre2"><span class="bold">6.3.1　改进的迭代尺度法</span></span></p><p class="calibre_7">改进的迭代尺度法（improved iterative scaling，IIS）是一种最大熵模型学习的最优化算法。</p><p class="calibre_8">已知最大熵模型为</p><p class="calibre_20"><img src="images/00715.jpg" class="calibre_348"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/00739.jpg" class="calibre_349"/></p><p class="calibre_24">对数似然函数为</p><p class="calibre_20"><img src="images/00297.jpg" class="calibre_350"/></p><p class="calibre_22">目标是通过极大似然估计学习模型参数，即求对数似然函数的极大值<img src="images/00513.jpg" class="calibre_19"/>。</p><p class="calibre_8">IIS的想法是：假设最大熵模型当前的参数向量是w＝(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，我们希望找到一个新的参数向量w+<img src="images/00850.jpg" class="calibre_73"/>＝(w<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>+<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>+<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,w<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>+<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，使得模型的对数似然函数值增大。如果能有这样一种参数向量更新的方法<img src="images/00784.jpg" class="calibre_351"/>(w):w→w+<img src="images/00850.jpg" class="calibre_73"/>，那么就可以重复使用这一方法，直至找到对数似然函数的最大值。</p><p class="calibre_8">对于给定的经验分布<img src="images/00541.jpg" class="calibre_306"/>(X,Y)，模型参数从w到w+<img src="images/00850.jpg" class="calibre_73"/>，对数似然函数的改变量是</p><p class="calibre_20"><img src="images/00510.jpg" class="calibre_352"/></p><p class="calibre_24">利用不等式</p><p class="calibre_20"><img src="images/00626.jpg" class="calibre_353"/></p><p class="calibre_22">建立对数似然函数改变量的下界：</p><p class="calibre_20"><img src="images/00316.jpg" class="calibre_354"/></p><p class="calibre_22">将右端记为</p><p class="calibre_20"><img src="images/00319.jpg" class="calibre_355"/></p><p class="calibre_22">于是有</p><p class="calibre_20"><img src="images/00950.jpg" class="calibre_356"/></p><p class="calibre_22">即A(<img src="images/00850.jpg" class="calibre_73"/>|w)是对数似然函数改变量的一个下界。</p><p class="calibre_8">如果能找到适当的<img src="images/00850.jpg" class="calibre_73"/>使下界A(<img src="images/00850.jpg" class="calibre_73"/>|w)提高，那么对数似然函数也会提高。然而，函数A(<img src="images/00850.jpg" class="calibre_73"/>|w)中的<img src="images/00850.jpg" class="calibre_73"/>是一个向量，含有多个变量，不易同时优化。IIS试图一次只优化其中一个变量<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>，而固定其他变量<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>，i≠j。</p><p class="calibre_8">为达到这一目的，IIS进一步降低下界A(<img src="images/00850.jpg" class="calibre_73"/>|w)。具体地，IIS引进一个量f<sup class="calibre5"><small class="calibre6"><span class="calibre7">#</span></small></sup>(X,Y)，</p><p class="calibre_20"><img src="images/01061.jpg" class="calibre_357"/></p><p class="calibre_22">因为f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>是二值函数，故f<sup class="calibre5"><small class="calibre6"><span class="calibre7">#</span></small></sup>(x,y)表示所有特征在(X,Y)出现的次数。这样，A(<img src="images/00850.jpg" class="calibre_73"/>|w)可以改写为</p><p class="calibre_20"><img src="images/00099.jpg" class="calibre_358"/></p><p class="calibre_22">利用指数函数的凸性以及对任意i，有<img src="images/00360.jpg" class="calibre_359"/>这一事实，根据Jensen不等式，得到</p><p class="calibre_20"><img src="images/00314.jpg" class="calibre_360"/></p><p class="calibre_22">于是式（6.30）可改写为</p><p class="calibre_20"><img src="images/00386.jpg" class="calibre_361"/></p><p class="calibre_22">记不等式（6.31）右端为</p><p class="calibre_20"><img src="images/00539.jpg" class="calibre_362"/></p><p class="calibre_22">于是得到</p><p class="calibre_20"><img src="images/00656.jpg" class="calibre_363"/></p><p class="calibre_22">这里，B(<img src="images/00850.jpg" class="calibre_73"/>|w)是对数似然函数改变量的一个新的（相对不紧的）下界。</p><p class="calibre_8">求B(<img src="images/00850.jpg" class="calibre_73"/>|w)对<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的偏导数：</p><p class="calibre_20"><img src="images/00966.jpg" class="calibre_364"/></p><p class="calibre_22">在式（6.32）里，除<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>外不含任何其他变量。令偏导数为0得到</p><p class="calibre_20"><img src="images/00865.jpg" class="calibre_365"/></p><p class="calibre_22">于是，依次对<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>求解方程（6.33）可以求出<img src="images/00850.jpg" class="calibre_73"/>。</p><p class="calibre_8">这就给出了一种求w的最优解的迭代算法，即改进的迭代尺度算法IIS。</p><p class="calibre_146"><span class="bold">算法6.1（改进的迭代尺度算法IIS）</span></p><p class="calibre_146">输入：特征函数f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,f<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,f<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>；经验分布<img src="images/00541.jpg" class="calibre_306"/>(X,Y)，模型P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>(Y|X)</p><p class="calibre_8">输出：最优参数值<img src="images/00068.jpg" class="calibre_218"/>；最优模型P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w<sup class="calibre14"><small class="calibre13"><span class="calibre13">*</span></small></sup></span></small></sub>。</p><p class="calibre_8">（1）对所有i∊{1,2,…,n}，取初值w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝0</p><p class="calibre_8">（2）对每一i∊{1,2,…,n}：</p><p class="calibre_8">（a）令<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>是方程</p><p class="calibre_20"><img src="images/00976.jpg" class="calibre_366"/></p><p class="calibre_22">的解，这里，</p><p class="calibre_20"><img src="images/00015.jpg" class="calibre_367"/></p><p class="calibre_24">　　（b）更新w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>值：w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>←w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>+<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub></p><p class="calibre_8">（3）如果不是所有w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>都收敛，重复步（2）。</p><p class="calibre_8">这一算法关键的一步是(a)，即求解方程（6.33）中的<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。如果f<sup class="calibre5"><small class="calibre6"><span class="calibre7">#</span></small></sup>(X,Y)是常数，即对任何x,y，有f<sup class="calibre5"><small class="calibre6"><span class="calibre7">#</span></small></sup>(X,Y)＝M，那么<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>可以显式地表示成</p><p class="calibre_20"><img src="images/00124.jpg" class="calibre_368"/></p><p class="calibre_24">如果f<sup class="calibre5"><small class="calibre6"><span class="calibre7">#</span></small></sup>(X,Y)不是常数，那么必须通过数值计算求<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>。简单有效的方法是牛顿法。以g(<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>)＝0表示方程（6.33），牛顿法通过迭代求得<img src="images/00072.jpg" class="calibre_219"/>，使得g(<img src="images/00072.jpg" class="calibre_219"/>)＝0。迭代公式是</p><p class="calibre_20"><img src="images/00234.jpg" class="calibre_369"/></p><p class="calibre_22">只要适当选取初始值<img src="images/00342.jpg" class="calibre_370"/>，由于<img src="images/00850.jpg" class="calibre_73"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>的方程（6.33）有单根，因此牛顿法恒收敛，而且收敛速度很快。</p><p id="filepos302026" class="calibre_6"><span class="calibre2"><span class="bold">6.3.2　拟牛顿法</span></span></p><p class="calibre_7">最大熵模型学习还可以应用牛顿法或拟牛顿法。参阅附录B。</p><p class="calibre_8">对于最大熵模型而言，</p><p class="calibre_20"><img src="images/00450.jpg" class="calibre_371"/></p><p class="calibre_24">目标函数：</p><p class="calibre_20"><img src="images/00571.jpg" class="calibre_372"/></p><p class="calibre_24">梯度：</p><p class="calibre_20"><img src="images/00681.jpg" class="calibre_373"/></p><p class="calibre_22">其中</p><p class="calibre_20"><img src="images/00790.jpg" class="calibre_374"/></p><p class="calibre_24">相应的拟牛顿法BFGS算法如下。</p><p class="calibre_146"><span class="bold">算法6.2（最大熵模型学习的BFGS算法）</span></p><p class="calibre_146">输入：特征函数f<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,f<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,f<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>；经验分布<img src="images/00541.jpg" class="calibre_306"/>(X,Y)，目标函数f(w),梯度g(w)＝<img src="images/00895.jpg" class="calibre_375"/>f(w),精度要求<img src="images/00860.jpg" class="calibre_76"/>；</p><p class="calibre_8">输出：最优参数值w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>；最优模型P<sub class="calibre8"><small class="calibre6"><span class="calibre7">w<sup class="calibre14"><small class="calibre13"><span class="calibre13">*</span></small></sup></span></small></sub>(Y|X)。</p><p class="calibre_8">（1）选定初始点w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>，取B<sub class="calibre8"><small class="calibre6"><span class="calibre7">0</span></small></sub>为正定对称矩阵，置k＝0</p><p class="calibre_8">（2）计算g<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>＝g(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k)</span></small></sup>)。若||g<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>||&lt;<img src="images/00860.jpg" class="calibre_76"/>，则停止计算，得w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k)</span></small></sup>；否则转（3）</p><p class="calibre_8">（3）由B<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub> p<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>＝-g<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>求出p<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub></p><p class="calibre_8">（4）一维搜索：求<img src="images/00759.jpg" class="calibre_47"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>使得</p><p class="calibre_20"><img src="images/01004.jpg" class="calibre_376"/></p><p class="calibre_24">（5）置w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k-1)</span></small></sup>＝w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k)</span></small></sup>+<img src="images/00759.jpg" class="calibre_47"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>p<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub></p><p class="calibre_8">（6）计算g<sub class="calibre8"><small class="calibre6"><span class="calibre7">k-1</span></small></sub>＝g(w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k-1)</span></small></sup>)，若||g<sub class="calibre8"><small class="calibre6"><span class="calibre7">k-1</span></small></sub>||&lt;<img src="images/00860.jpg" class="calibre_76"/>，则停止计算，得w<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>＝w<sup class="calibre5"><small class="calibre6"><span class="calibre7">(k-1)</span></small></sup>；否则，按下式求出B<sub class="calibre8"><small class="calibre6"><span class="calibre7">k-1</span></small></sub>：</p><p class="calibre_20"><img src="images/00135.jpg" class="calibre_377"/></p><p class="calibre_22">其中，</p><p class="calibre_20"><img src="images/00137.jpg" class="calibre_378"/></p><p class="calibre_24">（7）置k＝k+1，转（3）。</p><p id="filepos306083" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．逻辑斯谛回归模型是由以下条件概率分布表示的分类模型。逻辑斯谛回归模型可以用于二类或多类分类。</p><p class="calibre_20"><img src="images/00141.jpg" class="calibre_379"/></p><p class="calibre_20"><img src="images/01027.jpg" class="calibre_380"/></p><p class="calibre_22">这里，x为输入特征，w为特征的权值。</p><p class="calibre_8">逻辑斯谛回归模型源自逻辑斯谛分布，其分布函数F(x)是S形函数。逻辑斯谛回归模型是由输入的线性函数表示的输出的对数几率模型。</p><p class="calibre_8">2．最大熵模型是由以下条件概率分布表示的分类模型。最大熵模型也可以用于二类或多类分类。</p><p class="calibre_20"><img src="images/00149.jpg" class="calibre_381"/></p><p class="calibre_22">其中，Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">w</span></small></sub>(x)是规范化因子，f<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为特征函数，w<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>为特征的权值。</p><p class="calibre_8">3．最大熵模型可以由最大熵原理推导得出。最大熵原理是概率模型学习或估计的一个准则。最大熵原理认为在所有可能的概率模型（分布）的集合中，熵最大的模型是最好的模型。</p><p class="calibre_8">最大熵原理应用到分类模型的学习中，有以下约束最优化问题：</p><p class="calibre_20"><img src="images/00153.jpg" class="calibre_382"/></p><p class="calibre_22">求解此最优化问题的对偶问题得到最大熵模型。</p><p class="calibre_8">4．逻辑斯谛回归模型与最大熵模型都属于对数线性模型。</p><p class="calibre_8">5．逻辑斯谛回归模型及最大熵模型学习一般采用极大似然估计，或正则化的极大似然估计。逻辑斯谛回归模型及最大熵模型学习可以形式化为无约束最优化问题。求解该最优化问题的算法有改进的迭代尺度法、梯度下降法、拟牛顿法。</p><p id="filepos308445" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">逻辑斯谛回归的介绍参见文献[1]，最大熵模型的介绍参见文献[2,3]。逻辑斯谛回归模型与朴素贝叶斯模型的关系参见文献[4]，逻辑斯谛回归模型与AdaBoost的关系参见文献[5]，逻辑斯谛回归模型与核函数的关系参见文献[6]。</p><p id="filepos308854" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">6.1　确认逻辑斯谛分布属于指数分布族。</p><p class="calibre_8">6.2　写出逻辑斯谛回归模型学习的梯度下降算法。</p><p class="calibre_8">6.3　写出最大熵模型学习的DFP算法。（关于一般的DFP算法参见附录B）</p><p id="filepos309243" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Berger A,Della Pietra SD,Pietra VD. A maximum entropy approach to natural language processing. Computational Linguistics,1996,22(1),39–71</p><p class="calibre_8">[2]　Berger A. The Improved Iterative Scaling Algorithm: A Gentle Introduction. http://www.cs.cmu.edu/ afs/cs/user/aberger/www/ps/scaling.ps</p><p class="calibre_8">[3]　Hastie T,Tibshirani R，Friedman J. The Elements of Statistical Learning: Data Mining,Inference,and Prediction. Springer-Verlag. 2001（中译本：统计学习基础——数据挖掘、推理与预测。范明，柴玉梅，昝红英等译。北京:电子工业出版社，2004）</p><p class="calibre_8">[4]　Mitchell TM. Machine Learning. McGraw-Hill Companies,Inc. 1997（中译本：机器学习。北京:机械工业出版社，2003）</p><p class="calibre_8">[5]　Collins M,Schapire RE,Singer Y. Logistic Regression,AdaBoost and Bregman Distances. Machine Learning Journal,2004</p><p class="calibre_8">[6]　Canu S,Smola AJ. Kernel method and exponential family. Neurocomputing,2005,69: 714–720</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos310526" class="calibre_99"><a href="#filepos272364"><span class="calibre4">[1]</span></a><span class="calibre4">　此例来自参考文献[1]。</span></p><p id="filepos310664" class="calibre_8"><a href="#filepos274401"><span class="calibre4">[2]</span></a><span class="calibre4">　单纯形是在n维欧氏空间中的n+1个仿射无关的点的集合的凸包。</span></p><p id="filepos310854" class="calibre_8"><a href="#filepos277007"><span class="calibre4">[3]</span></a><span class="calibre4">　一般地，特征函数可以是任意实值函数。</span></p><p id="filepos311016" class="calibre_8"><a href="#filepos280599"><span class="calibre4">[4]</span></a><span class="calibre4">　参阅附录C。</span></p><div class="mbp_pagebreak" id="calibre_pb_11"></div>
</body></html>
