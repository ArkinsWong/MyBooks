<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos153508" class="calibre_"><span class="calibre1"><span class="bold">第3章　k近邻法</span></span></p><p class="calibre_13">k近邻法（k-nearest neighbor，k-NN）是一种基本分类与回归方法。本书只讨论分类问题中的k近邻法。k近邻法的输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时，对新的实例，根据其k个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k近邻法不具有显式的学习过程。k近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素。k近邻法1968年由Cover和Hart提出。</p><p class="calibre_8">本章首先叙述k近邻算法，然后讨论k近邻法的模型及三个基本要素，最后讲述k近邻法的一个实现方法——kd树，介绍构造kd树和搜索kd树的算法。</p><p id="filepos154592" class="calibre_6"><span class="calibre9"><span class="bold">3.1　k近邻算法</span></span></p><p class="calibre_7">k近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。下面先叙述k近邻算法，然后再讨论其细节。</p><p class="calibre_146"><span class="bold">算法3.1（k近邻法）</span></p><p class="calibre_146">输入：训练数据集</p><p class="calibre_20"><img src="images/00521.jpg" class="calibre_161"/></p><p class="calibre_22">其中，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊x⊆R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>为实例的特征向量，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>∊<img src="images/00940.jpg" class="calibre_29"/>＝{c<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，c<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,c<sub class="calibre8"><small class="calibre6"><span class="calibre7">K</span></small></sub>}为实例的类别，i＝1,2,…,N；实例特征向量x；</p><p class="calibre_8">输出：实例x所属的类y。</p><p class="calibre_8">（1）根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的邻域记作N<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>(x)；</p><p class="calibre_8">（2）在N<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>(x)中根据分类决策规则（如多数表决）决定x的类别y：</p><p class="calibre_20"><img src="images/00522.jpg" class="calibre_162"/></p><p class="calibre_22">式（3.1）中，I为指示函数，即当y<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>＝c<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>时I为1，否则I为0。</p><p class="calibre_8">k近邻法的特殊情况是k＝1的情形，称为最近邻算法。对于输入的实例点（特征向量）x，最近邻法将训练数据集中与x最邻近点的类作为x的类。</p><p class="calibre_8">k近邻法没有显式的学习过程。</p><p id="filepos156801" class="calibre_6"><span class="calibre9"><span class="bold">3.2　k近邻模型</span></span></p><p class="calibre_7">k近邻法使用的模型实际上对应于对特征空间的划分。模型由三个基本要素——距离度量、k值的选择和分类决策规则决定。</p><p id="filepos157089" class="calibre_6"><span class="calibre2"><span class="bold">3.2.1　模型</span></span></p><p class="calibre_7">k近邻法中，当训练集、距离度量（如欧氏距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一地确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里的每个点所属的类。这一事实从最近邻算法中可以看得很清楚。</p><p class="calibre_8">特征空间中，对每个训练实例点ix，距离该点比其他点更近的所有点组成一个区域，叫作单元（cell）。每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。最近邻法将实例ix的类iy作为其单元中所有点的类标记（class label）。这样，每个单元的实例点的类别是确定的。图3.1是二维特征空间划分的一个例子。</p><p class="calibre_20"><img src="images/01015.jpg" class="calibre_163"/></p><p class="calibre_20"><span class="calibre4">图3.1　</span><span class="calibre4"><span class="italic">k</span></span><span class="calibre4">近邻法的模型对应特征空间的一个划分</span></p><p id="filepos158356" class="calibre_6"><span class="calibre2"><span class="bold">3.2.2　距离度量</span></span></p><p class="calibre_7">特征空间中两个实例点的距离是两个实例点相似程度的反映。k近邻模型的特征空间一般是n维实数向量空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>。使用的距离是欧氏距离，但也可以是其他距离，如更一般的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>距离（L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub> distance）或Minkowski距离（Minkowski distance）。</p><p class="calibre_8">设特征空间x是n维实数向量空间R<sup class="calibre5"><small class="calibre6"><span class="calibre7">n</span></small></sup>，<img src="images/00532.jpg" class="calibre_164"/>，<img src="images/01063.jpg" class="calibre_165"/>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">i</span></small></sub>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>距离定义为</p><p class="calibre_20"><img src="images/00268.jpg" class="calibre_166"/></p><p class="calibre_22">这里p≥1。当p＝2时，称为欧氏距离(Euclidean distance)，即</p><p class="calibre_20"><img src="images/00270.jpg" class="calibre_167"/></p><p class="calibre_22">当p＝1时，称为曼哈顿距离（Manhattan distance），即</p><p class="calibre_20"><img src="images/00273.jpg" class="calibre_168"/></p><p class="calibre_22">当p＝<img src="images/00633.jpg" class="calibre_169"/>时，它是各个坐标距离的最大值，即</p><p class="calibre_20"><img src="images/00711.jpg" class="calibre_170"/></p><p class="calibre_24">图3.2给出了二维空间中p取不同值时，与原点的L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>距离为1（L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>＝1）的点的图形。</p><p class="calibre_20"><img src="images/00205.jpg" class="calibre_171"/></p><p class="calibre_20"><span class="calibre4">图3.2　L<sub class="calibre11"><small class="calibre6"><span class="calibre7">p</span></small></sub>距离间的关系</span></p><p class="calibre_27">下面的例子说明，由不同的距离度量所确定的最近邻点是不同的。</p><p class="calibre_8"><span class="bold">例3.1</span>　已知二维空间的3个点x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝(1,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>＝(5,1)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>＝(4,4)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，试求在p取不同值时，L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>距离下x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的最近邻点。</p><p class="calibre_8"><span class="bold">解</span>　因为x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>和x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>只有第二维上值不同，所以p为任何值时，L<sub class="calibre8"><small class="calibre6"><span class="calibre7">p</span></small></sub>(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>)＝4。而</p><p class="calibre_20"><img src="images/00695.jpg" class="calibre_172"/></p><p class="calibre_22">于是得到：p等于1或2时，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的最近邻点；p大于等于3时，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">3</span></small></sub>是x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>的最近邻点。</p><p id="filepos162162" class="calibre_6"><span class="calibre2"><span class="bold">3.2.3　k值的选择</span></span></p><p class="calibre_7">k值的选择会对k近邻法的结果产生重大影响。</p><p class="calibre_8">如果选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，“学习”的近似误差（approximation error）会减小，只有与输入实例较近的（相似的）训练实例才会对预测结果起作用。但缺点是“学习”的估计误差（estimation error）会增大，预测结果会对近邻的实例点非常敏感<sup class="calibre5"><small class="calibre6"><span class="calibre7">[2]</span></small></sup>。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。</p><p class="calibre_8">如果选择较大的k值，就相当于用较大邻域中的训练实例进行预测。其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时与输入实例较远的（不相似的）训练实例也会对预测起作用，使预测发生错误。k值的增大就意味着整体的模型变得简单。</p><p class="calibre_8">如果k＝N，那么无论输入实例是什么，都将简单地预测它属于在训练实例中最多的类。这时，模型过于简单，完全忽略训练实例中的大量有用信息，是不可取的。</p><p class="calibre_8">在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。</p><p id="filepos163718" class="calibre_6"><span class="calibre2"><span class="bold">3.2.4　分类决策规则</span></span></p><p class="calibre_7">k近邻法中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类。</p><p class="calibre_8">多数表决规则（majority voting rule）有如下解释：如果分类的损失函数为0-1损失函数，分类函数为</p><p class="calibre_20"><img src="images/00232.jpg" class="calibre_173"/></p><p class="calibre_22">那么误分类的概率是</p><p class="calibre_20"><img src="images/00295.jpg" class="calibre_174"/></p><p class="calibre_22">对给定的实例x∊x，其最近邻的k个训练实例点构成集合N<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>(x)。如果涵盖N<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>(x)的区域的类别是c<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>，那么误分类率是</p><p class="calibre_20"><img src="images/00185.jpg" class="calibre_175"/></p><p class="calibre_22">要使误分类率最小即经验风险最小，就要使<img src="images/00301.jpg" class="calibre_176"/>最大，所以多数表决规则等价于经验风险最小化。</p><p id="filepos165104" class="calibre_6"><span class="calibre9"><span class="bold">3.3　k近邻法的实现：kd树</span></span></p><p class="calibre_7">实现k近邻法时，主要考虑的问题是如何对训练数据进行快速k近邻搜索。这点在特征空间的维数大及训练数据容量大时尤其必要。</p><p class="calibre_8">k近邻法最简单的实现方法是线性扫描（linear scan）。这时要计算输入实例与每一个训练实例的距离。当训练集很大时，计算非常耗时，这种方法是不可行的。</p><p class="calibre_8">为了提高k近邻搜索的效率，可以考虑使用特殊的结构存储训练数据，以减少计算距离的次数。具体方法很多，下面介绍其中的kd树（kd tree）方法<sup class="calibre5"><small id="filepos165877" class="calibre6"><a href="#filepos179333"><span class="calibre7">[1]</span></a></small></sup>。</p><p id="filepos165956" class="calibre_6"><span class="calibre2"><span class="bold">3.3.1　构造kd树</span></span></p><p class="calibre_7">kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd树是二叉树，表示对k维空间的一个划分（partition）。构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的k维超矩形区域。kd树的每个结点对应于一个k维超矩形区域。</p><p class="calibre_8">构造kd树的方法如下：构造根结点，使根结点对应于k维空间中包含所有实例点的超矩形区域；通过下面的递归方法，不断地对k维空间进行切分，生成子结点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上。</p><p class="calibre_8">通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数（median）<sup class="calibre5"><small id="filepos167284" class="calibre6"><a href="#filepos179581"><span class="calibre7">[2]</span></a></small></sup>为切分点，这样得到的kd树是平衡的。注意，平衡的kd树搜索时的效率未必是最优的。</p><p class="calibre_8">下面给出构造kd树的算法。</p><p class="calibre_146"><span class="bold">算法3.2（构造平衡kd树）</span></p><p class="calibre_146">输入：k维空间数据集T＝{x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>}，</p><p class="calibre_8">其中<img src="images/00806.jpg" class="calibre_177"/>，i＝1,2,…,N；</p><p class="calibre_8">输出：kd树。</p><p class="calibre_8">（1）开始：构造根结点，根结点对应于包含T的k维空间的超矩形区域。</p><p class="calibre_8">选择x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>为坐标轴，以T中所有实例的x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>垂直的超平面实现。</p><p class="calibre_8">由根结点生成深度为1的左、右子结点：左子结点对应坐标x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>小于切分点的子区域，右子结点对应于坐标x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>大于切分点的子区域。</p><p class="calibre_8">将落在切分超平面上的实例点保存在根结点。</p><p class="calibre_8">（2）重复：对深度为j的结点，选择x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(l)</span></small></sup>为切分的坐标轴，l＝j(modk)+1，以该结点的区域中所有实例的x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(l)</span></small></sup>坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(l)</span></small></sup>垂直的超平面实现。</p><p class="calibre_8">由该结点生成深度为j+1的左、右子结点：左子结点对应坐标x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(l)</span></small></sup>小于切分点的子区域，右子结点对应坐标x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(l)</span></small></sup>大于切分点的子区域。</p><p class="calibre_8">将落在切分超平面上的实例点保存在该结点。</p><p class="calibre_8">（3）直到两个子区域没有实例存在时停止。从而形成kd树的区域划分。</p><p class="calibre_8"><span class="bold">例3.2　</span>给定一个二维空间的数据集：</p><p class="calibre_20"><img src="images/00310.jpg" class="calibre_178"/></p><p class="calibre_22">构造一个平衡kd树<sup class="calibre5"><small id="filepos170169" class="calibre6"><a href="#filepos179797"><span class="calibre7">[3]</span></a></small></sup>。</p><p class="calibre_8"><span class="bold">解</span>　根结点对应包含数据集T的矩形，选择x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>轴，6个数据点的x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>坐标的中位数是7，以平面x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(1)</span></small></sup>＝7将空间分为左、右两个子矩形（子结点）；接着，左矩形以x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>＝4分为两个子矩形，右矩形以x<sup class="calibre5"><small class="calibre6"><span class="calibre7">(2)</span></small></sup>＝6分为两个子矩形，如此递归，最后得到如图3.3所示的特征空间划分和如图3.4所示的kd树。</p><p class="calibre_20"><img src="images/00846.jpg" class="calibre_179"/></p><p class="calibre_20"><span class="calibre4">图3.3　特征空间划分</span></p><p class="calibre_180"><img src="images/00740.jpg" class="calibre_181"/></p><p class="calibre_20"><span class="calibre4">图3.4　kd树示例</span></p><p id="filepos171320" class="calibre_6"><span class="calibre2"><span class="bold">3.3.2　搜索kd树</span></span></p><p class="calibre_7">下面介绍如何利用kd树进行k近邻搜索。可以看到，利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。这里以最近邻为例加以叙述，同样的方法可以应用到k近邻。</p><p class="calibre_8">给定一个目标点，搜索其最近邻。首先找到包含目标点的叶结点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止。这样搜索就被限制在空间的局部区域上，效率大为提高。</p><p class="calibre_8">包含目标点的叶结点对应包含目标点的最小超矩形区域。以此叶结点的实例点作为当前最近点。目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部（参阅图3.5）。然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点。如果存在这样的点，将此点作为新的当前最近点。算法转到更上一级的父结点，继续上述过程。如果父结点的另一子结点的超矩形区域与超球体不相交，或不存在比当前最近点更近的点，则停止搜索。</p><p class="calibre_20"><img src="images/00887.jpg" class="calibre_182"/></p><p class="calibre_20"><span class="calibre4">图3.5　通过kd树搜索最近邻</span></p><p class="calibre_27">下面叙述用kd树的最近邻搜索算法。</p><p class="calibre_146"><span class="bold">算法3.3（用kd树的最近邻搜索）</span></p><p class="calibre_146">输入：已构造的kd树；目标点x；</p><p class="calibre_8">输出：x的最近邻。</p><p class="calibre_8">（1）在kd树中找出包含目标点x的叶结点：从根结点出发，递归地向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。</p><p class="calibre_8">（2）以此叶结点为“当前最近点”。</p><p class="calibre_8">（3）递归地向上回退，在每个结点进行以下操作：</p><p class="calibre_8">（a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”。</p><p class="calibre_8">（b）当前最近点一定存在于该结点一个子结点对应的区域。检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与以目标点为球心、以目标点与“当前最近点”间的距离为半径的超球体相交。</p><p class="calibre_8">如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索；</p><p class="calibre_8">如果不相交，向上回退。</p><p class="calibre_8">（4）当回退到根结点时，搜索结束。最后的“当前最近点”即为x的最近邻点。</p><p class="calibre_8">如果实例点是随机分布的，kd树搜索的平均计算复杂度是O(logN)，这里N是训练实例数。kd树更适用于训练实例数远大于空间维数时的k近邻搜索。当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描。</p><p class="calibre_8">下面通过一个例题来说明搜索方法。</p><p class="calibre_8"><span class="bold">例3.3</span>　给定一个如图3.5所示的kd树，根结点为A，其子结点为B，C等。树上共存储7个实例点；另有一个输入目标实例点S，求S的最近邻。</p><p class="calibre_8"><span class="bold">解</span>　首先在kd树中找到包含点S的叶结点D（图中的右下区域），以点D作为近似最近邻。真正最近邻一定在以点S为中心通过点D的圆的内部。然后返回结点D的父结点B，在结点B的另一子结点F的区域内搜索最近邻。结点F的区域与圆不相交，不可能有最近邻点。继续返回上一级父结点A，在结点A的另一子结点C的区域内搜索最近邻。结点C的区域与圆相交；该区域在圆内的实例点有点E，点E比点D更近，成为新的最近邻近似。最后得到点E是点S的最近邻。</p><p id="filepos175878" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．k近邻法是基本且简单的分类与回归方法。k近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的k个最近邻训练实例点，然后利用这k个训练实例点的类的多数来预测输入实例点的类。</p><p class="calibre_8">2．k近邻模型对应于基于训练数据集对特征空间的一个划分。k近邻法中，当训练集、距离度量、k值及分类决策规则确定后，其结果唯一确定。</p><p class="calibre_8">3．k近邻法三要素：距离度量、k值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。k值小时，k近邻模型更复杂；k值大时，k近邻模型更简单。k值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的k。常用的分类决策规则是多数表决，对应于经验风险最小化。</p><p class="calibre_8">4．k近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对k维空间的一个划分，其每个结点对应于k维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</p><p id="filepos177335" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">k近邻法由Cover与Hart提出<sup class="calibre5"><small class="calibre6"><span class="calibre7">[1]</span></small></sup>。k近邻法相关的理论在文献[2,3]中已有论述。k近邻法的扩展可参考文献[4]。kd树及其他快速搜索算法可参见文献[5]。关于k近邻法的介绍可参考文献[2]。</p><p id="filepos177736" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">3.1　参照图3.1，在二维空间中给出实例点，画出k为1和2时的k近邻法构成的空间划分，并对其进行比较，体会k值选择与模型复杂度及预测准确率的关系。</p><p class="calibre_8">3.2　利用例题3.2构造的kd树求点x＝(3,4.5)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>的最近邻点。</p><p class="calibre_8">3.3　参照算法3.3，写出输出为x的k近邻的算法。</p><p id="filepos178295" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Cover T,Hart P. Nearest neighbor pattern classification. IEEE Transactions on Information Theory,1967</p><p class="calibre_8">[2]　Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: Data Mining,Inference,and Prediction,2001（中译本：统计学习基础——数据挖掘、推理与预测。范明，柴玉梅，昝红英等译。北京：电子工业出版社，2004）</p><p class="calibre_8">[3]　Friedman J. Flexible metric nearest neighbor classification. Technical Report,1994</p><p class="calibre_8">[4]　Weinberger KQ,Blitzer J,Saul LK. Distance metric learning for large margin nearest neighbor classification. In: Proceedings of the NIPS. 2005</p><p class="calibre_8">[5]　Samet H. The Design and Analysis of Spatial Data Structures. Reading,MA: Addison-Wesley,1990</p><p class="calibre_98"><span class="calibre2"><span class="bold">注释</span></span></p><p id="filepos179333" class="calibre_99"><a href="#filepos165877"><span class="calibre4">[1]</span></a><span class="calibre4">　kd树是存储k维空间数据的树结构，这里的k与k近邻法的k意义不同，为了与习惯一致，本书仍用kd树的名称。</span></p><p id="filepos179581" class="calibre_8"><a href="#filepos167284"><span class="calibre4">[2]</span></a><span class="calibre4">　一组数据按大小顺序排列起来，处在中间位置的一个数或最中间两个数的平均值。</span></p><p id="filepos179797" class="calibre_8"><a href="#filepos170169"><span class="calibre4">[3]</span></a><span class="calibre4">　取自Wikipedia。</span></p><div class="mbp_pagebreak" id="calibre_pb_8"></div>
</body></html>
