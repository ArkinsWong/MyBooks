<?xml version='1.0' encoding='utf-8'?>
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>统计学习方法</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
  <link href="stylesheet.css" rel="stylesheet" type="text/css"/>
<link href="page_styles.css" rel="stylesheet" type="text/css"/>
</head>
  <body class="calibre">
<p id="filepos499025" class="calibre_"><span class="calibre1"><span class="bold">第9章　EM算法及其推广</span></span></p><p class="calibre_13">EM算法是一种迭代算法，1977年由Dempster等人总结提出，用于含有隐变量（hidden variable）的概率模型参数的极大似然估计，或极大后验概率估计。EM算法的每次迭代由两步组成：E步，求期望（expectation）；M步，求极大（maximization）。所以这一算法称为期望极大算法（expectation maximization algorithm），简称EM算法。本章首先叙述EM算法，然后讨论EM算法的收敛性；作为EM算法的应用，介绍高斯混合模型的学习；最后叙述EM算法的推广——GEM算法。</p><p id="filepos499740" class="calibre_6"><span class="calibre9"><span class="bold">9.1　EM算法的引入</span></span></p><p class="calibre_7">概率模型有时既含有观测变量（observable variable），又含有隐变量或潜在变量（latent variable）。如果概率模型的变量都是观测变量，那么给定数据，可以直接用极大似然估计法，或贝叶斯估计法估计模型参数。但是，当模型含有隐变量时，就不能简单地使用这些估计方法。EM算法就是含有隐变量的概率模型参数的极大似然估计法，或极大后验概率估计法。我们仅讨论极大似然估计，极大后验概率估计与其类似。</p><p id="filepos500414" class="calibre_6"><span class="calibre2"><span class="bold">9.1.1　EM算法</span></span></p><p class="calibre_7">首先介绍一个使用EM算法的例子。</p><p class="calibre_8"><span class="bold">例9.1（三硬币模型）</span>　假设有3枚硬币，分别记作A，B，C。这些硬币正面出现的概率分别是<img src="images/01057.jpg" class="calibre_686"/>，p和q。进行如下掷硬币试验：先掷硬币A，根据其结果选出硬币B或硬币C，正面选硬币B，反面选硬币C；然后掷选出的硬币，掷硬币的结果，出现正面记作1，出现反面记作0；独立地重复n次试验（这里，n＝10），观测结果如下：</p><p class="calibre_33">1,1,0,1,0,0,1,0,1,1</p><p class="calibre_9">假设只能观测到掷硬币的结果，不能观测掷硬币的过程。问如何估计三硬币正面出现的概率，即三硬币模型的参数。</p><p class="calibre_8"><span class="bold">解</span>　三硬币模型可以写作</p><p class="calibre_20"><img src="images/00725.jpg" class="calibre_687"/></p><p class="calibre_22">这里，随机变量y是观测变量，表示一次试验观测的结果是1或0；随机变量z是隐变量，表示未观测到的掷硬币A的结果；<img src="images/00692.jpg" class="calibre_18"/>＝(<img src="images/01057.jpg" class="calibre_686"/>,p，q)是模型参数。这一模型是以上数据的生成模型。注意，随机变量y的数据可以观测，随机变量z的数据不可观测。</p><p class="calibre_8">将观测数据表示为Y＝(Y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，Y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,Y<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，未观测数据表示为Z＝(Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,Z<sub class="calibre8"><small class="calibre6"><span class="calibre7">n</span></small></sub>)<sup class="calibre5"><small class="calibre6"><span class="calibre7">T</span></small></sup>，则观测数据的似然函数为</p><p class="calibre_20"><img src="images/00202.jpg" class="calibre_688"/></p><p class="calibre_22">即</p><p class="calibre_20"><img src="images/00858.jpg" class="calibre_689"/></p><p class="calibre_22">考虑求模型参数<img src="images/00692.jpg" class="calibre_18"/>＝(<img src="images/01057.jpg" class="calibre_686"/>,p,q)的极大似然估计，即</p><p class="calibre_20"><img src="images/00423.jpg" class="calibre_690"/></p><p class="calibre_24">这个问题没有解析解，只有通过迭代的方法求解。EM算法就是可以用于求解这个问题的一种迭代算法。下面给出针对以上问题的EM算法，其推导过程省略。</p><p class="calibre_8">EM算法首先选取参数的初值，记作<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝(<img src="images/01057.jpg" class="calibre_686"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>,p<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>,q<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>)，然后通过下面的步骤迭代计算参数的估计值，直至收敛为止。第i次迭代参数的估计值为<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>＝(<img src="images/01057.jpg" class="calibre_686"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>,p<sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>,q<sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)。EM算法的第i+1次迭代如下。</p><p class="calibre_8">E步：计算在模型参数<img src="images/01057.jpg" class="calibre_686"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，p<sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，q<sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>下观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>来自掷硬币B的概率</p><p class="calibre_20"><img src="images/00537.jpg" class="calibre_691"/></p><p class="calibre_24">M步：计算模型参数的新估计值</p><p class="calibre_20"><img src="images/00653.jpg" class="calibre_692"/></p><p class="calibre_24">进行数字计算。假设模型参数的初值取为</p><p class="calibre_20"><img src="images/01064.jpg" class="calibre_693"/></p><p class="calibre_22">由式（9.5），对y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>＝1与y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>＝0均有<img src="images/00863.jpg" class="calibre_449"/>＝0.5。</p><p class="calibre_8">利用迭代公式（9.6）～（9.8），得到</p><p class="calibre_20"><img src="images/00973.jpg" class="calibre_694"/></p><p class="calibre_22">由式（9.5），</p><p class="calibre_20"><img src="images/00012.jpg" class="calibre_695"/></p><p class="calibre_22">继续迭代，得</p><p class="calibre_20"><img src="images/00121.jpg" class="calibre_696"/></p><p class="calibre_22">于是得到模型参数<img src="images/00692.jpg" class="calibre_18"/>的极大似然估计：</p><p class="calibre_20"><img src="images/00230.jpg" class="calibre_697"/></p><p class="calibre_24"><img src="images/01057.jpg" class="calibre_686"/>＝0.5表示硬币A是均匀的，这一结果容易理解。</p><p class="calibre_8">如果取初值<img src="images/01057.jpg" class="calibre_686"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.4，p<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.6，q<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.7，那么得到的模型参数的极大似然估计是<img src="images/00080.jpg" class="calibre_586"/>＝0.4064，<img src="images/00697.jpg" class="calibre_30"/>＝0.5368，<img src="images/00448.jpg" class="calibre_698"/>＝0.6432。这就是说，EM算法与初值的选择有关，选择不同的初值可能得到不同的参数估计值。</p><p class="calibre_8">一般地，用Y表示观测随机变量的数据，Z表示隐随机变量的数据。Y和Z连在一起称为完全数据（complete-data），观测数据Y又称为不完全数据（incomplete-data）。假设给定观测数据Y，其概率分布是P(Y|<img src="images/00692.jpg" class="calibre_18"/>)，其中<img src="images/00692.jpg" class="calibre_18"/>是需要估计的模型参数，那么不完全数据Y的似然函数是P(Y|<img src="images/00692.jpg" class="calibre_18"/>)，对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)；假设Y和Z的联合概率分布是P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)，那么完全数据的对数似然函数是logP(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)。</p><p class="calibre_8">EM算法通过迭代求L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)的极大似然估计。每次迭代包含两步：E步，求期望；M步，求极大化。下面来介绍EM算法。</p><p class="calibre_146"><span class="bold">算法9.1（EM算法）</span></p><p class="calibre_146">输入：观测变量数据Y，隐变量数据Z，联合分布P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)，条件分布P(Z|Y,<img src="images/00692.jpg" class="calibre_18"/>)；</p><p class="calibre_8">输出：模型参数<img src="images/00692.jpg" class="calibre_18"/>。</p><p class="calibre_8">（1）选择参数的初值<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>，开始迭代；</p><p class="calibre_8">（2）E步：记<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>为第i次迭代参数<img src="images/00692.jpg" class="calibre_18"/>的估计值，在第i+1次迭代的E步，计算</p><p class="calibre_20"><img src="images/00567.jpg" class="calibre_699"/></p><p class="calibre_22">这里，P(Z|Y,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)是在给定观测数据Y和当前的参数估计<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>下隐变量数据Z的条件概率分布；</p><p class="calibre_8">（3） M步：求使Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)极大化的<img src="images/00692.jpg" class="calibre_18"/>，确定第i+1次迭代的参数的估计值<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup></p><p class="calibre_20"><img src="images/00663.jpg" class="calibre_700"/></p><p class="calibre_24">（4）重复第（2）步和第（3）步，直到收敛。</p><p class="calibre_8">式（9.9）的函数Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)是EM算法的核心，称为Q函数（Q function）。</p><p class="calibre_8"><span class="bold">定义9.1（Q函数）</span>　完全数据的对数似然函数logP(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)关于在给定观测数据Y和当前参数<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>下对未观测数据Z的条件概率分布P(Z|Y,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的期望称为Q函数，即</p><p class="calibre_20"><img src="images/00787.jpg" class="calibre_701"/></p><p class="calibre_24">下面关于EM算法作几点说明：</p><p class="calibre_8">步骤（1）　参数的初值可以任意选择，但需注意EM算法对初值是敏感的。</p><p class="calibre_8">步骤（2）　E步求Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)。Q函数式中Z是未观测数据，Y是观测数据。注意，Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的第1个变元表示要极大化的参数，第2个变元表示参数的当前估计值。每次迭代实际在求Q函数及其极大。</p><p class="calibre_8">步骤（3）　M步求Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的极大化，得到<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>，完成一次迭代<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>→<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>。后面将证明每次迭代使似然函数增大或达到局部极值。</p><p class="calibre_8">步骤（4）　给出停止迭代的条件，一般是对较小的正数<img src="images/00860.jpg" class="calibre_76"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,<img src="images/00860.jpg" class="calibre_76"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，若满足</p><p class="calibre_20"><img src="images/00891.jpg" class="calibre_702"/></p><p class="calibre_22">则停止迭代。</p><p id="filepos513510" class="calibre_6"><span class="calibre2"><span class="bold">9.1.2　EM算法的导出</span></span></p><p class="calibre_7">上面叙述了EM算法。为什么EM算法能近似实现对观测数据的极大似然估计呢？下面通过近似求解观测数据的对数似然函数的极大化问题来导出EM算法，由此可以清楚地看出EM算法的作用。</p><p class="calibre_8">我们面对一个含有隐变量的概率模型，目标是极大化观测数据（不完全数据）Y关于参数<img src="images/00692.jpg" class="calibre_18"/>的对数似然函数，即极大化</p><p class="calibre_20"><img src="images/00159.jpg" class="calibre_703"/></p><p class="calibre_22">注意到这一极大化的主要困难是式（9.12）中有未观测数据并有包含和（或积分）的对数。</p><p class="calibre_8">事实上，EM算法是通过迭代逐步近似极大化L(<img src="images/00692.jpg" class="calibre_18"/>)的。假设在第i次迭代后<img src="images/00692.jpg" class="calibre_18"/>的估计值是<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>。我们希望新估计值<img src="images/00692.jpg" class="calibre_18"/>能使L(<img src="images/00692.jpg" class="calibre_18"/>)增加，即L(<img src="images/00692.jpg" class="calibre_18"/>)&gt;L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)，并逐步达到极大值。为此，考虑两者的差：</p><p class="calibre_20"><img src="images/00043.jpg" class="calibre_347"/></p><p class="calibre_22">利用Jensen不等式（Jensen inequality）得到其下界：</p><p class="calibre_20"><img src="images/00186.jpg" class="calibre_704"/></p><p class="calibre_22">令</p><p class="calibre_20"><img src="images/00628.jpg" class="calibre_705"/></p><p class="calibre_22">则</p><p class="calibre_20"><img src="images/00716.jpg" class="calibre_706"/></p><p class="calibre_22">即函数B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)是L(<img src="images/00692.jpg" class="calibre_18"/>)的一个下界，而且由式（9.13）可知，</p><p class="calibre_20"><img src="images/00827.jpg" class="calibre_261"/></p><p class="calibre_22">因此，任何可以使B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)增大的<img src="images/00692.jpg" class="calibre_18"/>，也可以使L(<img src="images/00692.jpg" class="calibre_18"/>)增大。为了使L(<img src="images/00692.jpg" class="calibre_18"/>)有尽可能大的增长，选择<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)达到极大，即</p><p class="calibre_20"><img src="images/00933.jpg" class="calibre_707"/></p><p class="calibre_22">现在求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>的表达式。省去对<img src="images/00692.jpg" class="calibre_18"/>的极大化而言是常数的项，由式（9.16）、式（9.13）及式（9.10），有</p><p class="calibre_20"><img src="images/01043.jpg" class="calibre_708"/></p><p class="calibre_22">式（9.17）等价于EM算法的一次迭代，即求Q函数及其极大化。EM算法是通过不断求解下界的极大化逼近求解对数似然函数极大化的算法。</p><p class="calibre_8">图9.1给出EM算法的直观解释。图中上方曲线为L(<img src="images/00692.jpg" class="calibre_18"/>)，下方曲线为B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)。由式（9.14），B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)为对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)的下界。由式（9.15），两个函数在点<img src="images/00692.jpg" class="calibre_18"/>＝<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>处相等。由式（9.16）和式（9.17），EM算法找到下一个点<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1</span></small></sup>)使函数B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)极大化，也使函数Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)极大化。这时由于L(<img src="images/00692.jpg" class="calibre_18"/>)≥B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)，函数B(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的增加，保证对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)在每次迭代中也是增加的。EM算法在点<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>重新计算Q函数值，进行下一次迭代。在这个过程中，对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)不断增大。从图可以推断出EM算法不能保证找到全局最优值。</p><p class="calibre_20"><img src="images/00921.jpg" class="calibre_709"/></p><p class="calibre_20"><span class="calibre4">图9.1　EM算法的解释</span></p><p id="filepos520645" class="calibre_6"><span class="calibre2"><span class="bold">9.1.3　EM算法在非监督学习中的应用</span></span></p><p class="calibre_7">监督学习是由训练数据{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>，y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>)}学习条件概率分布P(Y|X)或决策函数Y＝f(X)作为模型，用于分类、回归、标注等任务。这时训练数据中的每个样本点由输入和输出对组成。</p><p class="calibre_8">有时训练数据只有输入没有对应的输出{(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,·),(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,·),…,(x<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,·)}，从这样的数据学习模型称为非监督学习问题。EM算法可以用于生成模型的非监督学习。生成模型由联合概率分布P(X,Y)表示，可以认为非监督学习训练数据是联合概率分布产生的数据。X为观测数据，Y为未观测数据。</p><p id="filepos521868" class="calibre_6"><span class="calibre9"><span class="bold">9.2　EM算法的收敛性</span></span></p><p class="calibre_7">EM算法提供一种近似计算含有隐变量概率模型的极大似然估计的方法。EM算法的最大优点是简单性和普适性。我们很自然地要问：EM算法得到的估计序列是否收敛？如果收敛，是否收敛到全局最大值或局部极大值？下面给出关于EM算法收敛性的两个定理。</p><p class="calibre_8"><span class="bold">定理9.1　</span>设P(Y|<img src="images/00692.jpg" class="calibre_18"/>)为观测数据的似然函数，<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>（i＝1,2,…）为EM算法得到的参数估计序列，P(Y|<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)（i＝1,2,…）为对应的似然函数序列，则P(Y|<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)是单调递增的，即</p><p class="calibre_20"><img src="images/00189.jpg" class="calibre_710"/></p><p class="calibre_24"><span class="bold">证明</span>　由于</p><p class="calibre_20"><img src="images/00296.jpg" class="calibre_711"/></p><p class="calibre_22">取对数有</p><p class="calibre_20"><img src="images/00409.jpg" class="calibre_712"/></p><p class="calibre_24">由式（9.11）</p><p class="calibre_20"><img src="images/00524.jpg" class="calibre_713"/></p><p class="calibre_22">令</p><p class="calibre_20"><img src="images/00638.jpg" class="calibre_714"/></p><p class="calibre_22">于是对数似然函数可以写成</p><p class="calibre_20"><img src="images/00743.jpg" class="calibre_715"/></p><p class="calibre_22">在式（9.20）中分别取<img src="images/00692.jpg" class="calibre_18"/>为<sub class="calibre8"><small class="calibre6"><span class="calibre7">(i)</span></small></sub><img src="images/00692.jpg" class="calibre_18"/>和<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sub>并相减，有</p><p class="calibre_20"><img src="images/00851.jpg" class="calibre_716"/></p><p class="calibre_24">为证式（9.18），只需证式（9.21）右端是非负的。式（9.21）右端的第1项，由于<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)达到极大，所以有</p><p class="calibre_20"><img src="images/00961.jpg" class="calibre_717"/></p><p class="calibre_22">其第2项，由式（9.19）可得：</p><p class="calibre_20"><img src="images/00002.jpg" class="calibre_718"/></p><p class="calibre_22">这里的不等号由Jensen不等式得到。</p><p class="calibre_8">由式（9.22）和式（9.23）即知式（9.21）右端是非负的。</p><p class="calibre_8"><span class="bold">定理9.2</span>　设L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)为观测数据的对数似然函数，<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>(i＝1,2,…)为EM算法得到的参数估计序列，L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)(i＝1,2,…)为对应的对数似然函数序列。</p><p class="calibre_8">（1）如果P(Y|<img src="images/00692.jpg" class="calibre_18"/>)有上界，则L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)收敛到某一值L<sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>；</p><p class="calibre_8">（2）在函数Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00111.jpg" class="calibre_719"/>)与L(<img src="images/00692.jpg" class="calibre_18"/>)满足一定条件下，由EM算法得到的参数估计序列<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>的收敛值<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>是L(<img src="images/00692.jpg" class="calibre_18"/>)的稳定点。</p><p class="calibre_8"><span class="bold">证明</span>　（1）由L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的单调性及P(Y|<img src="images/00692.jpg" class="calibre_18"/>)的有界性立即得到。</p><p class="calibre_8">（2）证明从略，参阅文献［6］。</p><p class="calibre_8">定理9.2关于函数Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00111.jpg" class="calibre_719"/>)与L(<img src="images/00692.jpg" class="calibre_18"/>)的条件在大多数情况下都是满足的。EM算法的收敛性包含关于对数似然函数序列L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的收敛性和关于参数估计序列<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>的收敛性两层意思，前者并不蕴涵后者。此外，定理只能保证参数估计序列收敛到对数似然函数序列的稳定点，不能保证收敛到极大值点。所以在应用中，初值的选择变得非常重要，常用的办法是选取几个不同的初值进行迭代，然后对得到的各个估计值加以比较，从中选择最好的。</p><p id="filepos528522" class="calibre_6"><span class="calibre9"><span class="bold">9.3　EM算法在高斯混合模型学习中的应用</span></span></p><p class="calibre_7">EM算法的一个重要应用是高斯混合模型的参数估计。高斯混合模型应用广泛，在许多情况下，EM算法是学习高斯混合模型（Gaussian misture model）的有效方法。</p><p id="filepos528886" class="calibre_6"><span class="calibre2"><span class="bold">9.3.1　高斯混合模型</span></span></p><p class="calibre_7"><span class="bold">定义9.2（高斯混合模型）</span>　高斯混合模型是指具有如下形式的概率分布模型：</p><p class="calibre_20"><img src="images/00543.jpg" class="calibre_720"/></p><p class="calibre_22">其中，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>是系数，a<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>≥0，<img src="images/00327.jpg" class="calibre_721"/>；Ø(y|<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>)是高斯分布密度，<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>＝(<img src="images/01071.jpg" class="calibre_283"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>,<img src="images/00335.jpg" class="calibre_420"/>)，</p><p class="calibre_20"><img src="images/00050.jpg" class="calibre_722"/></p><p class="calibre_22">称为第k个分模型。</p><p class="calibre3" style="margin:0pt; border:0pt; height:1em"> </p><p class="calibre_8">一般混合模型可以由任意概率分布密度代替式（9.25）中的高斯分布密度，我们只介绍最常用的高斯混合模型。</p><p id="filepos530283" class="calibre_6"><span class="calibre2"><span class="bold">9.3.2　高斯混合模型参数估计的EM算法</span></span></p><p class="calibre_7">假设观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>,由高斯混合模型生成，</p><p class="calibre_20"><img src="images/00604.jpg" class="calibre_723"/></p><p class="calibre_22">其中，<img src="images/00692.jpg" class="calibre_18"/>＝(a<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,a<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>;<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>)。我们用EM算法估计高斯混合模型的参数<img src="images/00692.jpg" class="calibre_18"/>。</p><p class="calibre_7"><span class="bold">1．明确隐变量，写出完全数据的对数似然函数</span></p><p class="calibre_7">可以设想观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>，j＝1,2,…,N，是这样产生的：首先依概率a<sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>选择第k个高斯分布分模型Ø(y|<img src="images/00692.jpg" class="calibre_18"/>k)；然后依第k个分模型的概率分布Ø(y|<img src="images/00692.jpg" class="calibre_18"/>k)生成观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>。这时观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>，j＝1,2,…,N，是已知的；反映观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>来自第k个分模型的数据是未知的，k＝1,2,…,K，以隐变量<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>表示，其定义如下：</p><p class="calibre_20"><img src="images/00077.jpg" class="calibre_724"/></p><p class="calibre_22"><img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>是0-1随机变量。</p><p class="calibre_8">有了观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>及未观测数据<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>，那么完全数据是</p><p class="calibre_20"><img src="images/00906.jpg" class="calibre_725"/></p><p class="calibre_24">于是，可以写出完全数据的似然函数：</p><p class="calibre_20"><img src="images/00105.jpg" class="calibre_726"/></p><p class="calibre_22">式中，<img src="images/00174.jpg" class="calibre_727"/></p><p class="calibre_8">那么，完全数据的对数似然函数为</p><p class="calibre_20"><img src="images/00131.jpg" class="calibre_728"/></p><p class="calibre_7"><span class="bold">2．EM算法的E步：确定Q函数</span></p><p class="calibre_6"><img src="images/00246.jpg" class="calibre_729"/></p><p class="calibre_24">这里需要计算E(<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>|y,<img src="images/00692.jpg" class="calibre_18"/>)，记为<img src="images/00866.jpg" class="calibre_398"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>。</p><p class="calibre_20"><img src="images/00928.jpg" class="calibre_730"/></p><p class="calibre_22"><img src="images/00866.jpg" class="calibre_398"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>是在当前模型参数下第j个观测数据来自第k个分模型的概率，称为分模型k对观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>的响应度。</p><p class="calibre_8">将<img src="images/00866.jpg" class="calibre_398"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>＝E<img src="images/00021.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">jk</span></small></sub>及<img src="images/00712.jpg" class="calibre_731"/>代入式（9.28）即得</p><p class="calibre_20"><img src="images/01066.jpg" class="calibre_732"/></p><p class="calibre_7"><span class="bold">3．确定EM算法的M步</span></p><p class="calibre_7">迭代的M步是求函数Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)对<img src="images/00692.jpg" class="calibre_18"/>的极大值，即求新一轮迭代的模型参数：</p><p class="calibre_20"><img src="images/00848.jpg" class="calibre_733"/></p><p class="calibre_24">用<img src="images/00218.jpg" class="calibre_124"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，<img src="images/00475.jpg" class="calibre_734"/>及<img src="images/00390.jpg" class="calibre_586"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，k＝1,2,…,K，表示<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>的各参数。求<img src="images/00218.jpg" class="calibre_124"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，<img src="images/00475.jpg" class="calibre_734"/>只需将式（9.29）分别对<img src="images/01071.jpg" class="calibre_283"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>，<img src="images/00335.jpg" class="calibre_420"/>求偏导数并令其为0，即可得到；求<img src="images/00390.jpg" class="calibre_586"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">k</span></small></sub>是在<img src="images/00244.jpg" class="calibre_721"/>条件下求偏导数并令其为0得到的。结果如下：</p><p class="calibre_20"><img src="images/00798.jpg" class="calibre_735"/></p><p class="calibre_24">重复以上计算，直到对数似然函数值不再有明显的变化为止。</p><p class="calibre_8">现将估计高斯混合模型参数的EM算法总结如下：</p><p class="calibre_146"><span class="bold">算法9.2（高斯混合模型参数估计的EM算法）</span></p><p class="calibre_146">输入：观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,y<sub class="calibre8"><small class="calibre6"><span class="calibre7">N</span></small></sub>，高斯混合模型；</p><p class="calibre_8">输出：高斯混合模型参数。</p><p class="calibre_8">（1）取参数的初始值开始迭代</p><p class="calibre_8">（2）E步：依据当前模型参数，计算分模型k对观测数据y<sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>的响应度</p><p class="calibre_20"><img src="images/00269.jpg" class="calibre_736"/></p><p class="calibre_24">（3）M步：计算新一轮迭代的模型参数</p><p class="calibre_20"><img src="images/00629.jpg" class="calibre_737"/></p><p class="calibre_24">（4）重复第（2）步和第（3）步，直到收敛。</p><p id="filepos538235" class="calibre_6"><span class="calibre9"><span class="bold">9.4　EM算法的推广</span></span></p><p class="calibre_7">EM算法还可以解释为F函数（F function）的极大-极大算法（maximizationmaximization algorithm），基于这个解释有若干变形与推广，如广义期望极大（generalized expectation maximization，GEM）算法。下面予以介绍。</p><p id="filepos538624" class="calibre_6"><span class="calibre2"><span class="bold">9.4.1　F函数的极大-极大算法</span></span></p><p class="calibre_7">首先引进F函数并讨论其性质。</p><p class="calibre_8"><span class="bold">定义9.3（F函数）</span>　假设隐变量数据Z的概率分布为<img src="images/00541.jpg" class="calibre_306"/>(Z)，定义分布<img src="images/00541.jpg" class="calibre_306"/>与参数<img src="images/00692.jpg" class="calibre_18"/>的函数F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)如下：</p><p class="calibre_20"><img src="images/00579.jpg" class="calibre_738"/></p><p class="calibre_22">称为F函数。式中H(<img src="images/00541.jpg" class="calibre_306"/>)＝-E<sub class="calibre8"><small class="calibre6"><img src="images/00541.jpg" class="calibre_306"/></small></sub>log<img src="images/00541.jpg" class="calibre_306"/>(Z)是分布<img src="images/00541.jpg" class="calibre_306"/>(Z)的熵。</p><p class="calibre_8">在定义9.3中，通常假设P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)是<img src="images/00692.jpg" class="calibre_18"/>的连续函数，因而F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)是<img src="images/00541.jpg" class="calibre_306"/>和<img src="images/00692.jpg" class="calibre_18"/>的连续函数。函数F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)还有以下重要性质：</p><p class="calibre_8"><span class="bold">引理9.1　</span>对于固定的<img src="images/00692.jpg" class="calibre_18"/>，存在唯一的分布<img src="images/00842.jpg" class="calibre_30"/>极大化F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)，这时<img src="images/00842.jpg" class="calibre_30"/>由下式给出：</p><p class="calibre_20"><img src="images/00951.jpg" class="calibre_739"/></p><p class="calibre_22">并且<img src="images/00842.jpg" class="calibre_30"/>随<img src="images/00692.jpg" class="calibre_18"/>连续变化。</p><p class="calibre_8"><span class="bold">证明</span>　对于固定的<img src="images/00692.jpg" class="calibre_18"/>，可以求得使F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)达到极大的分布<img src="images/00842.jpg" class="calibre_30"/>(Z)。为此，引进拉格朗日乘子<img src="images/00759.jpg" class="calibre_47"/>，拉格朗日函数为</p><p class="calibre_20"><img src="images/00254.jpg" class="calibre_740"/></p><p class="calibre_22">将其对<img src="images/00541.jpg" class="calibre_306"/>求偏导数：</p><p class="calibre_20"><img src="images/00102.jpg" class="calibre_741"/></p><p class="calibre_22">令偏导数等于0，得出</p><p class="calibre_20"><img src="images/00209.jpg" class="calibre_742"/></p><p class="calibre_22">由此推出<img src="images/00842.jpg" class="calibre_30"/>(Z)与P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)成比例</p><p class="calibre_20"><img src="images/00317.jpg" class="calibre_743"/></p><p class="calibre_22">再从约束条件<img src="images/00427.jpg" class="calibre_744"/>得式（9.34）。</p><p class="calibre_8">由假设P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)是<img src="images/00692.jpg" class="calibre_18"/>的连续函数，得到<img src="images/00842.jpg" class="calibre_30"/>是<img src="images/00692.jpg" class="calibre_18"/>的连续函数。</p><p class="calibre_8"><span class="bold">引理9.2　</span>若<img src="images/00842.jpg" class="calibre_30"/>(Z)＝P(Z|Y,<img src="images/00692.jpg" class="calibre_18"/>)，则</p><p class="calibre_20"><img src="images/00076.jpg" class="calibre_745"/></p><p class="calibre_24">证明作为习题，留给读者。</p><p class="calibre_8">由以上引理，可以得到关于EM算法用F函数的极大-极大算法的解释。</p><p class="calibre_8"><span class="bold">定理9.3　</span>设L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)为观测数据的对数似然函数，<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，i＝1,2,…，为EM算法得到的参数估计序列，函数F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)由式（9.33）定义。如果F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)在<img src="images/00541.jpg" class="calibre_306"/>*和<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>有局部极大值，那么L(<img src="images/00692.jpg" class="calibre_18"/>)也在<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>有局部极大值。类似地，如果F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)在<img src="images/00541.jpg" class="calibre_306"/>*和<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>达到全局最大值，那么L(<img src="images/00692.jpg" class="calibre_18"/>)也在<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>达到全局最大值。</p><p class="calibre_8"><span class="bold">证明</span>　由引理9.1和引理9.2可知，L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)＝F(<img src="images/00842.jpg" class="calibre_30"/>,<img src="images/00692.jpg" class="calibre_18"/>)对任意<img src="images/00692.jpg" class="calibre_18"/>成立。特别地，对于使F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)达到极大的参数<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，有</p><p class="calibre_20"><img src="images/00659.jpg" class="calibre_746"/></p><p class="calibre_22">为了证明<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>是L(<img src="images/00692.jpg" class="calibre_18"/>)的极大点，需要证明不存在接近<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>的点<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>，使L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>)&gt;L(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)。假如存在这样的点<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>，那么应有F(<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>)&gt;F(<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>)，这里<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>＝<img src="images/00842.jpg" class="calibre_30"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>。但因<img src="images/00842.jpg" class="calibre_30"/>是随<img src="images/00692.jpg" class="calibre_18"/>连续变化的，<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">**</span></small></sup>应接近<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>，这与<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>和<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">*</span></small></sup>是F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/>)的局部极大点的假设矛盾。</p><p class="calibre_8">类似可以证明关于全局最大值的结论。</p><p class="calibre_8"><span class="bold">定理9.4　</span>EM算法的一次迭代可由F函数的极大-极大算法实现。</p><p class="calibre_8">设<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>为第i次迭代参数<img src="images/00692.jpg" class="calibre_18"/>的估计，<img src="images/00541.jpg" class="calibre_306"/>(i)为第i次迭代函数<img src="images/00541.jpg" class="calibre_306"/>的估计。在第i+1次迭代的两步为</p><p class="calibre_8">（1）对固定的<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，求<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)极大化；</p><p class="calibre_8">（2）对固定的<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>，求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使F(<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/>)极大化。</p><p class="calibre_8"><span class="bold">证明</span>　（1）由引理9.1，对于固定的<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，</p><p class="calibre_20"><img src="images/00761.jpg" class="calibre_747"/></p><p class="calibre_22">使F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)极大化。此时，</p><p class="calibre_20"><img src="images/00869.jpg" class="calibre_748"/></p><p class="calibre_22">由Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的定义式（9.11）有</p><p class="calibre_20"><img src="images/00979.jpg" class="calibre_749"/></p><p class="calibre_24">（2）固定<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>，求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使F(<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/>)极大化。得到</p><p class="calibre_20"><img src="images/00019.jpg" class="calibre_750"/></p><p class="calibre_22">通过以上两步完成了EM算法的一次迭代。由此可知，由EM算法与F函数的极大-极大算法得到的参数估计序列<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>，i＝1,2,…，是一致的。</p><p class="calibre_8">这样，就有EM算法的推广。</p><p id="filepos552339" class="calibre_6"><span class="calibre2"><span class="bold">9.4.2　GEM算法</span></span></p><p class="calibre_7"><span class="bold">算法9.3（GEM算法1）</span></p><p class="calibre_146">输入：观测数据，F函数；</p><p class="calibre_8">输出：模型参数。</p><p class="calibre_8">（1）初始化参数<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>，开始迭代</p><p class="calibre_8">（2）第i+1次迭代，第1步：记<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>为参数<img src="images/00692.jpg" class="calibre_18"/>的估计值，<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>为函数<img src="images/00541.jpg" class="calibre_306"/>的估计。求<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使<img src="images/00541.jpg" class="calibre_306"/>极大化F(<img src="images/00541.jpg" class="calibre_306"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)</p><p class="calibre_8">（3）第2步：求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使F(<img src="images/00541.jpg" class="calibre_306"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/>)极大化</p><p class="calibre_8">（4）重复（2）和（3），直到收敛。</p><p class="calibre_8">在GEM算法1中，有时求Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的极大化是很困难的。下面介绍的GEM算法2和GEM算法3并不是直接求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)达到极大的<img src="images/00692.jpg" class="calibre_18"/>，而是找一个<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使得Q(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)&gt;Q(<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)。</p><p class="calibre_146"><span class="bold">算法9.4（GEM算法2）</span></p><p class="calibre_146">输入：观测数据，Q函数；</p><p class="calibre_8">输出：模型参数。</p><p class="calibre_8">（1）初始化参数<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>，开始迭代</p><p class="calibre_8">（2）第i+1次迭代，第1步：记<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>为参数<img src="images/00692.jpg" class="calibre_18"/>的估计值，计算</p><p class="calibre_20"><img src="images/00127.jpg" class="calibre_751"/></p><p class="calibre_24">（3）第2步：求<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>使</p><p class="calibre_20"><img src="images/00237.jpg" class="calibre_752"/></p><p class="calibre_24">（4）重复（2）和（3），直到收敛。</p><p class="calibre_8">当参数<img src="images/00692.jpg" class="calibre_18"/>的维数为d（d≥2）时，可采用一种特殊的GEM算法，它将EM算法的M步分解为d次条件极大化，每次只改变参数向量的一个分量，其余分量不改变。</p><p class="calibre_146"><span class="bold">算法9.5（GEM算法3）</span></p><p class="calibre_146">输入：观测数据，Q函数；</p><p class="calibre_8">输出：模型参数。</p><p class="calibre_8">（1）初始化参数<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝(<img src="images/00815.jpg" class="calibre_753"/>,<img src="images/00453.jpg" class="calibre_754"/>,…,<img src="images/00783.jpg" class="calibre_753"/>)，开始迭代</p><p class="calibre_8">（2）第i+1次迭代，第1步：记<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>＝(<img src="images/00815.jpg" class="calibre_753"/>,<img src="images/00453.jpg" class="calibre_754"/>,…,<img src="images/00783.jpg" class="calibre_753"/>)为参数<img src="images/00692.jpg" class="calibre_18"/>＝(<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>,<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">2</span></small></sub>,…,<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">d</span></small></sub>)的估计值，计算</p><p class="calibre_20"><img src="images/00684.jpg" class="calibre_755"/></p><p class="calibre_24">（3）第2步：进行d次条件极大化：</p><p class="calibre_8">首先，在<img src="images/00792.jpg" class="calibre_449"/>,…,<img src="images/00456.jpg" class="calibre_754"/>保持不变的条件下求使Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)达到极大的<img src="images/01006.jpg" class="calibre_554"/>；</p><p class="calibre_8">然后，在<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">1</span></small></sub>＝<img src="images/01036.jpg" class="calibre_756"/>，<img src="images/00692.jpg" class="calibre_18"/><sub class="calibre8"><small class="calibre6"><span class="calibre7">j</span></small></sub>＝<img src="images/00215.jpg" class="calibre_757"/>，j＝3,4,…,k的条件下求使Q(<img src="images/00692.jpg" class="calibre_18"/>,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)达到极大的<img src="images/00769.jpg" class="calibre_756"/>；</p><p class="calibre_8">如此继续，经过d次条件极大化，得到<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i+1)</span></small></sup>＝(<img src="images/01006.jpg" class="calibre_554"/>,<img src="images/00769.jpg" class="calibre_756"/>,…,<img src="images/00373.jpg" class="calibre_756"/>)使得</p><p class="calibre_20"><img src="images/00482.jpg" class="calibre_758"/></p><p class="calibre_24">（4）重复（2）和（3），直到收敛。</p><p id="filepos560345" class="calibre_6"><span class="calibre9"><span class="bold">本章概要</span></span></p><p class="calibre_7">1．EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。含有隐变量的概率模型的数据表示为P(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)。这里，Y是观测变量的数据，Z是隐变量的数据，<img src="images/00692.jpg" class="calibre_18"/>是模型参数。EM算法通过迭代求解观测数据的对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)＝logP(Y|<img src="images/00692.jpg" class="calibre_18"/>)的极大化，实现极大似然估计。每次迭代包括两步：E步，求期望，即求logP(Y，Z|<img src="images/00692.jpg" class="calibre_18"/>)关于P(Z|Y,<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>)的期望：</p><p class="calibre_20"><img src="images/00603.jpg" class="calibre_759"/></p><p class="calibre_22">称为Q函数，这里<img src="images/00692.jpg" class="calibre_18"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(i)</span></small></sup>是参数的现估计值；M步，求极大，即极大化Q函数得到参数的新估计值：</p><p class="calibre_20"><img src="images/00707.jpg" class="calibre_760"/></p><p class="calibre_24">在构建具体的EM算法时，重要的是定义Q函数。每次迭代中，EM算法通过极大化Q函数来增大对数似然函数L(<img src="images/00692.jpg" class="calibre_18"/>)。</p><p class="calibre_8">2．EM算法在每次迭代后均提高观测数据的似然函数值，即</p><p class="calibre_20"><img src="images/00820.jpg" class="calibre_761"/></p><p class="calibre_22">在一般条件下EM算法是收敛的，但不能保证收敛到全局最优。</p><p class="calibre_8">3．EM算法应用极其广泛，主要应用于含有隐变量的概率模型的学习。高斯混合模型的参数估计是EM算法的一个重要应用，下一章将要介绍的隐马尔可夫模型的非监督学习也是EM算法的一个重要应用。</p><p class="calibre_8">4．EM算法还可以解释为F函数的极大-极大算法。EM算法有许多变形，如GEM算法。GEM算法的特点是每次迭代增加F函数值（并不一定是极大化F函数），从而增加似然函数值。</p><p id="filepos562981" class="calibre_6"><span class="calibre9"><span class="bold">继续阅读</span></span></p><p class="calibre_7">EM算法由Dempster等人总结提出<sup class="calibre5"><small class="calibre6"><span class="calibre7">[1]</span></small></sup>。类似的算法之前已被提出，如Baum与Welch算法，但是都没有EM算法那么广泛。EM算法的介绍可参见文献[2～4]。EM算法收敛性定理的有关证明见文献[5]。GEM是由Neal与Hinton提出的<sup class="calibre5"><small class="calibre6"><span class="calibre7">[6]</span></small></sup>。</p><p id="filepos563470" class="calibre_6"><span class="calibre9"><span class="bold">习题</span></span></p><p class="calibre_7">9.1　如例9.1的三硬币模型。假设观测数据不变，试选择不同的初值，例如，<img src="images/01057.jpg" class="calibre_686"/><sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.46，p<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.55，q<sup class="calibre5"><small class="calibre6"><span class="calibre7">(0)</span></small></sup>＝0.67，求模型参数<img src="images/00692.jpg" class="calibre_18"/>＝(<img src="images/01057.jpg" class="calibre_686"/>,p,q)的极大似然估计。</p><p class="calibre_8">9.2　证明引理9.2。</p><p class="calibre_8">9.3　已知观测数据</p><p class="calibre_8">–67，–48，6，8，14，16，23，24，28，29，41，49，56，60，75</p><p class="calibre_8">试估计两个分量的高斯混合模型的5个参数。</p><p class="calibre_8">9.4　EM算法可以用到朴素贝叶斯法的非监督学习。试写出其算法。</p><p id="filepos564542" class="calibre_6"><span class="calibre9"><span class="bold">参考文献</span></span></p><p class="calibre_7">[1]　Dempster AP,Laird NM,Rubin DB. Maximum-likelihood from incomplete data via the EM algorithm. J. Royal Statist. Soc. Ser. B.,1977，39</p><p class="calibre_8">[2]　Hastie T,Tibshirani R,Friedman J. The Elements of Statistical Learning: Data Mining,Inference,and Prediction. Springer-Verlag,2001（中译本：统计学习基础——数据挖掘、推理与预测。范明，柴玉梅，昝红英等译。北京：电子工业出版社，2004）</p><p class="calibre_8">[3]　McLachlan G,Krishnan T. The EM Algorithm and Extensions. New York: John Wiley &amp; Sons,1996</p><p class="calibre_8">[4]　茆诗松，王静龙，濮晓龙。高等数理统计。北京：高等教育出版社；海登堡：斯普林格出版社，1998</p><p class="calibre_8">[5]　Wu CFJ. On the convergence properties of the EM algorithm. The Annals of Statistics,1983,11: 95–103</p><p class="calibre_8">[6]　Radford N,Geoffrey H,Jordan MI. A view of the EM algorithm that justifies incremental,sparse,and other variants. In: Learning in Graphical Models. Cambridge,MA: MIT Press,1999,355–368</p><div class="mbp_pagebreak" id="calibre_pb_14"></div>
</body></html>
